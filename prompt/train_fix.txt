===== args.py =====
import argparse

DATASET_NAMES = ["rico", "crello",'dataforfd']


class BaseArgs:
    def __init__(self):
        self.parser = argparse.ArgumentParser(
            formatter_class=argparse.ArgumentDefaultsHelpFormatter
        )
        self.parser.add_argument(
            "--dataset_name",
            required=True,
            choices=DATASET_NAMES,
            help="Name of the dataset.",
        )
        self.parser.add_argument(
            "--data_dir",
            # required=True,
            help="The GCS or local path of the data location.",
        )
        self.parser.add_argument(
            "--weights",
            default=None,
            type=str,
            help="Path to the initial model weight.",
        )
        self.parser.add_argument(
            "--latent_dim",
            default=256,
            type=int,
            help="Latent dimension.",
        )
        self.parser.add_argument(
            "--num_blocks",
            default=4,
            type=int,
            help="Number of stacked blocks in sequence encoder.",
        )
        self.parser.add_argument(
            "--arch_type",
            default="oneshot",
            help="Overall model type",
        )
        self.parser.add_argument(
            "--block_type",
            default="deepsvg",
            help="Stacked block type.",
        )
        self.parser.add_argument(
            "--l2",
            default=1e-2,
            type=float,
            help="Scalar coefficient for L2 regularization.",
        )
        self.parser.add_argument(
            "--dropout",
            default=0.1,
            type=float,
            help="Scalar ratio for dropout in transformer",
        )
        self.parser.add_argument(
            "--masking_method",
            type=str,
            default="random",
        )
        self.parser.add_argument(
            "--seq_type",
            type=str,
            default="default",
            choices=["default", "flat", "concat_enc"],
            help="transformer's input is: element-wise feature (default), field-wise feature (flat)",
        )
        self.parser.add_argument("--log_level", default="INFO", type=str)
        self.parser.add_argument("--verbose", default=2, type=int)
        self.parser.add_argument("--seed", default=0, type=int)
        self.parser.add_argument("--mult", default=1.0, type=float)
        self.parser.add_argument(
            "--context",
            default=None,
        )
        self.parser.add_argument(
            "--input_dtype",
            type=str,
            default="set",
            choices=["set", "shuffled_set"],
        )
        self.parser.add_argument("--batch_size", default=256, type=int)

    def parse_args(self):
        return self.parser.parse_args()


class TrainArgs(BaseArgs):
    def __init__(self):
        super().__init__()
        self.parser.add_argument(
            "--job-dir",
            required=True,
            help="The GCS or local path of logs and saved models.",
        )
        self.parser.add_argument(
            "--num_epochs",
            default=500,
            type=int,
            help="Number of epochs to train.",
        )
        self.parser.add_argument(
            "--learning_rate",
            default=1e-4,
            type=float,
            help="Base learning rate.",
        )
        self.parser.add_argument(
            "--enable_profile",
            dest="enable_profile",
            action="store_true",
            help="Enable profiling for tensorboard. (See tensorflow/tensorboard#3149)",
        )
        self.parser.add_argument(
            "--validation_freq",
            default=5,
            type=int,
            help="Validation frequency in terms of epochs.",
        )

    def __call__(self):
        return self.parser.parse_args()


===== main.py =====
import logging

from mfp.args import TrainArgs

logger = logging.getLogger(__name__)


def main():
    args = TrainArgs().parse_args()
    logging.basicConfig(level=getattr(logging, args.log_level.upper()))
    logger.info(args)

    from mfp.train import train

    train(args)


if __name__ == "__main__":
    main()


===== crello-spec.yml =====
name: crello
columns:
  id:
    dtype: string
    demo_only: true
  length:
    dtype: int64
    lookup:
      vocabulary:
        min: 1
        max: 50
      num_oov_indices: 0
      mask_value: null
  group:
    dtype: string
    lookup:
      mask_token: ''
      num_oov_indices: 0
  format:
    dtype: string
    lookup:
      mask_token: ''
      num_oov_indices: 0
  canvas_width:
    dtype: int64
    lookup:
      num_oov_indices: 0
  canvas_height:
    dtype: int64
    lookup:
      num_oov_indices: 0
  category:
    dtype: string
    lookup:
      mask_token: ''
      num_oov_indices: 0
  type:
    is_sequence: true
    dtype: string
    lookup:
      mask_token: ''
      num_oov_indices: 0
    primary_label:
      default: ''
  left:
    is_sequence: true
    dtype: float32
    discretize:
      min: 0.0
      max: 1.0
      bins: 64
  top:
    is_sequence: true
    dtype: float32
    discretize:
      min: 0.0
      max: 1.0
      bins: 64
  width:
    is_sequence: true
    dtype: float32
    discretize:
      min: 0.0
      max: 1.0
      bins: 64
  height:
    is_sequence: true
    dtype: float32
    discretize:
      min: 0.0
      max: 1.0
      bins: 64
  opacity:
    is_sequence: true
    dtype: float32
    discretize:
      min: 0.0
      max: 1.0
      bins: 8
  color:
    is_sequence: true
    shape: [3]
    dtype: int64
    discretize:
      min: 0
      max: 255
      bins: 16
    loss_condition:
      key: type
      values:
        - textElement
        - coloredBackground
  image_embedding:
    is_sequence: true
    shape: [512]
    dtype: float32
    loss_condition:
      key: type
      values:
        - svgElement
        - imageElement
        - maskElement
  text_embedding:
    is_sequence: true
    shape: [512]
    dtype: float32
    loss_condition:
      key: type
      values:
        - textElement
  font_family:
    is_sequence: true
    dtype: string
    min_freq: 500
    lookup:
      num_oov_indices: 1
      mask_token: null
    loss_condition:
      key: type
      values:
        - textElement
  uuid:
    is_sequence: true
    dtype: string
    demo_only: true


===== layoutvae.py =====
from typing import Dict

import tensorflow as tf
import tensorflow_probability as tfp
from mfp.data.spec import get_valid_input_columns
from mfp.models.architecture.cvae import MACVAEDecoder, MACVAEEncoder, MAPrior
from mfp.models.architecture.decoder import Decoder
from mfp.models.architecture.encoder import Encoder
from mfp.models.architecture.transformer import Blocks

MND = tfp.distributions.MultivariateNormalDiag


class LayoutVAE(tf.keras.layers.Layer):
    def __init__(
        self,
        input_columns: Dict,
        num_blocks: int = 4,
        block_type: str = "deepsvg",
        input_dtype: str = "set",
        kl: float = 1e-0,
        **kwargs,  # keys are latent_dim, dropout, l2
    ):
        super().__init__()
        l2 = kwargs.get("l2", None)
        self.kl = kl  # kl search range: [1e0, 1e1, 1e2]
        self.arch_type = "autoreg"
        self.input_columns = input_columns
        self.valid_input_columns = get_valid_input_columns(input_columns, False)
        self.lookahead = False
        self.encoder = Encoder(input_columns, **kwargs)
        self.decoder = Decoder(input_columns, detachment="none", **kwargs)

        # separately encode each attribute
        self.encoder_gt = Encoder(input_columns, fusion="none", **kwargs)
        self.encoder_cvae = MACVAEEncoder(self.valid_input_columns, l2=l2)
        self.decoder_cvae = MACVAEDecoder(self.valid_input_columns, l2=l2)
        self.prior = MAPrior(self.valid_input_columns, l2=l2)

        self.blocks = Blocks(
            num_blocks=num_blocks,
            block_type=block_type,
            **kwargs,
        )

    def call(
        self,
        inputs: Dict,
        targets: Dict,
        mfp_masks: Dict,
        training: bool,
        add_masked_input: bool = True,
    ):
        S = tf.shape(inputs["left"])[1]
        h_inputs, mask = self.encoder(inputs, training=training)
        if training:
            h_targets, _ = self.encoder(targets, training=training)

        stack = {k: None for k in self.valid_input_columns}
        buffer = {}

        h_pred = None  # should be the result of Encoder(x)
        for i in range(S):
            if i == 0:
                h_fused = h_inputs
            else:
                # use GT in 0~i-1 th, use masked inputs in i~S-1 th in training
                h_fused = h_targets[:, 0:i] if training else h_pred[:, 0:i]
                h_fused = tf.concat([h_fused, h_inputs[:, i:]], axis=1)

            c = self.blocks(h_fused, mask, training=training)[:, i : i + 1]
            if training:
                h, _ = self.encoder_gt(targets, training=training)
                h = {k: v[:, i : i + 1] for (k, v) in h.items()}
                zs = self.encoder_cvae(h, c, training=training)
                zs_p = self.prior(c, training=training)
            else:
                zs = self.prior(c, training=training)
            z = {k: v["z"] for (k, v) in zs.items()}

            for (k, v) in self.decoder_cvae(z, c, training=training).items():
                if i == 0:
                    stack[k] = v
                    if training:
                        for name in ["mean", "log_sigma"]:
                            buffer[f"{k}_{name}"] = zs[k][f"z_{name}"]
                            buffer[f"{k}_{name}_p"] = zs_p[k][f"z_{name}"]
                else:
                    stack[k] = tf.concat([stack[k], v], axis=1)
                    if training:
                        for name in ["mean", "log_sigma"]:
                            buffer[f"{k}_{name}"] = tf.concat(
                                [buffer[f"{k}_{name}"], zs[k][f"z_{name}"]],
                                axis=1,
                            )
                            buffer[f"{k}_{name}_p"] = tf.concat(
                                [buffer[f"{k}_{name}_p"], zs_p[k][f"z_{name}"]],
                                axis=1,
                            )

            if not training:
                elem = self._compute_next(i, stack, mask, inputs, mfp_masks)
                h_pred = (
                    tf.concat([h_pred, elem], axis=1) if tf.is_tensor(h_pred) else elem
                )

        if training:
            self._compute_kl(buffer, mfp_masks)

        outputs = self.decoder(stack, training=training)
        return outputs

    def _compute_kl(self, x: Dict[str, tf.Tensor], mfp_masks: Dict[str, tf.Tensor]):
        loss_total = 0.0
        for k in self.valid_input_columns:
            dist = MND(x[f"{k}_mean"], tf.exp(0.5 * x[f"{k}_log_sigma"]))
            dist_p = MND(x[f"{k}_mean_p"], tf.exp(0.5 * x[f"{k}_log_sigma_p"]))
            loss = dist.kl_divergence(dist_p)
            weight = tf.cast(mfp_masks[k], tf.float32)
            loss = loss * self.kl * weight
            loss = tf.reduce_mean(loss)
            self.add_metric(loss, name=k + "_loss")
            loss_total += loss

        self.add_metric(loss_total, name="kl_loss_total")
        self.add_loss(loss_total)

    def _compute_next(
        self,
        i: int,
        h: Dict[str, tf.Tensor],
        mask: tf.Tensor,
        inputs: Dict[str, tf.Tensor],
        mfp_masks: Dict[str, tf.Tensor],
    ) -> tf.Tensor:
        B = tf.shape(mask)[0]
        h_i = {}
        for (k, v) in h.items():
            h_i[k] = v[:, i : i + 1]
        outputs_i = self.decoder(h_i, training=False)

        new_inputs = {}
        for key, column in self.input_columns.items():
            if column["is_sequence"] and not column.get("demo_only", False):
                if column["type"] == "categorical":
                    outputs_i[key] = tf.argmax(
                        outputs_i[key], axis=-1, output_type=tf.int32
                    )
                new_inputs[key] = tf.where(
                    mfp_masks[key][:, i : i + 1, tf.newaxis],
                    outputs_i[key],
                    inputs[key][:, i : i + 1],
                )
        new_inputs["length"] = tf.zeros((B, 1))
        next_elem, _ = self.encoder(new_inputs)  # (B, 1, D)
        return next_elem


===== crello-texts-spec.yml =====
name: crello-texts
columns:
  text_hash:
    is_sequence: true
    dtype: string
  text_embedding:
    is_sequence: true
    shape: [512]
    dtype: float32


===== model.py =====
from typing import Dict, Union

import tensorflow as tf
from mfp.models.architecture.decoder import Decoder
from mfp.models.architecture.encoder import Encoder
from mfp.models.architecture.transformer import Blocks, CrossBlocks

# tf.config.set_visible_devices(tf.config.list_physical_devices('GPU')[1], 'GPU')
class _OneShot(tf.keras.layers.Layer):
    def __init__(
        self,
        input_columns: Dict,
        num_blocks: int = 4,
        block_type: str = "deepsvg",
        **kwargs,  # keys are latent_dim, dropout, l2
    ):
        super().__init__()
        self.arch_type = "oneshot"
        self.encoder, self.decoder = None, None
        self.blocks = Blocks(
            num_blocks=num_blocks,
            block_type=block_type,
            **kwargs,
        )

    def call(self, inputs, training):
        h, mask = self.encoder(inputs, training=training)
        h = self.blocks(h, mask, training=training)
        outputs = self.decoder(h, training=training)
        return outputs


class Model(_OneShot):
    def __init__(
        self,
        input_columns: Dict,
        num_blocks: int = 4,
        block_type: str = "deepsvg",
        context: Union[str, None] = None,
        input_dtype: str = "set",
        use_elemwise_noise: bool = False,
        **kwargs,
    ):
        super().__init__(input_columns, num_blocks, block_type, **kwargs)
        self.encoder = Encoder(
            input_columns,
            context=context,
            input_dtype=input_dtype,
            use_elemwise_noise=use_elemwise_noise,
            **kwargs,
        )
        self.decoder = Decoder(input_columns, context=context, **kwargs)


class VanillaTransformer(_OneShot):
    def __init__(
        self,
        input_columns: Dict,
        num_blocks: int = 4,
        block_type: str = "deepsvg",
        context: Union[str, None] = None,
        input_dtype: str = "set",
        use_elemwise_noise: bool = False,
        **kwargs,
    ):
        super().__init__(input_columns, num_blocks, block_type, **kwargs)
        assert input_dtype == "shuffled_set"
        self.encoder = Encoder(
            input_columns, fusion="flat", input_dtype=input_dtype, **kwargs
        )
        self.decoder = Decoder(input_columns, detachment="flat", **kwargs)


class _AutoReg(tf.keras.layers.Layer):
    def __init__(
        self,
        input_columns: Dict,
        num_blocks: int = 4,
        block_type: str = "deepsvg",
        context: Union[str, None] = None,
        input_dtype: str = "set",
        **kwargs,  # keys are latent_dim, dropout, l2
    ):
        super().__init__()
        self.add_masked_input = False
        # self.add_masked_input = True

        self.lookahead = False
        self.latent_dim = kwargs["latent_dim"]
        dim = self.latent_dim // 2 if self.add_masked_input else self.latent_dim
        self.input_columns = get_valid_input_columns(input_columns)

        self.encoder = Encoder(input_columns, input_dtype=input_dtype, **kwargs)
        self.decoder = Decoder(input_columns, **kwargs)

        initializer = tf.random_normal_initializer()
        self.bos = tf.Variable(
            initial_value=initializer(shape=(1, 1, dim), dtype=tf.float32),
            trainable=True,
        )
        if self.add_masked_input:
            self.dimred = tf.keras.layers.Dense(
                units=dim,
                name="dimred",
                **make_dense_options(kwargs.get("l2", None)),
            )

    def _compute_next(self, h, mask, inputs, mfp_masks):
        # Transform sequence and get the last element.
        if isinstance(mask, tuple):
            # (tgt_mask, memory_mask)
            B = tf.shape(mask[0])[0]
            S = tf.shape(mask[0])[1]
        else:
            B = tf.shape(mask)[0]
            S = tf.shape(mask)[1]

        h = self.blocks(h, mask, training=False)
        h_t = h[:, S - 1 : S]

        # Get output (=next input) at step t.
        outputs_t = self.decoder(h_t, training=False)
        new_inputs = {}

        for key, column in self.input_columns.items():
            if column["is_sequence"]:
                if column["type"] == "categorical":
                    outputs_t[key] = tf.argmax(
                        outputs_t[key], axis=-1, output_type=tf.int32
                    )
                new_inputs[key] = tf.where(
                    mfp_masks[key][:, S - 1 : S, tf.newaxis],
                    outputs_t[key],
                    inputs[key][:, S - 1 : S],
                )

        new_inputs["length"] = tf.zeros((B, 1))
        next_elem, _ = self.encoder(new_inputs)

        tf.debugging.assert_rank(next_elem, 3)
        return next_elem


class AutoReg(_AutoReg):
    def __init__(
        self,
        input_columns: Dict,
        num_blocks: int = 4,
        block_type: str = "deepsvg",
        context: Union[str, None] = None,
        input_dtype: str = "set",
        **kwargs,  # keys are latent_dim, dropout, l2
    ):
        super().__init__(
            input_columns=input_columns,
            num_blocks=num_blocks,
            block_type=block_type,
            context=context,
            input_dtype=input_dtype,
            **kwargs,
        )
        self.blocks = Blocks(
            num_blocks=num_blocks,
            block_type=block_type,
            lookahead=False,
            **kwargs,
        )

    def call(self, inputs, targets, mfp_masks, training):
        """
        If add_masked_input, each point in a sequence will be concat. of
        [previous_emb, current_emb(masked)] instead of previous_emb
        """
        if training:
            h_masked, mask = self.encoder(inputs, training=training)
            h_tgt, _ = self.encoder(targets, training=training)
            B = tf.shape(h_masked)[0]

            if self.add_masked_input:
                h_masked = self.dimred(h_masked)
                h_tgt = self.dimred(h_tgt)

            # Prepend the beginning-of-seq embedding, and drop the last.
            bos = tf.tile(self.bos, (B, 1, 1))
            h = tf.concat([bos, h_tgt[:, 0:-1, :]], axis=1)
            if self.add_masked_input:
                h = tf.concat([h, h_masked], axis=-1)  # (B, 1, 2*D)

            h = self.blocks(h, mask, training=training)
            outputs = self.decoder(h, training=training)
        else:
            h_masked, mask = self.encoder(inputs, training=training)

            B = tf.shape(mask)[0]
            S = tf.shape(mask)[1]
            h = tf.tile(self.bos, (B, 1, 1))

            if self.add_masked_input:
                h_masked = self.dimred(h_masked)
                h = tf.concat([h, h_masked[:, 0:1]], axis=-1)  # (B, 1, 2*D)

            for t in range(S - 1):
                tf.autograph.experimental.set_loop_options(
                    shape_invariants=[
                        (h, tf.TensorShape([None, None, self.latent_dim])),
                    ]
                )

                next_elem = self._compute_next(h, mask[:, : t + 1], inputs, mfp_masks)
                if self.add_masked_input:
                    next_elem = tf.concat(
                        [self.dimred(next_elem), h_masked[:, t + 1 : t + 2]],
                        axis=-1,
                    )  # (B, 1, 2*D)
                h = tf.concat([h, next_elem], axis=1)  # (B, (t+1)+1, ?)

            # [<BOS>, T_{1}, ..., T_{t-1}] -> [T_{1}, ..., T_{t}]
            h = self.blocks(h, mask, training=training)
            outputs = self.decoder(h, training=training)
        return outputs


# class OneShotAutoReg(_AutoReg):
#     def __init__(
#         self,
#         input_columns: Dict,
#         num_blocks: int = 4,
#         block_type: str = "deepsvg",
#         context: Union[str, None] = None,
#         input_dtype: str = "set",
#         **kwargs,  # keys are latent_dim, dropout, l2
#     ):
#         super().__init__(
#             input_columns=input_columns,
#             num_blocks=num_blocks,
#             block_type=block_type,
#             context=context,
#             input_dtype=input_dtype,
#             **kwargs,
#         )
#         self.enc_blocks = Blocks(
#             num_blocks=num_blocks // 2,
#             block_type=block_type,
#             lookahead=True,
#             **kwargs,
#         )
#         self.blocks = Blocks(
#             num_blocks=num_blocks // 2,
#             block_type=block_type,
#             lookahead=False,
#             conditional=True,
#             **kwargs,
#         )
#         initializer = tf.random_normal_initializer()
#         self.cls = tf.Variable(
#             initial_value=initializer(
#                 shape=(1, 1, kwargs["latent_dim"]), dtype=tf.float32
#             ),
#             trainable=True,
#         )

#     def call(self, inputs, targets, mfp_masks, training):
#         training = False
#         if training:
#             h_masked, mask = self.encoder(inputs, training=training)
#             h_tgt, _ = self.encoder(targets, training=training)
#             B, S, _ = tf.shape(h_masked)

#             # add [CLS] token
#             new_mask = get_seq_mask(inputs["length"] + 1)
#             new_h_masked = tf.concat(
#                 [tf.tile(self.cls, (B, 1, 1)), h_masked], axis=1
#             )

#             z = self.enc_blocks(new_h_masked, new_mask, training=training)[:, 0]
#             if self.add_masked_input:
#                 h_masked = self.dimred(h_masked)
#                 h_tgt = self.dimred(h_tgt)

#             # Prepend the beginning-of-seq embedding, and drop the last.
#             bos = tf.tile(self.bos, (B, 1, 1))
#             h = tf.concat([bos, h_tgt[:, 1:, :]], axis=1)
#             if self.add_masked_input:
#                 h = tf.concat([h, h_masked], axis=-1)  # (B, 1, 2*D)

#             h = self.blocks((h, z), mask, training=training)
#             outputs = self.decoder(h, training=training)
#         else:
#             # add [CLS] token
#             h_masked, mask = self.encoder(inputs, training=training)
#             B = tf.shape(h_masked)[0]
#             new_mask = get_seq_mask(inputs["length"] + 1)
#             new_h_masked = tf.concat(
#                 [tf.tile(self.cls, (B, 1, 1)), h_masked], axis=1
#             )

#             z = self.enc_blocks(new_h_masked, new_mask, training=training)[:, 0]

#             # make sure to ignore first element (only for z)

#             bos = tf.tile(self.bos, (B, 1, 1))
#             if self.add_masked_input:
#                 h_masked = self.dimred(h_masked)
#                 h = tf.concat([bos, h_masked[:, 0:1]], axis=-1)  # (B, 1, 2*D)
#             else:
#                 h = bos

#             S = tf.shape(mask)[1]
#             for t in range(S - 1):
#                 tf.autograph.experimental.set_loop_options(
#                     shape_invariants=[
#                         (h, tf.TensorShape([None, None, self.latent_dim])),
#                     ]
#                 )
#                 next_elem = self._compute_next(
#                     (h, z), mask[:, : t + 1], inputs, mfp_masks
#                 )
#                 if self.add_masked_input:
#                     next_elem = tf.concat(
#                         [self.dimred(next_elem), h_masked[:, t + 1 : t + 2]],
#                         axis=-1,
#                     )  # (B, 1, 2*D)
#                 h = tf.concat([h, next_elem], axis=1)  # (B, (t+1)+1, 2*D)

#             # [<BOS>, T_{1}, ..., T_{t-1}] -> [T_{1}, ..., T_{t}]
#             h = self.blocks((h, z), mask, training=training)
#             outputs = self.decoder(h, training=training)

#         return outputs


class BART(_AutoReg):
    def __init__(
        self,
        input_columns: Dict,
        num_blocks: int = 4,
        block_type: str = "deepsvg",
        context: Union[str, None] = None,
        input_dtype: str = "set",
        **kwargs,  # keys are latent_dim, dropout, l2
    ):
        assert input_dtype == "shuffled_set"
        super().__init__(
            input_columns=input_columns,
            num_blocks=num_blocks,
            block_type=block_type,
            context=context,
            input_dtype=input_dtype,
            **kwargs,
        )
        self.enc_blocks = Blocks(
            num_blocks=num_blocks // 2,
            block_type=block_type,
            lookahead=True,
            **kwargs,
        )
        self.blocks = CrossBlocks(
            num_blocks=num_blocks // 2,
            block_type=f"{block_type}_cross",
            lookahead=False,
            **kwargs,
        )
        # initializer = tf.random_normal_initializer()

    def call(self, inputs, targets, mfp_masks, training):
        if training:
            h_masked, mask = self.encoder(inputs, training=training)
            h_tgt, _ = self.encoder(targets, training=training)

            B, _, _ = tf.shape(h_masked)
            z = self.enc_blocks(h_masked, mask, training=training)
            tgt_mask = mask

            # Prepend the beginning-of-seq embedding, and drop the last.
            bos = tf.tile(self.bos, (B, 1, 1))
            h = tf.concat([bos, h_tgt[:, :-1, :]], axis=1)
            h = self.blocks((h, z), (tgt_mask, mask), training=training)
            outputs = self.decoder(h, training=training)
        else:
            # add [CLS] token
            h_masked, mask = self.encoder(inputs, training=training)
            B, S, _ = tf.shape(h_masked)
            z = self.enc_blocks(h_masked, mask, training=training)

            h = tf.tile(self.bos, (B, 1, 1))
            for t in range(S - 1):
                tf.autograph.experimental.set_loop_options(
                    shape_invariants=[
                        (h, tf.TensorShape([None, None, self.latent_dim])),
                    ]
                )
                next_elem = self._compute_next(
                    (h, z), (mask[:, : t + 1], mask), inputs, mfp_masks
                )
                h = tf.concat([h, next_elem], axis=1)  # (B, (t+1)+1, 2*D)

            # [<BOS>, T_{1}, ..., T_{t-1}] -> [T_{1}, ..., T_{t}]
            h = self.blocks((h, z), (mask, mask), training=training)
            outputs = self.decoder(h, training=training)

        return outputs


===== tensor_utils.py =====
import logging
import random
from typing import Dict, List, Union

import tensorflow as tf
from mfp.models.architecture.mask import get_seq_mask

logger = logging.getLogger(__name__)


KEYS = ["type", "left", "top", "width", "height"]


def sort_inputs(inputs: Dict, input_columns: Dict, from_logits: bool = False):
    CONST = 100
    # assert set(inputs.keys()) == (set(input_columns.keys()))
    assert "length" in inputs
    assert tf.executing_eagerly()
    for key in KEYS:
        assert key in inputs
        assert input_columns[key]["input_dim"] < CONST

    data = {k: tf.identity(v) for (k, v) in inputs.items()}
    for key, column in input_columns.items():
        if column["is_sequence"] and column["type"] == "categorical":
            if from_logits:
                data[key] = tf.argmax(data[key], axis=-1)
            data[key] = tf.cast(data[key], tf.int64)

    invalid = tf.logical_not(get_seq_mask(data["length"]))
    priority = 0  # use int64 to avoid overflow
    for key in KEYS:
        priority = priority * CONST + data[key][..., 0]  # (B, S)
    priority += tf.cast(invalid, tf.int64) * (CONST ** len(KEYS))
    indices = tf.argsort(priority, axis=-1)

    new_inputs = {}
    for key in inputs:
        val = tf.identity(inputs[key])
        if key in input_columns and input_columns[key]["is_sequence"]:
            new_inputs[key] = tf.gather(val, indices, batch_dims=1)
        else:
            new_inputs[key] = val
    return new_inputs


def shuffle_inputs(inputs: Dict):
    """
    Used to shuffle input sets for training
    - auto-regressive models
    - models that take shuffled sets as inputs
    """
    assert "length" in inputs and "left" in inputs
    if tf.executing_eagerly():
        shape = tf.shape(inputs["left"])
        B = shape[0]
        S = shape[1]
        data = []
        for i in range(B):
            N = inputs["length"][i, 0] + 1
            x = list(range(N))
            random.shuffle(x)
            x = x + list(range(N, S))
            data.append(x)
        indices = tf.convert_to_tensor(data)

        new_inputs = {}
        for key in inputs.keys():
            val = tf.identity(inputs[key])
            if val.shape[1] == S:
                new_inputs[key] = tf.gather(val, indices, batch_dims=1)
            else:
                new_inputs[key] = val
        return new_inputs
    else:
        logger.info("Shuffling sequences in order not to feed order for autoreg models")
        # backdoor for model._make() (done in graph mode)
        return inputs


def reorganize_indices(
    from_inds: tf.Tensor, n_elems: tf.Tensor, maxlen: Union[int, None] = None
):
    """
    Used to reorganize the element order (for element-wise masking)
    """
    if tf.executing_eagerly():
        tf.debugging.assert_rank(from_inds, 2)  # (B, 1)
        tf.debugging.assert_rank(n_elems, 2)  # (B, 1)
        # tf.debugging.assert_less_equal(from_inds, n_elems)
        B = tf.shape(from_inds)[0]
        if not maxlen:
            maxlen = tf.reduce_max(n_elems).numpy() + 1
        data = []
        for i in range(B):
            from_ind = from_inds[i, 0].numpy()
            n_elem = n_elems[i, 0].numpy()
            ids = list(range(maxlen))
            del ids[from_ind]
            ids = ids[:n_elem] + [from_ind] + ids[n_elem:]
            data.append(ids)
        return tf.convert_to_tensor(data)
    else:
        # backdoor for model._make() (done in graph mode)
        B = tf.shape(n_elems)[0]
        maxlen = tf.reduce_max(n_elems) + 1
        indices = tf.tile(tf.range(maxlen)[tf.newaxis, :], (B, 1))
        return indices


def merge_list_of_dict_of_tensors(
    inputs: List[Dict[str, tf.Tensor]], axis: int = 0
) -> Dict[str, tf.Tensor]:
    result = {}
    for k in inputs[0].keys():
        result[k] = tf.concat([x[k] for x in inputs], axis=axis)
    return result


def split_dict_of_tensors(
    inputs: Dict[str, tf.Tensor], num_splits: int = 1, axis: int = 0
) -> List[Dict[str, tf.Tensor]]:
    result = [{} for _ in range(num_splits)]
    for (k, v) in inputs.items():
        for i, x in enumerate(tf.split(v, num_splits, axis=axis)):
            result[i][k] = x
            if i >= 1:  # num of dim. along axis should be divisible
                tf.debugging.assert_equal(tf.shape(x), tf.shape(result[0][k]))
    return result


if __name__ == "__main__":
    x = [
        {"a": tf.reshape(tf.range(6), (2, 3)), "b": tf.zeros((3, 2))},
        {"a": 10 + tf.reshape(tf.range(6), (2, 3)), "b": tf.ones((3, 2))},
    ]
    h = merge_list_of_dict_of_tensors(x, axis=0)


===== metrics.py =====
from typing import Dict, Union

import tensorflow as tf
from mfp.data.spec import get_valid_input_columns
from mfp.models.architecture.mask import get_seq_mask

from .tensor_utils import sort_inputs

BIG_CONST = 1000.0


def mae_from_logits(y_true: tf.Tensor, y_pred: tf.Tensor, from_logits: bool = True):
    # tf.debugging.assert_rank(y_true, 2)
    # tf.debugging.assert_rank(y_pred, 3)
    tf.debugging.assert_equal(tf.rank(y_true) + 1, tf.rank(y_pred))

    C = tf.shape(y_pred)[-1]
    div = tf.cast(C - 1, tf.float32)
    target = tf.cast(y_true, tf.float32)
    target = target / div
    output = tf.nn.softmax(y_pred) if from_logits else y_pred
    values = tf.cast(tf.range(C), tf.float32) / div
    if tf.rank(y_true) == 2:
        output *= values[tf.newaxis, tf.newaxis, :]
    elif tf.rank(y_true) == 3:
        output *= values[tf.newaxis, tf.newaxis, tf.newaxis, :]
    else:
        raise NotImplementedError

    output = tf.reduce_sum(output, axis=-1)
    # loss = tf.keras.metrics.mean_absolute_error(target, output)
    loss = tf.math.abs(target - output)
    return loss


def compute_categorical_mfp_metric(
    y_true: tf.Tensor, y_pred: tf.Tensor, from_logits: bool = True
):
    # shape of y_true and y_pred is (..., C, X)
    # shape of loss and score is both (..., C)
    if from_logits:
        y_pred_ = tf.nn.softmax(y_pred)
    else:
        y_pred_ = y_pred

    y_pred_argmax = tf.argmax(y_pred_, axis=-1, output_type=tf.int32)
    loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred_)
    score = tf.cast(y_true == y_pred_argmax, tf.float32)
    return loss, score


def compute_continuous_mfp_metric(y_true: tf.Tensor, y_pred: tf.Tensor):
    # shape of y_true and y_pred is (..., C, X)
    # shape of loss and score is both (..., )
    loss = tf.keras.losses.mean_squared_error(y_true, y_pred)
    score = -0.5 * tf.keras.losses.cosine_similarity(y_true, y_pred) + 0.5
    return loss, score


class BeautyLayer(tf.keras.layers.Layer):
    """
    For definition of each metric, please refer to
    Attribute-conditioned Layout GAN for Automatic Graphic Design
    https://arxiv.org/abs/2009.05284
    """

    def __init__(self, input_columns: Dict, name: str = "beauty_layer", **kwargs):
        super().__init__(name=name, **kwargs)
        assert "left" in input_columns and "width" in input_columns
        self.input_columns = input_columns

    def call(self, inputs, training=False, from_logits: bool = True):
        if from_logits:
            y_pred, masks = inputs
        else:
            y_true, masks = inputs
        mask = masks["left"]  # (B, S)
        B, S = tf.shape(mask)

        mask_float = tf.cast(mask, tf.float32)
        count = tf.reduce_sum(mask_float, axis=-1)
        num_invalid_documents = tf.reduce_sum(tf.cast(count <= 1, tf.float32))
        num_valid_documents = tf.cast(B, tf.float32) - num_invalid_documents

        data = {}
        for key in ["left", "width", "top", "height"]:
            column = self.input_columns[key]
            C = tf.cast(column["input_dim"], tf.float32)
            if from_logits:
                coords = tf.math.argmax(y_pred[key], axis=-1)[..., 0]  # (B, S)
            else:
                coords = y_true[key][..., 0]
            data[key] = tf.cast(coords, tf.float32) / (C - 1)  # (B, S)

        eye = tf.eye(S, batch_shape=[B], dtype=tf.bool)
        valid = tf.math.logical_and(mask[:, tf.newaxis, :], mask[..., tf.newaxis])
        invalid = tf.math.logical_or(eye, tf.logical_not(valid))

        keys = [("left", "width"), ("top", "height")]
        diff = []
        for (start_key, interval_key) in keys:
            for i in range(3):
                s = i / 2
                h = data[start_key] + data[interval_key] * s  # (B, S)
                h = h[:, :, tf.newaxis] - h[:, tf.newaxis, :]  # (B, S, S)
                # Eq. 11
                h = tf.math.abs(h)
                h = tf.where(invalid, tf.ones_like(h), h)
                h = tf.reduce_min(h, axis=-1)  # (B, S)
                h = -1.0 * tf.math.log(1.0 - h)
                diff.append(h)

        # Eq. 10
        diff = tf.stack(diff, axis=-1)  # (B, S, 6)
        diff = tf.reduce_min(diff, axis=-1)  # (B, S)
        diff = tf.where(tf.math.is_finite(diff), diff, tf.zeros_like(diff))
        alignment = tf.reduce_sum(diff, axis=-1) / count  # (B, )
        alignment = tf.where(count > 1, alignment, tf.zeros_like(alignment))

        # Overlap
        right = data["left"] + data["width"]
        bottom = data["top"] + data["height"]
        l1, t1 = data["left"][..., tf.newaxis], data["top"][..., tf.newaxis]
        r1, b1 = right[..., tf.newaxis], bottom[..., tf.newaxis]
        l2, t2 = data["left"][:, tf.newaxis, :], data["top"][:, tf.newaxis, :]
        r2, b2 = right[:, tf.newaxis, :], bottom[:, tf.newaxis, :]

        a1 = (r1 - l1) * (b1 - t1)
        l_max, t_max = tf.math.maximum(l1, l2), tf.math.maximum(t1, t2)
        r_min, b_min = tf.math.minimum(r1, r2), tf.math.minimum(b1, b2)
        cond = (l_max < r_min) & (t_max < b_min)
        ai = (r_min - l_max) * (b_min - t_max)
        ai = tf.where((cond & tf.logical_not(eye)), ai, tf.zeros_like(ai))
        ai = tf.where(a1 > 0.0, ai / a1, tf.zeros_like(ai))
        overlap = tf.reduce_sum(ai, axis=[-2, -1]) / count
        overlap = tf.where(count > 1, overlap, tf.zeros_like(overlap))

        # lb = data["type"]
        # label_match = (lb[..., tf.newaxis] == lb[:, tf.newaxis, :])

        # au = a1 + a2 - ai
        # iou = tf.where(au > 0.0, ai / au, tf.zeros_like(au))
        # cost = tf.fill(tf.shape(ai), 10000.0)
        # cost = tf.where(label_match & valid, 1.0 - iou, cost)
        # # cost = 1.0 - iou  # (0.0 is best, 1.0 is worst)
        # for i in range(B):
        #     score = 0.0
        #     # for (j, k) in linear_sum_assignment(cost[i]):
        #     #     score +=

        scores = {
            "alignment_num": tf.reduce_sum(alignment),
            "alignment_den": num_valid_documents,
            "overlap_num": tf.reduce_sum(overlap),
            "overlap_den": num_valid_documents,
        }
        return scores


class LossLayer(tf.keras.layers.Layer):
    def __init__(
        self,
        input_columns: Dict,
        name: str = "loss_layer",
        predict_context: bool = False,
        **kwargs,
    ):
        super().__init__(name=name, **kwargs)
        self._input_columns = input_columns
        self._valid_input_columns = get_valid_input_columns(input_columns)
        self._predict_context = predict_context

    def call(
        self,
        inputs,
        training=False,
        sort_flag: Union[bool, tf.Tensor] = None,
        ignore_sort: str = None,
    ):
        if tf.is_tensor(sort_flag):
            assert ignore_sort in ["gt", "pred", None]
            y_true_, y_pred_, mfp_masks = inputs
            if ignore_sort == "gt":
                y_true_sort = y_true_
            else:
                y_true_sort = sort_inputs(y_true_, self._valid_input_columns)

            y_pred_["length"] = y_true_["length"]
            if ignore_sort == "pred":
                y_pred_sort = y_pred_
            else:
                y_pred_sort = sort_inputs(
                    y_pred_, self._valid_input_columns, from_logits=True
                )

            y_true, y_pred = {}, {}
            for key in y_true_.keys():
                column = self._input_columns[key]
                if column.get("demo_only", False):
                    continue
                if column["is_sequence"]:
                    flag = sort_flag[:, tf.newaxis, tf.newaxis]
                    y_true[key] = tf.where(flag, y_true_sort[key], y_true_[key])
                    if column["type"] == "categorical":
                        flag = flag[:, tf.newaxis]
                    y_pred[key] = tf.where(flag, y_pred_sort[key], y_pred_[key])
                else:
                    if key in y_true_:
                        y_true[key] = y_true_[key]
                    if key in y_pred_:
                        y_pred[key] = y_pred_[key]
        else:
            y_true, y_pred, mfp_masks = inputs

        seq_mask = get_seq_mask(y_true["length"])

        loss_total = 0
        score_total = 0
        losses = {}
        scores = {}

        for key, column in self._input_columns.items():
            if column.get("demo_only", False):
                continue

            if not column["is_sequence"] and not self._predict_context:
                continue

            prediction = y_pred[key]
            # Cut extra elements in prediction.
            prediction = prediction[:, : tf.shape(seq_mask)[1]]

            if column["type"] == "categorical":
                # check if the labels are in intended range of values
                C = tf.cast(column["input_dim"], tf.int32)
                tf.debugging.assert_less_equal(tf.reduce_max(y_true[key]), C - 1)
                tf.debugging.assert_greater_equal(tf.reduce_min(y_true[key]), 0)

                y_true[key] = tf.cast(y_true[key], tf.int32)
                loss, score = compute_categorical_mfp_metric(
                    y_true[key], prediction, from_logits=True
                )
                # if key == 'font_size':
                #     score = mae_from_logits(y_true[key], prediction, from_logits=True)
            else:
                loss, score = compute_continuous_mfp_metric(y_true[key], prediction)
                loss = tf.expand_dims(loss, -1)
                loss = loss * tf.cast(column["shape"][-1], tf.float32)
                score = tf.expand_dims(score, -1)

            mfp_weight = tf.cast(mfp_masks[key][..., tf.newaxis], tf.float32)
            loss *= mfp_weight
            score *= mfp_weight
            den = tf.cast(tf.ones(tf.shape(loss)), tf.float32) * mfp_weight

            if "loss_condition" in column:
                cond = column["loss_condition"]
                weight = tf.gather(cond["mask"], y_true[cond["key"]])
                loss *= tf.cast(weight, tf.float32)
                score *= tf.cast(weight, tf.float32)
                den *= tf.cast(weight, tf.float32)

            if column["is_sequence"]:
                weight = tf.cast(seq_mask[:, :, tf.newaxis], tf.float32)
                loss = tf.reduce_sum(loss * weight, axis=1)  # sum timesteps
                score = tf.reduce_sum(score * weight, axis=1)
                den = tf.reduce_sum(den * weight, axis=1)

            loss = tf.reduce_sum(loss, axis=1)  # sum features
            score = tf.reduce_sum(score, axis=1)
            den = tf.reduce_sum(den, axis=1)

            tf.debugging.assert_rank(loss, 1)
            tf.debugging.assert_rank(score, 1)
            tf.debugging.assert_rank(den, 1)

            loss = tf.reduce_mean(loss)  # average batch

            score = tf.reduce_sum(score)
            den = tf.reduce_sum(den)
            normalized_score = tf.where(den == 0.0, 1.0, score / den)

            score_total += normalized_score

            self.add_metric(normalized_score, name=key + "_score")

            scores[key + "_score_num"] = score
            scores[key + "_score_den"] = den
            losses[key] = loss

        losses_normalized = losses  # currently no reweight operation

        for key, loss in losses_normalized.items():
            self.add_metric(loss, name=key + "_loss")
            loss_total += loss

        self.add_loss(loss_total)
        self.add_metric(score_total / len(self._input_columns), name="total_score")
        return [scores]


class LayoutMetricLayer(tf.keras.layers.Layer):
    """Compute Accuracy and mean IoU of the layout map."""

    def __init__(self, input_columns, from_logits=True, **kwargs):
        super().__init__(**kwargs)
        self._xsize = tf.cast(input_columns["left"]["input_dim"], tf.int32)
        self._ysize = tf.cast(input_columns["top"]["input_dim"], tf.int32)
        self._label_name = next(
            key for key, c in input_columns.items() if c["primary_label"] is not None
        )
        self._default_label = tf.cast(
            input_columns[self._label_name]["primary_label"], tf.int32
        )
        self._label_size = tf.cast(
            input_columns[self._label_name]["input_dim"], tf.int32
        )
        self._from_logits = from_logits
        assert input_columns["left"]["input_dim"] == input_columns["width"]["input_dim"]
        assert input_columns["top"]["input_dim"] == input_columns["height"]["input_dim"]

    def call(self, inputs, training=False):
        y_true, y_pred = inputs
        mask_true, mask_pred = self._get_seq_masks(y_true, y_pred, training)
        map_true = _compute_gridmaps(
            y_true,
            mask_true,
            from_logits=False,
            label_name=self._label_name,
            xsize=self._xsize,
            ysize=self._ysize,
            default_label=self._default_label,
        )
        map_pred = _compute_gridmaps(
            y_pred,
            mask_pred,
            from_logits=self._from_logits,
            label_name=self._label_name,
            xsize=self._xsize,
            ysize=self._ysize,
            default_label=self._default_label,
        )
        acc, miou = _compute_acc_miou(map_true, map_pred, self._label_size)
        self.add_metric(acc, name="layout_acc")
        self.add_metric(miou, name="layout_miou")
        return {"layout_acc": acc, "layout_miou": miou}

    def _get_seq_masks(self, y_true, y_pred, training):
        maxlen = tf.shape(y_true[self._label_name])[1]
        seq_mask_true = get_seq_mask(y_true["length"], maxlen=maxlen)
        if training:
            seq_mask_pred = seq_mask_true
        else:
            maxlen = tf.shape(y_pred[self._label_name])[1]
            seq_mask_pred = get_seq_mask(
                y_pred["length"],
                from_logits=self._from_logits,
                maxlen=maxlen,
            )
        tf.debugging.assert_rank(seq_mask_true, 2)
        tf.debugging.assert_rank(seq_mask_pred, 2)
        return seq_mask_true, seq_mask_pred


# @tf.function(experimental_relax_shapes=True)
def _compute_gridmaps(
    example,
    mask,
    from_logits,
    label_name,
    xsize,
    ysize,
    default_label,
):
    if from_logits:
        # Assume all categorical here.
        example = {
            key: tf.cast(
                tf.argmax(tf.stop_gradient(example[key]), axis=-1),
                tf.int32,
            )
            for key in ("left", "top", "width", "height", label_name)
        }
    else:
        example = {
            key: tf.cast(tf.stop_gradient(example[key]), tf.int32)
            for key in ("left", "top", "width", "height", label_name)
        }

    batch_size = tf.shape(mask)[0]
    gridmaps = tf.TensorArray(tf.int32, size=batch_size)
    for i in tf.range(batch_size):
        left = tf.reshape(example["left"][i][mask[i]], (-1,))
        top = tf.reshape(example["top"][i][mask[i]], (-1,))
        width = tf.reshape(example["width"][i][mask[i]], (-1,))
        height = tf.reshape(example["height"][i][mask[i]], (-1,))

        label = tf.cast(
            tf.reshape(example[label_name][i][mask[i]], (-1,)),
            tf.int32,
        )
        tf.assert_rank(left, 1)

        right = tf.minimum(xsize - 1, left + width)
        bottom = tf.minimum(ysize - 1, top + height)

        gridmap = _make_gridmap(
            left,
            top,
            right,
            bottom,
            label,
            ysize,
            xsize,
            default_label,
        )
        gridmaps = gridmaps.write(i, gridmap)
    return gridmaps.stack()


# @tf.function(experimental_relax_shapes=True)
def _make_gridmap(left, top, right, bottom, label, ysize, xsize, default_label):
    # Fill bbox region with the specified label.
    canvas = tf.fill((ysize, xsize), default_label)
    for j in tf.range(tf.shape(label)[0]):
        if top[j] >= bottom[j] or left[j] >= right[j]:
            continue
        y, x = tf.meshgrid(
            tf.range(top[j], bottom[j] + 1),
            tf.range(left[j], right[j] + 1),
        )
        indices = tf.stack([tf.reshape(y, (-1,)), tf.reshape(x, (-1,))], axis=1)
        updates = tf.fill((tf.shape(indices)[0],), label[j])
        canvas = tf.tensor_scatter_nd_update(canvas, indices, updates)
    return canvas


# @tf.function(experimental_relax_shapes=True)
def _compute_acc_miou(map_true, map_pred, label_size):
    batch_size = tf.shape(map_pred)[0]
    batch_index = tf.reshape(
        tf.tile(tf.range(batch_size)[:, tf.newaxis], [1, tf.size(map_pred[0])]),
        (-1,),
    )
    indices = tf.stack(
        [
            tf.cast(batch_index, tf.int32),
            tf.reshape(map_pred, (-1,)),
            tf.reshape(map_true, (-1,)),
        ],
        axis=1,
    )
    updates = tf.ones((tf.shape(indices)[0],), dtype=tf.int32)
    confusion = tf.cast(
        tf.scatter_nd(indices, updates, (batch_size, label_size, label_size)),
        tf.float32,
    )

    inter = tf.linalg.diag_part(confusion)
    union = tf.reduce_sum(confusion, axis=1) + tf.reduce_sum(confusion, axis=2) - inter

    # Compute accuracy
    acc = tf.math.truediv(
        tf.reduce_sum(inter, axis=1), tf.reduce_sum(confusion, axis=(1, 2))
    )

    # Compute nanmean of iou.
    weight = tf.cast(union > 0, tf.float32)
    iou = inter / (union + 1e-9)
    miou = tf.reduce_sum(weight * iou, axis=1) / tf.reduce_sum(weight, axis=1)
    return acc, miou


===== spec.py =====
import json
import logging
import os
from typing import Dict, List

import numpy as np
import tensorflow as tf
import yaml
from tensorflow.keras.layers.experimental import preprocessing
# from tensorflow.keras import preprocessing

from .discretizer import SequenceDiscretizer
# from discretizer import SequenceDiscretizer

logger = logging.getLogger(__name__)


def set_visual_default(decoded_data: Dict):
    for i in range(len(decoded_data["elements"])):
        decoded_data["elements"][i]["color"] = [0.0, 0.0, 0.0]
        decoded_data["elements"][i]["opacity"] = 1.0
        decoded_data["elements"][i]["font_family"] = "DummyFont"
    return decoded_data


class DataSpec(object):
    """
    Utility class to handle data schema.

    We assume the following directory structure::

        root/
        root/count.json
        root/vocabulary.json
        root/train-*-of-*.tfrecord
        root/val-*-of-*.tfrecord
        root/test-*-of-*.tfrecord

    Additionally, there must be a spec file in YAML format (loaded via name)::

        name: rico
        columns:
          column1:
            shape: []
            dtype: int64
          column2:
            is_sequence: true
            dtype: string
            lookup:
              num_oov_indices: 1
              mask_token: null

    Usage::

        dataspec = DataSpec('crello', '/path/to/tfrecords', batch_size=256)

        train_dataset = dataspec.make_dataset(
            'train', shuffle=True, cache=True)
        batch = next(iter(train_dataset))

        for item in dataspec.unbatch(batch):
            print(item)
    """

    def __init__(
        self,
        name,
        path,
        batch_size=8,
    ):
        self._path = path
        self._batch_size = batch_size

        spec_path = os.path.join(os.path.dirname(__file__), name + "-spec.yml")
        if os.path.exists(spec_path):
            name = spec_path
        self._spec = self._load_resource(spec_path, rel_path=False)
        self._splits = self._load_resource("count.json")
        self._init_preprocessor()

    @property
    def columns(self):
        return self._spec.get("columns", {})

    @property
    def preprocessor(self):
        return self._preprocessor

    def _init_preprocessor(self):
        # Initialize preprocessing functions.
        vocabulary = self._load_resource("vocabulary.json")

        self._preprocessor = {}
        for name, column in self.columns.items():
            if "lookup" in column:
                self._preprocessor[name] = self._create_lookup(name, column, vocabulary)
            elif "discretize" in column:
                spec = column["discretize"]
                boundaries = list(np.linspace(spec["min"], spec["max"], spec["bins"]))[
                    1:
                ]
                self._preprocessor[name] = SequenceDiscretizer(boundaries)
                logger.info("Discretizer for %s: bins=%s" % (name, len(boundaries) + 1))

    def _create_lookup(self, name, column, vocabulary):
        assert name in vocabulary or "vocabulary" in column["lookup"]
        layer_fn = {
            "string": preprocessing.StringLookup,
            "int64": preprocessing.IntegerLookup,
        }[column["dtype"]]

        if name in vocabulary:
            vocab = vocabulary[name]
        else:
            # Integer [min, max] vocabulary.
            min_value = column["lookup"]["vocabulary"]["min"]
            max_value = column["lookup"]["vocabulary"]["max"]
            vocab = list(range(min_value, max_value + 1))
        if isinstance(vocab, dict):
            vocab = [
                int(key) if column["dtype"] == "int64" else key
                for key, value in vocab.items()
                if value >= column.get("min_freq", 1)
            ]

        options = (
            {}
            if column["lookup"] is True
            else {k: v for k, v in column["lookup"].items() if k != "vocabulary"}
        )
        logger.info(
            "Lookup for %s: vocabulary_size=%s, options=%s"
            % (name, len(vocab), options)
        )

        return layer_fn(vocabulary=vocab, **options)

    def size(self, split):
        """Length of the records for the split."""
        return self._splits[split]

    def steps_per_epoch(self, split, batch_size=None):
        """Steps per epoch."""
        return int(np.ceil(self.size(split) / (batch_size or self._batch_size)))

    def make_input_columns(self):
        """Returns input specification for a model."""
        inputs = {}
        for key, column in self.columns.items():
            # Inspect categorical inputs and its size.
            layer = self._preprocessor.get(key)
            if column.get("demo_only", False):
                # for demo-only elements
                inputs[key] = {"demo_only": True}
            elif isinstance(layer, SequenceDiscretizer):
                inputs[key] = {
                    "type": "categorical",
                    "input_dim": len(layer.bin_boundaries) + 1,
                }
            elif isinstance(
                layer,
                (
                    preprocessing.StringLookup,
                    preprocessing.IntegerLookup,
                ),
            ):
                if tf.__version__.split(".")[1] in ("3", "4"):
                    vocabulary_size = layer.vocab_size()
                else:
                    vocabulary_size = layer.vocabulary_size()
                inputs[key] = {
                    "type": "categorical",
                    "input_dim": vocabulary_size,
                }
            elif column["dtype"] in ("int", "int32", "int64"):
                inputs[key] = {
                    "type": "categorical",
                    "input_dim": column["max"] + 1,  # Include zero.
                }
            elif column["dtype"] in ("float", "float32", "float64"):
                inputs[key] = {
                    "type": "numerical",
                }
            else:
                raise NotImplementedError

            inputs[key]["shape"] = tuple(column.get("shape", (1,)))
            inputs[key]["is_sequence"] = column.get("is_sequence", False)

            if "primary_label" in column:
                inputs[key]["primary_label"] = self._preprocessor[key](
                    column["primary_label"]["default"]
                )
            else:
                inputs[key]["primary_label"] = None

        for key, column in self.columns.items():
            if "loss_condition" in column:
                cond = column["loss_condition"]
                logger.info(
                    "Loss condition for %s: %s included in %s"
                    % (key, cond["key"], cond["values"])
                )
                mask = [
                    v in cond["values"]
                    for v in self._preprocessor[cond["key"]].get_vocabulary()
                ]
                inputs[key]["loss_condition"] = {
                    "key": cond["key"],
                    "mask": mask,
                }
                # inputs[key]["loss_condition"] = {
                #     "key": cond["key"],
                #     "mask": mask[1:],
                # }

        return inputs

    def make_dataset(
        self,
        split,
        batch_size=None,
        shuffle=None,
        repeat=False,
        prefetch=tf.data.experimental.AUTOTUNE,
        parallel=None,
        cache=None,
    ):
        assert split in self._splits, "split must be one of (%s)" % ", ".join(
            self._splits.keys()
        )
        if shuffle is True:
            shuffle = self.size(split)
        if parallel is None:
            parallel = tf.data.experimental.AUTOTUNE if shuffle else None

        file_pattern = os.path.join(self._path, split + "-*.tfrecord")
        logger.info("TFRecord from %s" % file_pattern)
        dataset = tf.data.Dataset.list_files(file_pattern, shuffle=shuffle)
        dataset = tf.data.TFRecordDataset(
            dataset,
            num_parallel_reads=parallel,
        )
        if cache:
            dataset = dataset.cache()
        if shuffle:
            dataset = dataset.shuffle(shuffle)
        if repeat:
            dataset = dataset.repeat()
        dataset = dataset.batch(batch_size or self._batch_size)
        dataset = dataset.map(
            self.parse_fn,
            num_parallel_calls=parallel,
            deterministic=(shuffle is False),
        )
        if prefetch:
            dataset = dataset.prefetch(prefetch)

        return dataset

    def parse_fn(self, serialized):
        context, sequence, _ = tf.io.parse_sequence_example(
            serialized,
            {
                name: tf.io.FixedLenFeature(
                    column.get("shape", (1,)),
                    column["dtype"],
                )
                for name, column in self.columns.items()
                if not column.get("is_sequence")
            },
            {
                name: tf.io.FixedLenSequenceFeature(
                    column.get("shape", (1,)),
                    column["dtype"],
                )
                for name, column in self.columns.items()
                if column.get("is_sequence")
            },
        )
        output = context
        output.update(sequence)

        for key, preprocess_fn in self._preprocessor.items():
            output[key] = preprocess_fn(output[key])

        # default data type is int64
        # string lookup returns int64 and discretization returns int32
        for key in output.keys():
            if output[key].dtype == tf.int64:
                output[key] = tf.cast(output[key], tf.int32)

        return output

    def logit_to_label(self, example):
        """Convert logit prediction to labels."""
        for key, column in self.columns.items():
            if column.get("demo_only", False):
                continue

            rank = 1 + column.get("is_sequence", 0) + len(column.get("shape", (1,)))
            if tf.rank(example[key]) >= rank + 1:
                example[key] = tf.cast(tf.argmax(example[key], axis=-1), tf.int32)
        return example

    def unbatch(self, example):
        """
        Convert a batch tensor example to a list of items for post-processing.

        Sequence items get stored in `elements` while others are in dict::

            items = [{key: value, 'elements': [{key: value}]}]
        """

        example = self.logit_to_label(example)
        batch_size = tf.shape(example["length"])[0]

        items = []
        for i in range(batch_size):
            # Find length.
            length = int(tf.squeeze(example["length"][i]) + 1)  # zero-based
            for name, column in self.columns.items():
                if column.get("is_sequence"):
                    length = min(length, tf.shape(example[name][i])[0])
                    break

            # Fill in items.
            item = {"elements": [{} for _ in range(length)]}
            for name, column in self.columns.items():
                x = example[name][i].numpy()

                # Un-preprocess.
                if "lookup" in column:
                    layer = self._preprocessor.get(name)
                    table = np.array(layer.get_vocabulary())
                    x = table[x]
                elif "discretize" in column:
                    spec = column["discretize"]
                    scale = (spec["max"] - spec["min"]) / (spec["bins"] - 1.0)
                    x = scale * x + spec["min"]

                if column.get("is_sequence"):
                    for j in range(length):
                        item["elements"][j][name] = (
                            x[j, :].tolist() if x.shape[1] > 1 else x[j, 0]
                        )
                else:
                    item[name] = x[0]
            items.append(item)
        return items

    def _load_resource(self, path, format=None, rel_path=True):
        """Load resource file."""
        format = format or os.path.splitext(path)[-1]
        format = format.replace(".", "").lower()

        if rel_path:
            path = os.path.join(self._path, path)
        logger.info("Loading resource at %s" % path)
        with tf.io.gfile.GFile(path) as f:
            if format == "json":
                return json.load(f)
            elif format in ("yml", "yaml"):
                return yaml.safe_load(f)
            else:
                logger.warning("Unsupported format: %s" % path)
                return f.read()


ATTRIBUTE_GROUPS = {
    "rico": {
        "type": ["type"],
        "pos": ["left", "top", "width", "height"],
        "attr": ["icon", "clickable", "text_button"],
    },
    "crello": {
        "type": ["type"],
        "pos": ["left", "top", "width", "height"],
        "attr": ["opacity", "color", "font_family"],
        "img": ["image_embedding"],
        "txt": ["text_embedding"],
    },
    # "pose":{
    #     "type": ["type"],
    #     "pos": ["left", "top", "width", "height"],
    #     "img": ["image_embedding"],
    # }
    "dataforfd": {
        "type": ["type"],
        "pos": ["left", "top", "width", "height"],
        "img": ["image_embedding"],
    }
}


def get_dataset_name(keys: List[str]):
    # if "clickable" in keys:
    #     dataset_name = "rico"
    # else:
    #     dataset_name = "crello"
    dataset_name="dataforfd"
    return dataset_name


def get_attribute_groups(keys: List[str]):
    dataset_name = get_dataset_name(keys)
    return ATTRIBUTE_GROUPS[dataset_name]


def get_valid_input_columns(input_columns: Dict, use_canvas: bool = False):
    outputs = {}
    for (key, column) in input_columns.items():
        if key == "length":
            continue
        if column.get("demo_only", False):
            continue
        if not column["is_sequence"] and not use_canvas:
            continue
        outputs[key] = column
    return outputs


===== cvae.py =====
from typing import Dict, Tuple

import tensorflow as tf
from mfp.models.architecture.utils import make_dense_options


class Head(tf.keras.layers.Layer):
    def __init__(
        self,
        latent_dim: int = 32,
        kl: float = 1.0,
        l2: float = None,
        compute_kl: bool = False,
        **kwargs,
    ):
        super().__init__()
        self.fc_mean = tf.keras.layers.Dense(
            units=latent_dim,
            **make_dense_options(l2),
        )
        self.fc_log_sigma = tf.keras.layers.Dense(
            units=latent_dim,
            **make_dense_options(l2),
        )
        self.kl = kl
        self.compute_kl = compute_kl

    def reparameterize(self, z_mean: tf.Tensor, z_log_sigma: tf.Tensor):
        epsilon = tf.random.normal(shape=tf.shape(z_log_sigma))
        return z_mean + tf.exp(0.5 * z_log_sigma) * epsilon

    def call(self, h: tf.Tensor, training: bool = False) -> Dict[str, tf.Tensor]:
        z_mean = self.fc_mean(h)
        z_log_sigma = self.fc_log_sigma(h)
        if training:
            z = self.reparameterize(z_mean, z_log_sigma)
        else:
            z = z_mean

        if training and self.compute_kl:
            # Compute KL divergence to normal distribution.
            kl_div = -0.5 * tf.reduce_mean(
                1 + z_log_sigma - tf.square(z_mean) - tf.exp(z_log_sigma)
            )
            self.add_loss(self.kl * kl_div)
            self.add_metric(kl_div, name="kl_divergence")

        return {"z": z, "z_mean": z_mean, "z_log_sigma": z_log_sigma}


class Prior(tf.keras.layers.Layer):
    def __init__(self, l2: float = None):
        super().__init__()
        latent_dim = 32
        self.fc = tf.keras.layers.Dense(
            units=latent_dim,
            activation="relu",
            **make_dense_options(l2),
        )
        self.head = Head(l2=l2)

    def call(self, h: tf.Tensor, training: bool = False) -> Dict[str, tf.Tensor]:
        return self.head(self.fc(h), training=training)


class MAPrior(tf.keras.layers.Layer):
    """
    It has separate models for each attribute
    """

    def __init__(
        self,
        input_columns: Dict,
        l2: float = None,
    ):
        super().__init__()
        self.input_columns = input_columns

        self.layers = {}
        for key in input_columns:
            self.layers[key] = Prior(l2=l2)

    def call(
        self,
        context: tf.Tensor,
        training: bool = False,
    ) -> Dict[str, Dict[str, tf.Tensor]]:
        outputs = {}
        for key, layer in self.layers.items():
            outputs[key] = layer(context, training=training)
        return outputs


class VAEEncoder(tf.keras.layers.Layer):
    def __init__(
        self,
        l2: float = None,
    ):
        super().__init__()
        dim_in, dim_out = 128, 32
        self.fc1 = tf.keras.layers.Dense(
            units=dim_in,
            **make_dense_options(l2),
        )
        self.fc2 = tf.keras.layers.Dense(
            units=dim_out,
            activation="relu",
            **make_dense_options(l2),
        )
        self.head = Head(l2=l2)

    def call(
        self, hidden: tf.Tensor, context: tf.Tensor, training: bool = False
    ) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor]:
        h = self.fc1(hidden)
        h = tf.concat([h, context], axis=-1)
        h = self.fc2(h)
        return self.head(h, training=training)


class MACVAEEncoder(tf.keras.layers.Layer):
    """
    It has separate models for each attribute
    """

    def __init__(
        self,
        input_columns: Dict,
        l2: float = None,
    ):
        super().__init__()
        self.input_columns = input_columns

        self.layers = {}
        for key in input_columns:
            self.layers[key] = VAEEncoder(l2=l2)

    def call(
        self,
        h_gts: Dict[str, tf.Tensor],
        context: tf.Tensor,
        training: bool = False,
    ) -> Dict[str, Dict[str, tf.Tensor]]:
        outputs = {}
        for key, layer in self.layers.items():
            outputs[key] = layer(h_gts[key], context, training=training)
        return outputs


class VAEDecoder(tf.keras.layers.Layer):
    def __init__(
        self,
        l2: float = None,
    ):
        super().__init__()
        latent_dim, dim_out = 128, 64
        self.model = tf.keras.Sequential(
            [
                tf.keras.layers.Dense(
                    units=latent_dim,
                    activation="relu",
                    **make_dense_options(l2),
                ),
                tf.keras.layers.Dense(
                    units=dim_out,
                    activation="relu",
                    **make_dense_options(l2),
                ),
            ]
        )

    def call(
        self, z: tf.Tensor, context: tf.Tensor, training: bool = False
    ) -> tf.Tensor:
        h = tf.concat([z, context], axis=-1)
        return self.model(h)


class MACVAEDecoder(tf.keras.layers.Layer):
    """
    It has separate models for each attribute
    """

    def __init__(
        self,
        input_columns: Dict,
        l2: float = None,
    ):
        super().__init__()
        self.input_columns = input_columns
        self.layers = {}
        for key in input_columns:
            self.layers[key] = VAEDecoder(l2=l2)

    def call(
        self,
        zs: Dict[str, tf.Tensor],
        context: tf.Tensor,
        training: bool = False,
    ) -> Dict[str, tf.Tensor]:
        outputs = {}
        for key, layer in self.layers.items():
            outputs[key] = layer(zs[key], context, training=training)
        return outputs


===== mfp.py =====
import logging
from typing import Dict, List, Optional

import tensorflow as tf
import tensorflow_probability as tfp
from mfp.data.spec import get_attribute_groups, get_dataset_name
from mfp.models.architecture.mask import get_seq_mask
from mfp.models.canvasvae import CanvasVAE
from mfp.models.layoutvae import LayoutVAE
from mfp.models.masking import (
    apply_token,
    elem_masking,
    feat_masking,
    filter_padding,
    get_task_names,
    random_masking,
)
from mfp.models.model import BART, AutoReg, Model, VanillaTransformer

from .metrics import LossLayer
from .tensor_utils import shuffle_inputs, sort_inputs

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)


# def load_weights(model: tf.keras.Model, weight_path: str):
#     model.compile(optimizer="adam")
#     logger.info(f"Loading: {weight_path}")
#     model.load_weights(weight_path)
#     return model


def get_task_cat_dist_sampler(task_names: List[str], masking_method: str):
    used_names = masking_method.split("_")
    probs = [1.0 if name in used_names else 0.0 for name in task_names]
    probs_total = sum(probs)
    assert probs_total > 0.0

    probs = [p / probs_total for p in probs]
    logger.info([item for item in zip(task_names, probs)])
    sampler = tfp.distributions.Categorical(logits=tf.math.log(probs))
    return sampler


def merge_inputs_and_prediction(inputs, input_columns, masks, prediction):
    for key, column in input_columns.items():
        if not column["is_sequence"]:
            # keep canvas attributes
            prediction[key] = inputs[key]
        elif key not in masks.keys():
            # demo only attributes
            continue
        elif column["type"] == "numerical":
            cond = masks[key][..., tf.newaxis]
            cond = tf.repeat(cond, tf.shape(prediction[key])[-1], axis=-1)
            prediction[key] = tf.where(cond, prediction[key], inputs[key])
        else:
            gt = tf.one_hot(inputs[key], depth=column["input_dim"])
            cond = masks[key][..., tf.newaxis, tf.newaxis]
            cond = tf.repeat(cond, tf.shape(gt)[-2], axis=-2)
            cond = tf.repeat(cond, tf.shape(gt)[-1], axis=-1)
            prediction[key] = tf.where(cond, prediction[key], gt)

    # copy unpredicted items for visualization
    for key, column in input_columns.items():
        if column.get("demo_only", False):
            prediction[key] = inputs[key]
    return prediction


def preprocess_for_test(inputs, input_columns, masks, tasks=None):
    seq_mask = get_seq_mask(inputs["length"])
    filtered_inputs = filter_padding(inputs, input_columns, seq_mask)

    modified_inputs = {}
    for key, column in input_columns.items():
        # don't mask variables defined for canvas
        if not column["is_sequence"]:
            modified_inputs[key] = filtered_inputs[key]
            continue

        modified_inputs[key] = apply_token(
            filtered_inputs[key], column, masks[key], "masked"
        )

    if tasks is None:
        # add dummy tensor
        tasks = tf.zeros(tf.shape(inputs["left"])[0])
    modified_inputs["task"] = tasks[..., tf.newaxis]

    return modified_inputs


def preprocess_for_train(
    inputs: Dict[str, tf.Tensor],
    input_columns: Dict,
    tasks: tf.Tensor,
    is_autoreg: bool = False,
    input_dtype: str = "set",
):
    tf.debugging.assert_rank(tasks, 1)
    attribute_groups = get_attribute_groups(input_columns.keys())

    if is_autoreg or input_dtype == "shuffled_set":
        inputs = shuffle_inputs(inputs)
    elif input_dtype == "sorted_set":
        inputs = sort_inputs(inputs, input_columns)

    seq_mask = get_seq_mask(inputs["length"])
    filtered_inputs = filter_padding(inputs, input_columns, seq_mask)

    data = []
    modified_inputs, masks = random_masking(filtered_inputs, input_columns, seq_mask)
    data.append(elem_masking(filtered_inputs, input_columns, seq_mask, is_autoreg))
    for attribute_group in attribute_groups.values():
        x = feat_masking(filtered_inputs, input_columns, seq_mask, attribute_group)
        data.append(x)

    for key in modified_inputs.keys():
        for i, (modified_inputs_tmp, masks_tmp) in enumerate(data):
            # cond = (method_probs_onehot[:, i + 1] == 1.0)
            cond = tasks == (i + 1)
            if input_columns[key]["is_sequence"]:
                cond = cond[..., tf.newaxis]

            modified_inputs[key] = tf.where(
                cond[..., tf.newaxis],
                modified_inputs_tmp[key],
                modified_inputs[key],
            )

            if input_columns[key]["is_sequence"]:
                masks[key] = tf.where(cond, masks_tmp[key], masks[key])

    # add task info.
    modified_inputs["task"] = tasks[..., tf.newaxis]
    return inputs, modified_inputs, masks


def iterative_decode(model, masks, inputs, input_columns, modified_inputs, num_iter):
    # MaskGIT-like decoding
    # NOTE: not optimal implementation, could be faster
    masks = masks.copy()
    seq_mask = get_seq_mask(inputs["length"])
    filtered_inputs = filter_padding(inputs, input_columns, seq_mask)
    categorical_keys = [
        k
        for k, v in input_columns.items()
        if v["is_sequence"] and v.get("type", None) == "categorical"
    ]
    num_masked = sum(masks[k].numpy().astype("int").sum(-1) for k in categorical_keys)
    num_update_per_iter = (num_masked / num_iter).round().astype("int")
    for i in range(num_iter):
        # predict masked fields
        outputs = model(modified_inputs, training=False)
        if i == 0:
            final_outputs = outputs

        # use top-k confident prediction
        confidence = {
            k: tf.where(
                masks[k],
                tf.reduce_mean(
                    tf.reduce_max(tf.nn.softmax(outputs[k], axis=-1), axis=-1),
                    axis=-1,
                ),  # mean(max_prob, -1); mean for "color" field
                0.0,
            )
            for k in categorical_keys
        }
        confidence_sorted = tf.sort(
            tf.concat([confidence[k] for k in categorical_keys], axis=-1),
            axis=-1,
            direction="DESCENDING",
        )
        threshold = tf.stack(
            [confidence_sorted[i, k] for i, k in enumerate(num_update_per_iter)]
        )

        # update filtered_inputs and mask
        for key in categorical_keys:
            pred = tf.argmax(outputs[key], axis=-1, output_type=tf.int32)
            update_field = (confidence[key] >= threshold) & (confidence[key] > 0)
            filtered_inputs[key] = tf.where(
                update_field[:, :, None], pred, filtered_inputs[key]
            )
            masks[key] = tf.where(masks[key] == update_field, False, masks[key])
            if i > 0:
                final_outputs[key] = tf.where(
                    update_field[:, :, None, None],
                    outputs[key],
                    final_outputs[key],
                )

        # update model input
        for key, column in input_columns.items():
            if column["is_sequence"]:
                modified_inputs[key] = apply_token(
                    filtered_inputs[key], column, masks[key], "masked"
                )

    # use last prediction for numerical fields
    for key in ["image_embedding", "text_embedding"]:
        final_outputs[key] = outputs[key]

    return final_outputs


class MFP(tf.keras.Model):
    """
    MFP trainer.
    """

    def __init__(
        self,
        input_columns: Dict,
        num_blocks: int = 4,
        block_type: str = "deepsvg",
        masking_method: str = "random",
        seq_type: str = "default",
        arch_type: str = "oneshot",
        context: Optional[str] = None,
        input_dtype: str = "set",
        name: str = "mfp",
        use_elemwise_noise: bool = False,
        **kwargs,  # keys are latent_dim, dropout, l2
    ):
        super().__init__(name=name)
        assert arch_type == "oneshot"
        self.arch_type = arch_type
        self.context = context
        self.input_dtype = input_dtype

        self.input_columns = {
            k: v for (k, v) in input_columns.items() if not v.get("demo_only", False)
        }

        self.is_autoreg = False if arch_type in ["oneshot", "canvasvae"] else True

        if arch_type.endswith("vae") and "kl" in kwargs:
            del kwargs["kl"]  # won't use it

        if arch_type == "oneshot":
            model_class = {
                "default": Model,
                "flat": VanillaTransformer,
            }[seq_type]
            self.model = model_class(
                input_columns=input_columns,
                num_blocks=num_blocks,
                block_type=block_type,
                context=context,
                input_dtype=input_dtype,
                use_elemwise_noise=use_elemwise_noise,
                **kwargs,
            )
        elif "vae" in arch_type:
            kl = kwargs.pop("kl") if "kl" in kwargs else 1.0
            model_class = {
                "layoutvae": LayoutVAE,
                "canvasvae": CanvasVAE,
            }[arch_type]

            self.model = model_class(
                input_columns=input_columns,
                num_blocks=num_blocks,
                block_type=block_type,
                input_dtype=input_dtype,
                kl=kl,
                **kwargs,
            )
        elif "autoreg" in arch_type:
            model_class = {
                "autoreg": AutoReg,
                "bart_autoreg": BART,
            }[arch_type]
            self.model = model_class(
                input_columns=input_columns,
                num_blocks=num_blocks,
                block_type=block_type,
                context=context,
                input_dtype=input_dtype,
                **kwargs,
            )
        else:
            raise NotImplementedError

        self.loss_layer = LossLayer(input_columns)

        self.task_names = get_task_names(input_columns)
        self.task_cat_dist = get_task_cat_dist_sampler(self.task_names, masking_method)
        if get_dataset_name(input_columns.keys()) == "rico":
            self.sort_pos = True
        else:
            self.sort_pos = False

    def call(self, inputs, training=False, demo_args=None):
        is_demo = True if demo_args else False
        B = tf.shape(inputs["left"])[0]
        tasks = self.task_cat_dist.sample(B)

        if is_demo:
            targets = inputs
            masks = demo_args["masks"]
            modified_inputs = preprocess_for_test(
                inputs,
                self.input_columns,
                masks,
                demo_args.get("tasks", tasks),
            )
        else:
            targets, modified_inputs, masks = preprocess_for_train(
                inputs,
                self.input_columns,
                tasks,
                is_autoreg=self.is_autoreg,
                input_dtype=self.input_dtype,
            )

        iter_decode = False
        if is_demo:
            num_iter = demo_args.get("num_iter", 1)
            iter_decode = num_iter > 1

        if iter_decode:
            outputs = iterative_decode(
                self.model, masks, inputs, self.input_columns, modified_inputs, num_iter
            )
        elif self.is_autoreg:
            outputs = self.model(modified_inputs, targets, masks, training)
        else:
            outputs = self.model(modified_inputs, training)

        if not is_demo:
            if self.sort_pos:
                ind = self.task_names.index("pos")
                self.loss_layer((targets, outputs, masks), training, (tasks == ind))
            else:
                self.loss_layer((targets, outputs, masks), training)

        outputs = merge_inputs_and_prediction(
            inputs, self.input_columns, masks, outputs
        )

        outputs["tasks"] = tasks
        return outputs


===== discretizer.py =====
import tensorflow as tf
from tensorflow.keras.layers.experimental import preprocessing
# from tensorflow.keras import preprocessing



class SequenceDiscretizer(tf.keras.layers.Layer):
    """
    Discretization wrapper for stable operation under variable shapes.
    """

    def __init__(self, bins, **kwargs):
        super().__init__(**kwargs)
        self.discretizer = preprocessing.Discretization(bins)

    def call(self, inputs):
        if tf.__version__.startswith("2.3"):
            # TF 2.3 behavior
            return self.discretizer(inputs)
        else:
            # TF 2.4 or later has unstable discretization behavior.
            inputs = tf.cast(inputs, tf.float32)
            shape = tf.shape(inputs)
            reshaped = tf.reshape(inputs, (-1, 1))
            outputs = self.discretizer(reshaped)
            return tf.reshape(outputs, shape)

    @property
    def bin_boundaries(self):
        if tf.__version__.split(".")[1] in ("3", "4"):
            return self.discretizer.bins
        else:
            return self.discretizer.bin_boundaries


===== __main__.py =====
if __name__ == "__main__":
    from .main import main

    main()


===== crello-images-spec.yml =====
name: crello-images
columns:
  image_hash:
    is_sequence: true
    dtype: string
  image_embedding:
    is_sequence: true
    shape: [512]
    dtype: float32


===== svg_crello.py =====
"""
Original implementation directly parsing crawled data.
"""

import logging
import math
import os
import pickle
import xml.etree.ElementTree as ET
from itertools import chain, groupby, repeat

from mfp.data.crello import schema

NS = {
    "svg": "http://www.w3.org/2000/svg",
    "xlink": "http://www.w3.org/1999/xlink",
    "xhtml": "http://www.w3.org/1999/xhtml",
}
ET.register_namespace("", NS["svg"])
ET.register_namespace("xlink", NS["xlink"])
ET.register_namespace("html", NS["xhtml"])

logger = logging.getLogger(__name__)

# DUMMY_TEXT = '''
# Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor
# incididunt ut labore et dolore magna aliqua.
# '''
DUMMY_TEXT = """
TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT
"""

PKL_DIR = f"{os.path.dirname(__file__)}/../../../../data/crello/pkls"


def load_fonts_css(path: str):
    """
    Load font-family to stylesheet rules mapping.
    Get css from
    """
    import tinycss

    parser = tinycss.make_parser("fonts3")
    stylesheet = parser.parse_stylesheet_file(path)
    faces = [
        {
            decl.name: decl.value.as_css().replace("_old", "")
            for decl in rule.declarations
        }
        for rule in stylesheet.rules
    ]
    return {
        face: list(it) for face, it in groupby(faces, lambda x: x.get("font-family"))
    }


class SVGBuilder(object):
    """
    Utility to generate SVG for visualization.

    Usage::

        dataspec = DataSpec(...)
        dataset = dataspec.make_dataset('val')
        example = next(iter(dataset))

        # Manual colormap.
        builder = SVGBuilder(
            'type',
            colormap={
                '': 'none',
                'svgElement': 'blue',
                'textElement': 'red',
                'imageElement': 'green',
                'maskElement': 'cyan',
                'coloredBackground': 'magenta',
                'videoElement': 'yellow',
            },
            max_width=144,
        )
        for item in dataspec.unbatch(example):
            svg = builder(item)

        # Auto colormap by preprocessor.
        builder = SVGBuilder(
            'component',
            preprocessor=dataspec.preprocessor,
            max_width=144,
        )
        for item in dataspec.unbatch(example):
            svg = builder(item)

    """

    def __init__(
        self,
        key=None,
        preprocessor=None,
        colormap=None,
        canvas_width=None,
        canvas_height=None,
        max_width=None,
        max_height=None,
        opacity=0.5,
        image_db=None,
        text_db=None,
        render_text=False,
        **kwargs,
    ):
        assert key
        self._key = key
        self._canvas_width = canvas_width or 256
        self._canvas_height = canvas_height or 256
        self._max_width = max_width
        self._max_height = max_height
        self._opacity = opacity
        self._render_text = render_text
        assert preprocessor or colormap
        if preprocessor is None or key == "color":
            self._colormap = colormap
        else:
            vocabulary = preprocessor[key].get_vocabulary()
            self._colormap = self._make_colormap(vocabulary, colormap)
        self._image_db = image_db
        self._text_db = text_db
        self.fonts = load_fonts_css(
            os.path.dirname(__file__) + "/../data/crello/fonts.css"
        )

    def __call__(self, document):
        canvas_width, canvas_height = self.compute_canvas_size(document)
        root = ET.Element(
            ET.QName(NS["svg"], "svg"),
            {
                "width": str(canvas_width),
                "height": str(canvas_height),
                "viewBox": "0 0 1 1",
                # 'style': 'background-color: #EEE',
                "style": "background-color: #FFF",
                "preserveAspectRatio": "none",
            },
        )

        doc_size = {
            "width": document["canvas_width"],
            "height": document["canvas_height"],
        }

        # # load pickled data
        id_ = document["id"].decode()
        pkl_file = f"{PKL_DIR}/{id_[:3]}/{id_}.pkl"
        with open(pkl_file, "rb") as f:
            pkl_data = pickle.load(f)
        pkl_elements = pkl_data.template[0].elements
        pkl_uuids = [e.uuid for e in pkl_elements]

        if len(pkl_elements) != len(document["elements"]):
            plen = len(pkl_elements)
            elen = len(document["elements"])
            logger.warning(f"#elements mismatch {plen},{elen} for pkl, tfr")

        # find one-to-one correspondense
        doc2pkl = {}
        for i, element in enumerate(document["elements"]):
            uuid_ = element["uuid"].decode()
            try:
                doc2pkl[i] = pkl_uuids.index(uuid_)
            except ValueError:
                logger.warning(f"Not found: {uuid_}")

        for i, element in enumerate(document["elements"]):
            if i not in doc2pkl:  # with very low prob. it cannot be found
                continue
            if self._key == "color":
                fill = "rgb(%g,%g,%g)" % tuple(map(int, element["color"]))
            else:
                fill = self._colormap.get(element[self._key], "none")

            image_url = ""
            if self._image_db:
                if (
                    element.get(self._image_db.condition["key"])
                    in self._image_db.condition["values"]
                ):
                    image_url = self._image_db.search(element[self._image_db.value])

            if self._text_db:
                if (
                    element.get(self._text_db.condition["key"])
                    in self._text_db.condition["values"]
                ):
                    text = self._text_db.search(element[self._text_db.value])
                else:
                    text = DUMMY_TEXT
            else:
                text = DUMMY_TEXT

            if image_url:
                node = self._make_image(root, element, image_url)
            elif self._render_text and element.get("type") == "textElement":
                node = self._make_text_element(
                    root,
                    element,
                    fill,
                    doc_size,
                    text,
                    pkl_elements[doc2pkl[i]],
                )
            else:
                node = self._make_rect(root, element, fill)

            title = ET.SubElement(node, ET.QName(NS["svg"], "title"))
            title.text = str(
                {
                    k: v
                    for k, v in element.items()
                    # if not (self._image_db and k == self._image_db.value)
                    # to filter out large array like image/text_embedding
                    if not isinstance(v, list)
                }
            )

        # get links for fonts
        style = ET.SubElement(root, "{%s}style" % NS["svg"])
        self._fill_stylesheet(root, style)

        return ET.tostring(root).decode("utf-8")

    def _fill_stylesheet(self, root, style):
        font_families = {
            text.get("font-family")
            for text in root.iter("{%s}text" % NS["svg"])
            if text.get("font-family") is not None
        }
        style.text = "\n".join(
            "@font-face { %s }" % " ".join("%s: %s;" % (key, item[key]) for key in item)
            for item in chain.from_iterable(
                self.fonts.get(family, []) for family in font_families
            )
        )

    def compute_canvas_size(self, document):
        canvas_width = document.get("canvas_width", self._canvas_width)
        canvas_height = document.get("canvas_height", self._canvas_height)
        scale = 1.0
        if self._max_width is not None:
            scale = min(self._max_width / canvas_width, scale)
        if self._max_height is not None:
            scale = min(self._max_height / canvas_height, scale)
        return canvas_width * scale, canvas_height * scale

    def _make_colormap(self, vocabulary, colormap=None):
        """
        Generate a colormap for the specified vocabulary list.
        """
        from matplotlib import cm

        vocab_size = len(vocabulary)
        cmap = cm.get_cmap(colormap or "tab20", vocab_size)
        return {
            label: "rgb(%g,%g,%g)" % tuple(int(x * 255) for x in c[:3])
            for label, c in zip(vocabulary, cmap(range(vocab_size)))
        }

    def _make_text_element(
        self, parent, element, fill, doc_size, text_str, pkl_element
    ):
        def _make_map(m, default_key=None):
            return chain.from_iterable(
                repeat(
                    (x.get("type", default_key), x["value"]),
                    x["endIndex"] - x["startIndex"],
                )
                for x in m
            )

        def _generate_spans(text, style_map):
            offset = 0
            for style, it in groupby(style_map):
                length = len(list(it)) + 1
                item = dict(style)
                item["text"] = text[offset : offset + length]
                yield item
                offset += length

        def _make_linespans(text, pkl_element):
            style_map = list(
                zip(
                    _make_map(pkl_element.colorMap, default_key="color"),
                    _make_map(pkl_element.boldMap, default_key="bold"),
                    _make_map(pkl_element.italicMap, default_key="italic"),
                )
            )
            br_inds = [i for (i, t) in enumerate(text) if t == "\n"]

            if pkl_element.lineMap is not None:
                default_line_map = []
            elif len(br_inds) == 0:
                default_line_map = [
                    {"startIndex": 0, "endIndex": len(pkl_element.text)}
                ]
            else:
                default_line_map = []
                start = 0
                for ind in br_inds:
                    default_line_map.append({"startIndex": start, "endIndex": ind - 1})
                    start = ind + 1
                default_line_map.append(
                    {"startIndex": start, "endIndex": len(text) - 1}
                )

            for line in pkl_element.lineMap or default_line_map:
                start = line["startIndex"]
                end = line["endIndex"] + 1

                line_text = text[start:end]
                line_style_map = style_map[start:end]
                yield _generate_spans(line_text, line_style_map)

        margin = element["height"] * 0.1  # To avoid unexpected clipping.
        container = ET.SubElement(
            parent,
            ET.QName(NS["svg"], "svg"),
            {
                "id": element["uuid"].decode(),
                "class": element["type"],
                "x": "%g" % (element["left"] or 0),
                "y": "%g" % ((element["top"] or 0) - margin),
                "width": "%g" % (element["width"]),
                "overflow": "visible",
            },
        )
        opacity = element.get("opacity", 1.0)
        if opacity < 1:
            container.set("opacity", "%g" % opacity)

        # in element filling, different type might be used
        # in that case, we should somehow feed default values
        font_size = (
            getattr(pkl_element, "fontSize", doc_size["height"]) / doc_size["height"]
        )
        text_align = getattr(pkl_element, "textAlign", "center")
        line_height = getattr(pkl_element, "lineHeight", 1.0)
        capitalize = getattr(pkl_element, "capitalize", False)
        underline = getattr(pkl_element, "underline", False)
        letter_spacing = getattr(pkl_element, "letterSpacing", doc_size["width"])
        if letter_spacing is None:
            letter_spacing = 0.0
        else:
            letter_spacing /= doc_size["width"]

        if not getattr(pkl_element, "lineMap", False):
            setattr(pkl_element, "lineMap", None)
        if not getattr(pkl_element, "colorMap", False):
            setattr(pkl_element, "colorMap", [])
        if not getattr(pkl_element, "boldMap", False):
            setattr(pkl_element, "boldMap", [])
        if not getattr(pkl_element, "italicMap", False):
            setattr(pkl_element, "italicMap", [])
        if not getattr(pkl_element, "text", False):
            setattr(pkl_element, "text", "a" * 1000)

        text = ET.SubElement(
            container,
            "{%s}text" % NS["svg"],
            {
                "font-size": "%g" % font_size,
                "font-family": element["font_family"],
                "letter-spacing": "%g" % letter_spacing,
            },
        )

        if underline:
            text.set("text-decoration", "underline")
        if pkl_element.angle is not None and pkl_element.angle != 0:
            # Note: Chromium clips the svg region.
            angle = 180 * (pkl_element.angle / math.pi)
            text.set(
                "transform",
                "rotate(%g, %g, %g)"
                % (angle, element["width"] / 2, element["height"] / 2),
            )
        x = {"left": "0", "center": "50%", "right": "100%"}[text_align]
        anchor = {"left": "start", "center": "middle", "right": "end"}[text_align]

        line_height = line_height * font_size

        # print('L343', fill)
        for index, line in enumerate(_make_linespans(text_str, pkl_element)):
            line_tspan = ET.SubElement(
                text,
                "{%s}tspan" % NS["svg"],
                {
                    "dy": "%g" % line_height,
                    "x": x,
                    "text-anchor": anchor,
                    "dominant-baseline": "central",
                },
            )
            if index == 0:
                text.set("y", "%g" % (margin))
                line_tspan.set("dy", "%g" % (line_height / 2))
            for span in line:

                def f(x):
                    # convert 'rgb(255,255,255)' to 'FFFFFF'
                    values = [
                        "{:02x}".format(int(s)).upper() for s in x[4:-1].split(",")
                    ]
                    return "".join(values)

                color = f(fill)

                # print('L359', span['color'])
                tspan = ET.SubElement(
                    line_tspan,
                    "{%s}tspan" % NS["svg"],
                    {
                        # 'fill': '#%s' % span['color'],
                        "fill": "#%s" % color,
                        "dominant-baseline": "central",
                    },
                )
                tspan.text = span["text"].strip()
                if span["bold"]:
                    tspan.set("font-weight", "bold")
                if span["italic"]:
                    tspan.set("font-style", "italic")
                if capitalize:
                    # Capitalize at the leaf span for Safari compatibility.
                    tspan.set("style", "text-transform: uppercase;")

        return container

    def _make_image(self, parent, element, image_url):
        return ET.SubElement(
            parent,
            ET.QName(NS["svg"], "image"),
            {
                "x": str(element["left"]),
                "y": str(element["top"]),
                "width": str(element["width"]),
                "height": str(element["height"]),
                ET.QName(NS["xlink"], "href"): image_url,
                "opacity": str(element.get("opacity", 1.0)),
                "preserveAspectRatio": "none",
            },
        )

    def _make_rect(self, parent, element, fill):
        return ET.SubElement(
            parent,
            ET.QName(NS["svg"], "rect"),
            {
                "x": str(element["left"]),
                "y": str(element["top"]),
                "width": str(element["width"]),
                "height": str(element["height"]),
                "fill": str(fill),
                "opacity": str(element.get("opacity", 1.0) * self._opacity),
            },
        )


===== rico-spec.yml =====
name: rico
columns:
  length:
    dtype: int64
    lookup:
      vocabulary:
        min: 1
        max: 50
      num_oov_indices: 0
      mask_value: null
  left:
    is_sequence: true
    dtype: float32
    discretize:
      min: 0.0
      max: 1.0
      bins: 64
  top:
    is_sequence: true
    dtype: float32
    discretize:
      min: 0.0
      max: 1.0
      bins: 64
  width:
    is_sequence: true
    dtype: float32
    discretize:
      min: 0.0
      max: 1.0
      bins: 64
  height:
    is_sequence: true
    dtype: float32
    discretize:
      min: 0.0
      max: 1.0
      bins: 64
  clickable:
    is_sequence: true
    dtype: int64
    max: 1
  type:
    is_sequence: true
    dtype: string
    lookup:
      num_oov_indices: 1
      mask_token: null
    primary_label:
      default: ''
  icon:
    is_sequence: true
    dtype: string
    min_freq: 500
    lookup:
      num_oov_indices: 1
      mask_token: null
  text_button:
    is_sequence: true
    dtype: string
    min_freq: 500
    lookup:
      num_oov_indices: 1
      mask_token: null


===== encoder.py =====
from typing import Dict, Union

import tensorflow as tf
from einops import rearrange
from mfp.data.spec import get_valid_input_columns
from mfp.models.architecture.mask import get_seq_mask
from mfp.models.architecture.transformer import PositionEmbedding
from mfp.models.architecture.utils import make_dense_options, make_emb_options
from mfp.models.masking import MASK_VALUE, NULL_VALUE, get_task_names

CONTEXT_NAMES = [None, "id", "canvas", "length", "canvas_add"]


class Encoder(tf.keras.layers.Layer):
    def __init__(
        self,
        input_columns: Dict,
        context: Union[str, None] = None,
        input_dtype: str = "set",
        use_elemwise_noise: bool = False,
        fusion: str = "add",
        latent_dim: int = 128,
        dropout: float = 0.1,
        l2: float = None,
        **kwargs,
    ):
        super().__init__(**kwargs)
        assert context in CONTEXT_NAMES
        # canvas_add: for CTX experiments, add canvas info. to each element in seq.

        self.input_columns = input_columns

        # to encode and aggregate canvas-level attributes
        self.use_canvas = context is not None and "canvas" in context
        self.use_elemwise_noise = use_elemwise_noise
        self.valid_input_columns = get_valid_input_columns(
            input_columns, self.use_canvas
        )

        self.context = context
        self.use_pos_token = True if input_dtype != "set" else False
        self.fusion = fusion
        self.latent_dim = latent_dim

        self.input_layer = {}

        # for shuffled sets or sequence
        if self.use_pos_token:
            self.input_layer["const"] = PositionEmbedding(
                latent_dim,
                self.input_columns["length"]["input_dim"],
                dropout=dropout,
                emb_options=make_emb_options(l2),
                name="input_const",
            )

        if self.use_elemwise_noise:
            self.noise_size = 4
            self.input_layer["noise_fc"] = tf.keras.layers.Dense(
                units=latent_dim,
                name="input_noise",
                **make_dense_options(l2),
            )

        # for global context features
        # initialize <CLS> token
        # w_init = tf.zeros_initializer()(
        #     shape=[1, 1, latent_dim], dtype=tf.float32
        # )
        # self.cls_token[key] = tf.Variable(initial_value=w_init, trainable=True)

        for key, column in self.valid_input_columns.items():
            if column["type"] == "categorical":
                self.input_layer[key] = tf.keras.layers.Embedding(
                    input_dim=column["input_dim"] + 2,
                    output_dim=latent_dim,
                    name="input_%s" % key,
                    **make_emb_options(l2),
                )
            elif column["type"] == "numerical":
                # used to embed <MASK> and <UNUSED> token
                self.input_layer["%s_special" % key] = tf.keras.layers.Embedding(
                    input_dim=2,
                    output_dim=latent_dim,
                    name="input_%s_special" % key,
                    **make_emb_options(l2),
                )
                self.input_layer[key] = tf.keras.layers.Dense(
                    units=latent_dim,
                    name="input_%s" % key,
                    **make_dense_options(l2),
                )
            else:
                raise ValueError("Invalid column: %s" % column)

        if self.context == "id":
            task_len = len(get_task_names(input_columns))
            self.input_layer["task"] = tf.keras.layers.Embedding(
                input_dim=task_len,
                output_dim=latent_dim,
                name="input_task",
                **make_emb_options(l2),
            )
        elif self.context == "length":
            self.input_layer["length"] = tf.keras.layers.Embedding(
                input_dim=input_columns["length"]["input_dim"],
                output_dim=latent_dim,
                name="input_task",
                **make_emb_options(l2),
            )

        assert fusion in ["add", "concat", "flat", "none"]
        if self.fusion == "concat":
            self.fusion = tf.keras.layers.Sequential(
                [
                    tf.keras.layers.Dense(
                        units=latent_dim,
                        name="fusion_fc",
                        **make_dense_options(l2),
                    ),
                    tf.keras.layers.LayerNormalization(),
                    tf.keras.layers.Dropout(dropout),
                ]
            )
        elif self.fusion == "flat":
            valid_feats = self.valid_input_columns.keys()
            maxlen = len(valid_feats)
            maxlen *= self.input_columns["length"]["input_dim"] + 1
            self.input_layer["emb_seq_pos"] = PositionEmbedding(
                latent_dim,
                self.input_columns["length"]["input_dim"] + 1,
                dropout=dropout,
                emb_options=make_emb_options(l2),
                name="input_emb_elem",
            )

            # valid_feats = self.valid_input_columns.keys()
            # self.input_layer["emb_feat"] = PositionEmbedding(
            #     latent_dim,
            #     len(valid_feats) + 1,
            #     # len(valid_feats),
            #     dropout=dropout,
            #     emb_options=make_emb_options(l2),
            #     name="input_emb_feat",
            # )

    def call(self, inputs: Dict, training: bool = False):
        B = tf.shape(inputs["length"])[0]
        # Sequence inputs.
        # Note that length is zero-based
        seq_mask = get_seq_mask(inputs["length"])

        # aggregate info for both canvas and sequence
        data_c, data_s, keys_c, keys_s = [], [], [], []
        for key, column in self.valid_input_columns.items():
            if column["type"] == "categorical":
                x = self.input_layer[key](inputs[key])
                # sum across multiple prediction targets (e.g., RGB)
                axis = 2 if column["is_sequence"] else 1
                x = tf.reduce_sum(x, axis=axis)
            else:
                # find vector corresponding to <MASK> and <UNUSED>,
                # and then retrieve dense embedding for both
                # see apply_token in mfp.models.masking
                is_masked = tf.math.reduce_all(inputs[key] == MASK_VALUE, axis=2)
                is_unused = tf.math.reduce_all(inputs[key] == NULL_VALUE, axis=2)
                masked_emb = self.input_layer["%s_special" % key](
                    tf.zeros(tf.shape(seq_mask))
                )
                unused_emb = self.input_layer["%s_special" % key](
                    tf.ones(tf.shape(seq_mask))
                )
                x = self.input_layer[key](inputs[key])
                x = tf.where(is_masked[..., tf.newaxis], masked_emb, x)
                x = tf.where(is_unused[..., tf.newaxis], unused_emb, x)

            # for global context features
            # cls_token = tf.tile(self.cls_token[key], [batch, 1, 1])
            # x = tf.concat([cls_token, x], axis=1)
            if column["is_sequence"]:
                data_s.append(x)
                keys_s.append(key)
            else:
                data_c.append(x)
                keys_c.append(key)

        if self.use_canvas:
            assert len(keys_c) > 0, (keys_s, keys_c)

        if self.fusion != "add":
            # did not implement unusual cases
            assert len(data_c) == 0

        if self.fusion == "add":
            seq, canvas = 0.0, 0.0
            for d in data_s:
                seq += d
            for d in data_c:
                canvas += d
        elif self.fusion == "flat":
            shape = tf.shape(inputs["left"])

            S = shape[1]
            F = len(data_s)
            D = self.latent_dim

            seq_mask = tf.repeat(seq_mask, F, axis=1)  # (B, S * F)
            seq = tf.concat([tf.expand_dims(d, axis=2) for d in data_s], axis=2)
            seq = tf.reshape(seq, (B, -1, D))  # (B, S * F, D)

            seq_ids = rearrange(tf.range(S * F), "s -> 1 s")
            seq += self.input_layer["emb_seq_pos"](seq_ids)  # (B, S * F, D)

            # elem_ids = (tf.range(S * F) // F)[:, tf.newaxis]  # (S * F, 1)
            # elem_emb = self.input_layer["emb_elem"](elem_ids)  # (S * F, 1, D)
            # elem_emb = tf.transpose(elem_emb, [1, 0, 2])  # (1, S * F, D)
            # feat_ids = (tf.range(S * F) % F)[:, tf.newaxis]  # (S * F, 1)
            # feat_emb = self.input_layer["emb_feat"](feat_ids)  # (S * F, 1, D)
            # feat_emb = tf.transpose(feat_emb, [1, 0, 2])  # (1, S * F, D)
            # seq = seq + elem_emb + feat_emb
        elif self.fusion == "none":
            seq = {}
            for k, v in zip(keys_s, data_s):
                seq[k] = v
        else:
            raise NotImplementedError

        if self.context == "canvas_add":
            canvas = rearrange(canvas, "b c -> b 1 c")
            seq += canvas
        elif self.context is not None:
            assert self.fusion == "add", self.fusion
            # add special token (currently including task information if available)
            if self.context == "id":
                task = inputs["task"]
                task = task[:, 0] if tf.rank(task).numpy() == 2 else task
                canvas = self.input_layer["task"](task)
            elif self.context == "length":
                length = inputs["length"]
                if tf.rank(length) == 2:
                    length = length[:, 0]
                canvas = self.input_layer["length"](length)
            elif self.context == "canvas":
                pass
            else:
                raise NotImplementedError
            canvas = rearrange(canvas, "b c -> b 1 c")
            seq = tf.concat([canvas, seq], axis=1)
            seq_mask = get_seq_mask(inputs["length"] + 1)

        if self.use_pos_token and not self.fusion == "flat":
            seq += self.input_layer["const"](seq_mask, training=training)

        if self.use_elemwise_noise:
            assert self.fusion == "add"
            shape = seq.shape[:2] + (self.noise_size,)
            noise = tf.random.normal(shape)
            seq += self.input_layer["noise_fc"](noise)

        if self.fusion == "none":
            for v in seq.values():
                tf.debugging.assert_rank(v, 3)
        else:
            tf.debugging.assert_rank(seq, 3)
        return seq, seq_mask


===== retrieve.py =====
import logging
from base64 import b64encode
from pathlib import Path
from typing import Any, Dict, Union

import faiss
import numpy as np
import tensorflow as tf
from mfp.data import DataSpec

logger = logging.getLogger(__name__)


class _Retriever(object):
    """Image retriever for visualization."""

    def __init__(
        self,
        path: Path,
        key: str,
        value: str,
        condition: Dict[str, Any] = None,
        dim: int = 512,
        # image_path=None,
        # **kwargs,
    ):
        self._path = path
        # self._dataspec = DataSpec("crello-images", path, **kwargs)
        self._dataspec = None
        self._key = key
        self._value = value
        self._condition = condition
        self._dim = dim

        #  or {
        #     "key": "type",
        #     "values": ("imageElement", "maskElement", "svgElement"),
        # }
        # self._image_path = image_path or os.path.join(self._path, "images")

    @property
    def key(self):
        return self._key

    @property
    def value(self):
        return self._value

    @property
    def condition(self):
        return self._condition

    def build(self, split="train"):
        """Build index."""
        logger.info("Fetching image embeddings...")
        dataset = self._dataspec.make_dataset(split)

        # Deduplicate entries.
        d = {}
        for batch in dataset:
            keys = tf.reshape(batch[self._key], (-1, tf.shape(batch[self._key])[-1]))
            values = tf.reshape(
                batch[self._value], (-1, tf.shape(batch[self._value])[-1])
            )
            for i in range(tf.shape(keys)[0]):
                d[keys[i, 0].numpy()] = values[i].numpy()

        # Build faiss index.
        logger.info("Building image index...")
        labels = np.array(list(d.keys()))
        data = np.stack(list(d.values()))
        db = faiss.IndexFlatL2(self._dim)
        db.add(data)

        self._labels = labels
        self._db = db

    def get_url(self, index: int):
        raise NotImplementedError

    def search(self, query, k=1):
        if not isinstance(query, np.ndarray):
            query = np.array([query], dtype=np.float32)

        _, index = self._db.search(query, k)
        urls = [self.get_url(i) for i in index[0].tolist()]
        if k == 1:
            return urls[0]
        return urls


class ImageRetriever(_Retriever):
    """Image retriever for visualization."""

    def __init__(
        self,
        path: Path,
        key: str = "image_hash",
        value: str = "image_embedding",
        condition: Dict[str, Any] = None,
        image_path: Path = None,
        dim: int = 512,
        **kwargs,
    ):
        super().__init__(path, key, value, condition, dim)
        self._dataspec = DataSpec("crello-images", path, **kwargs)
        if self._condition is None:
            self._condition = {
                "key": "type",
                "values": ("imageElement", "maskElement", "svgElement"),
            }
        self._image_path = image_path or self._path / "images"

    def get_url(self, index: int):
        label = self._labels[index]
        if label:
            return make_data_uri(self._image_path / (label.decode() + ".png"))
        return ""


class TextRetriever(_Retriever):
    """Text retriever for visualization."""

    def __init__(
        self,
        path: Path,
        key: str = "text_hash",
        value: str = "text_embedding",
        condition: Dict[str, Any] = None,
        text_path: Path = None,
        dim: int = 512,
        **kwargs,
    ):
        super().__init__(path, key, value, condition, dim)
        self._dataspec = DataSpec("crello-texts", path, **kwargs)
        if self._condition is None:
            self._condition = {
                "key": "type",
                "values": ("textElement",),
            }
        self._text_path = text_path or self._path / "texts"

    def get_url(self, index: int):
        label = self._labels[index]
        if label:
            url = self._text_path / (label.decode() + ".txt")
            with tf.io.gfile.GFile(str(url), "rb") as f:
                text = f.read()
            return text.decode()
        return ""


def make_data_uri(url: Union[str, Path], mime_type="image/png"):
    if isinstance(url, Path):
        url = str(url)
    with tf.io.gfile.GFile(url, "rb") as f:
        image_bytes = f.read()
    data = b64encode(image_bytes).decode("ascii")
    return "data:%s;base64,%s" % (mime_type, data)

===== rasterizer.py =====
import math
import xml.etree.ElementTree as ET
from pathlib import Path
from typing import List, Tuple

from selenium import webdriver
from selenium.webdriver.chrome.options import Options


def get_svg_size(input_path: Path) -> Tuple[int, int]:
    svg_root = ET.parse(input_path).getroot()
    canvas_width = math.ceil(float(svg_root.get("width")))
    canvas_height = math.ceil(float(svg_root.get("height")))
    return (canvas_width, canvas_height)


class Rasterizer:
    def __init__(self):
        options = Options()
        options.add_argument("--headless")
        options.add_argument("--hide-scrollbars")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        self.options = options

    def __call__(self, svg_path: Path, svg_img_path: Path, size: List[int]):
        assert len(size) == 2
        url = f"file://{str(svg_path.absolute())}"  # need full path
        driver = webdriver.Chrome(options=self.options)
        driver.set_window_size(*size)
        driver.get(url)
        driver.get_screenshot_as_file(str(svg_img_path))
        driver.quit()


===== train.py =====
import json
import logging
import os
import random

import numpy as np
import tensorflow as tf
from fsspec.core import url_to_fs
from mfp.data import DataSpec
from mfp.helpers.callbacks import get_callbacks
from mfp.models.mfp import MFP

logger = logging.getLogger(__name__)
os.environ["CUDA_VISIBLE_DEVICES"] = "1"  #  GPU 0

def train(args):
    logger.info(f"tensorflow version {tf.__version__}")
    # fix seeds for reproducibility and stable validation
    seed = args.seed
    tf.random.set_seed(seed)
    np.random.seed(seed)
    random.seed(seed)
    os.environ["PYTHONHASHSEED"] = str(seed)

    # strategy = tf.distribute.MirroredStrategy()
    fs, _ = url_to_fs(args.job_dir)
    if not fs.exists(args.job_dir):
        fs.makedir(args.job_dir)

    json_path = os.path.join(args.job_dir, "args.json")

    with fs.open(json_path, "w") as file_obj:
        json.dump(vars(args), file_obj, indent=2)
    checkpoint_dir = os.path.join(args.job_dir, "checkpoints")
    checkpoint_path = os.path.join(checkpoint_dir, "best.ckpt")

    dataspec = DataSpec(
        args.dataset_name,
        args.data_dir,
        batch_size=args.batch_size,
    )

    train_dataset = dataspec.make_dataset(
        "train",
        shuffle=True,
        repeat=True,
        cache=True,
    )
    val_dataset = dataspec.make_dataset("val", cache=True)
    # val_dataset = dataspec.make_dataset("test", cache=True)

    test_dataset = dataspec.make_dataset("test", cache=True)

    input_columns = dataspec.make_input_columns()
    model = MFP(
        input_columns,
        num_blocks=args.num_blocks,
        block_type=args.block_type,
        masking_method=args.masking_method,
        seq_type=args.seq_type,
        arch_type=args.arch_type,
        context=args.context,
        latent_dim=args.latent_dim,
        dropout=args.dropout,
        l2=args.l2,
        input_dtype=args.input_dtype,
    )

    if args.weights:
        logger.info("Loading %s" % args.weights)
        model.load_weights(args.weights)

    model.compile(
        optimizer=tf.keras.optimizers.Adam(
            learning_rate=args.learning_rate,
            clipnorm=1.0,
        ),
        run_eagerly=True,
    )

    # model.compile(
    # optimizer=tf.keras.optimizers.legacy.Adam(
    #     learning_rate=args.learning_rate,
    #     clipnorm=1.0,
    # ),
    # run_eagerly=True,
    # )

    model.fit(
        train_dataset,
        steps_per_epoch=dataspec.steps_per_epoch("train"),
        epochs=args.num_epochs,
        validation_data=val_dataset,
        validation_steps=dataspec.steps_per_epoch("val"),
        validation_freq=min(args.validation_freq, args.num_epochs),
        callbacks=get_callbacks(args, dataspec, checkpoint_path),
        verbose=args.verbose,
    )

    results = model.evaluate(test_dataset, batch_size=args.batch_size)
    for k, v in zip(model.metrics_names, results):
        print(k, v)

    # Save the last model.
    model_path = os.path.join(args.job_dir, "checkpoints", "final.ckpt")
    logger.info("Saving %s" % model_path)
    model.save_weights(model_path)


===== component_legend.json =====
{
  "Web View": {
    "rgb": [
      66, 
      166, 
      246
    ], 
    "hex": "#42A5F5"
  }, 
  "List Item": {
    "rgb": [
      256, 
      225, 
      179
    ], 
    "hex": "#FFE0B2"
  }, 
  "Multi-Tab": {
    "rgb": [
      256, 
      242, 
      118
    ], 
    "hex": "#FFF176"
  }, 
  "Input": {
    "rgb": [
      145, 
      203, 
      250
    ], 
    "hex": "#90CAF9"
  }, 
  "Button": {
    "rgb": [
      206, 
      221, 
      57
    ], 
    "hex": "#CDDC39"
  }, 
  "Slider": {
    "rgb": [
      206, 
      220, 
      221
    ], 
    "hex": "#CDDBDC"
  }, 
  "Background Image": {
    "rgb": [
      211, 
      20, 
      83
    ], 
    "hex": "#D21453"
  }, 
  "Advertisement": {
    "rgb": [
      13, 
      71, 
      162
    ], 
    "hex": "#0D47A1"
  }, 
  "Card": {
    "rgb": [
      216, 
      190, 
      227
    ], 
    "hex": "#D7BDE2"
  }, 
  "Bottom Navigation": {
    "rgb": [
      187, 
      104, 
      201
    ], 
    "hex": "#BA68C8"
  }, 
  "Modal": {
    "rgb": [
      0, 
      256, 
      205
    ], 
    "hex": "#00FFCC"
  }, 
  "On/Off Switch": {
    "rgb": [
      79, 
      196, 
      248
    ], 
    "hex": "#4FC3F7"
  }, 
  "Button Bar": {
    "rgb": [
      256, 
      206, 
      211
    ], 
    "hex": "#FFCDD2"
  }, 
  "Number Stepper": {
    "rgb": [
      175, 
      214, 
      130
    ], 
    "hex": "#AED581"
  }, 
  "Text": {
    "rgb": [
      74, 
      20, 
      141
    ], 
    "hex": "#4A148C"
  }, 
  "Map View": {
    "rgb": [
      226, 
      191, 
      232
    ], 
    "hex": "#E1BEE7"
  }, 
  "Checkbox": {
    "rgb": [
      256, 
      139, 
      101
    ], 
    "hex": "#FF8A65"
  }, 
  "Date Picker": {
    "rgb": [
      205, 
      102, 
      154
    ], 
    "hex": "#CC6699"
  }, 
  "Image": {
    "rgb": [
      241, 
      98, 
      147
    ], 
    "hex": "#F06292"
  }, 
  "Drawer": {
    "rgb": [
      103, 
      58, 
      184
    ], 
    "hex": "#673AB7"
  }, 
  "Video": {
    "rgb": [
      0, 
      205, 
      0
    ], 
    "hex": "#00CC00"
  }, 
  "Toolbar": {
    "rgb": [
      77, 
      209, 
      226
    ], 
    "hex": "#4DD0E1"
  }, 
  "Pager Indicator": {
    "rgb": [
      249, 
      188, 
      209
    ], 
    "hex": "#F8BBD0"
  }
}

===== callbacks.py =====
import gc
import logging
import os
from datetime import datetime, timezone

import tensorflow as tf

logger = logging.getLogger(__name__)

class MetricLogger(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs=None):
        if logs:
            print(f"Epoch {epoch} metrics: {list(logs.keys())}")

class HyperTune(tf.keras.callbacks.Callback):
    """Callback for HyperTune on AI Platform."""

    def __init__(self, metric, tag=None, logdir=None, **kwargs):
        super().__init__(**kwargs)
        self._metric = metric
        self._tag = tag or "training/hptuning/metric"
        self._logdir = logdir or "/tmp/hypertune/output.metrics"
        self._writer = tf.summary.create_file_writer(self._logdir)

    def on_epoch_end(self, epoch, logs=None):
        if logs and self._metric in logs:
            with self._writer.as_default():
                tf.summary.scalar(self._tag, logs[self._metric], step=epoch)
            now = datetime.now(timezone.utc).astimezone().isoformat()
            print(f"{now} {self._tag} = {logs['val_loss']}")


class GarbageCollector(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs=None):
        gc.collect()
        tf.keras.backend.clear_session()


def get_callbacks(args, dataspec, checkpoint_path: str):
    log_dir = os.path.join(args.job_dir, "logs")
    if tf.io.gfile.exists(log_dir):
        logger.warning("Overwriting log dir: %s" % log_dir)
        tf.io.gfile.rmtree(log_dir)

    logger.info(f"checkpoint_path={checkpoint_path}")
    logger.info(f"log_dir={log_dir}")

    tensorboard = tf.keras.callbacks.TensorBoard(
        log_dir=log_dir,
        write_graph=False,
        profile_batch=2 if args.enable_profile else 0,
    )
    # checkpoint = tf.keras.callbacks.ModelCheckpoint(
    #     checkpoint_path,
    #     save_weights_only=True,
    #     monitor="val_total_score",
    #     mode="max",
    #     save_best_only=True,
    #     verbose=1,
    # )
    checkpoint = tf.keras.callbacks.ModelCheckpoint(
        checkpoint_path,
        save_weights_only=True,
        monitor="val_loss",  #  val_loss 
        mode="min",          #  min
        save_best_only=True,
        verbose=1,
    )

    terminate_on_nan = tf.keras.callbacks.TerminateOnNaN()
    gc = GarbageCollector()
    callbacks_list = [tensorboard, checkpoint, terminate_on_nan, gc]
    return callbacks_list


===== datatest.py =====

from spec import DataSpec


dataspec = DataSpec('crello', '/home/usr/dell/Project-HCL/BaseLine/flex-dm/data/crello', batch_size=16)

# dataspec = DataSpec('dataforfd', '/storage/homes/hzj/hzj/Project-HCL/BaseLine/flex-dm/data/DataForFlexDm/tfrecords_output_v5', batch_size=16)


train_dataset = dataspec.make_dataset(
    'train', shuffle=True, cache=True)


for _ in range(10):
    batch = next(iter(train_dataset))

        # Print sample
    print("\nSample data:")
    for item in dataspec.unbatch(batch):
        print(f"Sample ID: {item['id']}")
        print(f"Canvas: {item['canvas_width']}x{item['canvas_height']}")
        print(f"Elements: {len(item['elements'])}")
        for i, elem in enumerate(item['elements'][:3]):  # Show first 3 elements
            print(f"  Element {i}: {elem['type']} at ({elem['left']:.3f}, {elem['top']:.3f})")
            print(f"    Size: {elem['width']:.3f} x {elem['height']:.3f}")
        if len(item['elements']) > 3:
            print(f"  ... and {len(item['elements']) - 3} more elements")
        break



# for item in dataspec.unbatch(batch):
#     print(item)

===== canvasvae.py =====
from typing import Dict, Optional

import tensorflow as tf
import tensorflow_probability as tfp
from einops import rearrange
from mfp.data.spec import get_valid_input_columns
from mfp.models.architecture.cvae import Head
from mfp.models.architecture.decoder import Decoder
from mfp.models.architecture.encoder import Encoder
from mfp.models.architecture.mask import Unmask, get_seq_mask
from mfp.models.architecture.transformer import Blocks, PositionEmbedding
from mfp.models.architecture.utils import make_dense_options, make_emb_options

MND = tfp.distributions.MultivariateNormalDiag


class CanvasVAE(tf.keras.layers.Layer):
    def __init__(
        self,
        input_columns: Dict,
        num_blocks: int = 4,
        block_type: str = "deepsvg",
        context: Optional[str] = "length",
        input_dtype: str = "set",
        kl: float = 1e-0,
        **kwargs,  # keys are latent_dim, dropout, l2
    ):
        super().__init__()
        assert context == "length"
        assert input_dtype == "sorted_set"

        # assert "l2" in kwargs and "latent_dim" in kwargs
        l2 = kwargs.get("l2", None)
        dropout = kwargs.get("dropout", 0.0)

        self.kl = kl
        self.input_columns = input_columns
        self.valid_input_columns = get_valid_input_columns(input_columns, False)

        self.encoder = Encoder(
            input_columns, context=context, input_dtype=input_dtype, **kwargs
        )
        self.decoder = Decoder(input_columns, **kwargs)

        self.enc_blocks = Blocks(
            num_blocks=num_blocks // 2,
            block_type=block_type,
            lookahead=True,
            conditional=True,
            **kwargs,
        )
        self.prior_head = Head(**kwargs, compute_kl=True)
        self.norm = tf.keras.layers.BatchNormalization()
        self.relu = tf.keras.layers.Activation("relu")
        self.pooling = tf.keras.layers.GlobalAveragePooling1D()
        self.unmask = Unmask()

        self.blocks = Blocks(
            num_blocks=num_blocks // 2,
            block_type=block_type,
            lookahead=True,
            conditional=True,
            **kwargs,
        )
        self.length_fc = tf.keras.layers.Dense(
            input_columns["length"]["input_dim"], **make_dense_options(l2)
        )
        self.length_loss_func = tf.keras.losses.SparseCategoricalCrossentropy(
            from_logits=True,
        )

        self.embedding_const = PositionEmbedding(
            kwargs["latent_dim"],
            self.input_columns["length"]["input_dim"],
            dropout=dropout,
            emb_options=make_emb_options(l2),
            name="embedding_const",
        )

    def call(
        self,
        inputs: Dict,
        training: bool,
    ):
        # note that the element in seq. is for canvas attributes
        h_masked, enc_mask = self.encoder(inputs, training=training)
        canvas = h_masked[:, 0]
        sequence = h_masked[:, 1:]
        enc_mask = enc_mask[:, 1:]
        B = enc_mask.shape[0]
        h = self.enc_blocks((sequence, canvas), enc_mask, training=training)

        # aggregate latent codes and sample
        pooled = self.norm(sequence, training=training)
        pooled = self.pooling(self.relu(pooled))  # (B, S, D) -> (B, D)
        pooled = self.unmask(pooled)
        z = self.prior_head(pooled, training=training)["z"]

        # get the length of sequence at first
        if training:
            length_logits = self.length_fc(z)
            length_loss = self.length_loss_func(inputs["length"], length_logits)
            self.add_loss(length_loss)
            self.add_metric(length_loss, name="length_loss")

            # At training, use the supplied GT mask.
            mask = get_seq_mask(inputs["length"])
        else:
            length_pred = tf.argmax(self.length_fc(z), axis=1)
            maxlen = tf.reduce_max(inputs["length"]) + 1
            mask = get_seq_mask(rearrange(length_pred, "b -> b 1"), maxlen=maxlen)

        sequence = self.embedding_const(mask, training=training)
        h = self.blocks((sequence, z), mask, training=training)
        outputs = self.decoder(h, training=training)
        return outputs


===== decoder.py =====
from typing import Dict, Union

import tensorflow as tf
from mfp.data.spec import get_valid_input_columns
from mfp.models.architecture.mask import get_seq_mask
from mfp.models.architecture.utils import make_dense_options


class Decoder(tf.keras.layers.Layer):
    """Multi-way head for decoders."""

    def __init__(
        self,
        input_columns: Dict,
        context: Union[str, None] = None,
        detachment: str = "default",
        latent_dim: int = 256,
        dropout: float = 0.1,
        l2: float = None,
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.input_columns = input_columns
        self.context = context
        self.use_canvas = context == "canvas"
        self.detachment = detachment
        self.latent_dim = latent_dim
        self.valid_input_columns = get_valid_input_columns(
            input_columns, self.use_canvas
        )

        self.decoders = {}
        for key, column in self.valid_input_columns.items():
            if column["type"] == "categorical":
                units = column["shape"][-1] * column["input_dim"]
            else:
                units = column["shape"][-1]

            self.decoders[key] = tf.keras.layers.Dense(
                units,
                name="decoder_%s" % key,
                **make_dense_options(l2),
            )

        # def compute_mask(self, z, mask=None):
        #     """Compute mask according to Keras specification."""
        #     if isinstance(z, tuple):
        #         _, z = z
        #         seq_mask = get_seq_mask(inputs['length'])
        #     else:
        #         seq_mask = self.predict_mask(z)
        #     tf.debugging.assert_rank(seq_mask, 2)

        #     outputs = {}
        #     for key, column in self.input_columns.items():
        #         if column['is_sequence']:
        #             outputs[key] = seq_mask
        #         else:
        #             outputs[key] = None
        #     return outputs

        assert detachment in ["default", "flat", "none"]
        if self.context is not None:
            assert detachment == "default"
        if self.detachment == "flat":
            self.valid_keys = self.valid_input_columns.keys()

    def predict_mask(self, z):
        length_logit = self.decoders["length"](z)
        return get_seq_mask(length_logit, from_logits=True)

    def call(self, inputs: tf.Tensor, training: bool = False):
        """Take a sequence of transformed embeddings and compute outputs."""
        if self.context in ["id", "length", "canvas"]:
            canvas = inputs[:, :1]  # for global tasks (e.g., classification)
            seq = inputs[:, 1:]
        else:
            seq = inputs

        if self.use_canvas:
            # raise NotImplementedError
            pass

        if self.detachment == "flat":
            keys = self.valid_keys
            B = tf.shape(seq)[0]
            seq = tf.reshape(seq, (B, -1, len(keys), self.latent_dim))
            seq = tf.split(seq, len(keys), axis=2)
            seq = {k: tf.squeeze(v, axis=2) for (k, v) in zip(keys, seq)}
        elif self.detachment == "none":
            B = tf.shape(inputs["left"])[0]
        else:
            B = tf.shape(seq)[0]

        # Predict output for each head.
        outputs = {}
        for key, column in self.valid_input_columns.items():
            if column["type"] == "categorical":
                shape = (column["shape"][-1], column["input_dim"])
            else:
                shape = (column["shape"][-1],)

            if column["is_sequence"]:
                input_ = seq if self.detachment == "default" else seq[key]
                outputs[key] = tf.reshape(self.decoders[key](input_), (B, -1) + shape)
                tf.debugging.assert_rank_at_least(outputs[key], 3)
            else:
                input_ = canvas
                outputs[key] = tf.reshape(self.decoders[key](input_), (B,) + shape)
                tf.debugging.assert_rank_at_least(outputs[key], 2)
        return outputs


===== masking.py =====
from typing import Any, Dict, List, Tuple

import numpy as np
import tensorflow as tf
import tensorflow_probability as tfp
from mfp.data.spec import get_attribute_groups

MASK_VALUE = 10.0
NULL_VALUE = 0.0

MASK_PROB = 0.15
REPLACE_PROB = 0.1
UNCHANGE_PROB = 0.1
CHANGE_PROB = 1.0 - UNCHANGE_PROB
THRESH = REPLACE_PROB / CHANGE_PROB


def get_task_names(input_columns):
    task_names = ["random", "elem"]
    task_names += list(get_attribute_groups(input_columns.keys()).keys())
    return task_names


def filter_padding(
    inputs: Dict[str, tf.Tensor], input_columns: Dict, mask: tf.Tensor
) -> Dict[str, tf.Tensor]:
    modified_inputs = {}

    # to set [NULL] for padding caused by making minibatch
    # from variable-length elements
    unused_mask = tf.logical_not(mask)

    for key, column in input_columns.items():
        input_ = inputs[key]
        if column["is_sequence"]:
            # to set [NULL] for invalid data
            # (e.g., TextElement does not have image_embedding)
            if "loss_condition" in column:
                cond = column["loss_condition"]
                mask_ = tf.fill(tf.shape(mask), False)
                for i, flag in enumerate(cond["mask"]):
                    if not flag:
                        mask_ = tf.math.logical_or(
                            mask_, (inputs[cond["key"]] == i)[..., 0]
                        )
                mask_ = tf.logical_or(mask_, unused_mask)
            else:
                mask_ = unused_mask
            modified_inputs[key] = apply_token(input_, column, mask_, "unused")
        else:
            modified_inputs[key] = input_

    return modified_inputs


def get_initial_masks(input_columns: Dict, mask: tf.Tensor) -> Dict[str, tf.Tensor]:
    # returning masks with all False
    masks = {}
    for key, column in input_columns.items():
        # don't mask variables defined for canvas
        if not column["is_sequence"]:
            masks[key] = tf.fill(tf.shape(mask)[:1], True)
        else:
            masks[key] = tf.fill(tf.shape(mask), False)
    return masks


def apply_token(
    input_: tf.Tensor, column: Dict[str, Any], mask: tf.Tensor, token_type: str
) -> tf.Tensor:

    # MASK_VALUE = 10.0
    # NULL_VALUE = 0.0

    assert token_type in ["masked", "unused", "random"]
    tf.debugging.assert_equal(tf.rank(mask), 2)
    tf.debugging.assert_equal(tf.rank(input_), 3)

    mask = mask[..., tf.newaxis]
    shape = tf.shape(input_)

    if column["type"] == "categorical":
        x = tf.cast(mask, dtype=tf.int32)
        data = {
            "masked": column["input_dim"],
            "unused": column["input_dim"] + 1,
            "random": tf.random.uniform(shape, 0, column["input_dim"], dtype=tf.int32),
        }
        output = input_ * (1 - x) + data[token_type] * x
    else:
        x = tf.cast(mask, dtype=tf.float32)
        data = {
            "masked": MASK_VALUE,
            "unused": NULL_VALUE,
            "random": tf.random.normal(shape, stddev=0.1),
        }
        output = input_ * (1.0 - x) + data[token_type] * x

    return output


def select_single_element(mask: tf.Tensor, select_last: bool = False) -> tf.Tensor:
    """
    Select a single element for each sample.
    If mask is all False, then return an array filled with False
    For autoregressive models, always return the last valid element
    """
    tf.debugging.assert_rank(mask, 2)  # (B, S)

    length = tf.cast(tf.reduce_sum(tf.cast(mask, tf.int64), axis=1), tf.float32)
    if select_last:
        arr = tf.cast(length - 1, tf.int32)
    else:
        arr = tf.cast(tf.random.uniform(tf.shape(mask)[:1]) * length, tf.int32)
    new_mask = tf.cast(tf.one_hot(arr, depth=tf.shape(mask)[1]), tf.bool)
    new_mask = new_mask & (length > 0.0)[:, tf.newaxis]
    return new_mask


def feat_masking(
    inputs: Dict[str, tf.Tensor],
    input_columns: Dict,
    mask: tf.Tensor,
    feat_group: List[str],
) -> Tuple[Dict[str, tf.Tensor], Dict[str, tf.Tensor]]:
    modified_inputs = {}
    for key in inputs.keys():
        modified_inputs[key] = tf.identity(inputs[key])

    masks = get_initial_masks(input_columns, mask)

    for key in feat_group:
        column = input_columns[key]
        modified_inputs[key] = apply_token(modified_inputs[key], column, mask, "masked")
        masks[key] = mask

    return modified_inputs, masks


def elem_masking(
    inputs: Dict[str, tf.Tensor],
    input_columns: Dict,
    mask: tf.Tensor,
    is_autoreg: bool = False,
) -> Tuple[Dict[str, tf.Tensor], Dict[str, tf.Tensor]]:
    # modifing a specific element of all the features in a sequence
    masks = get_initial_masks(input_columns, mask)
    selected_mask = select_single_element(mask, is_autoreg)

    modified_inputs = {}
    for key, column in input_columns.items():
        if not column["is_sequence"]:
            modified_inputs[key] = inputs[key]
        else:
            modified_inputs[key] = apply_token(
                inputs[key], column, selected_mask, "masked"
            )
            masks[key] = selected_mask
    return modified_inputs, masks


def unused_masking(
    inputs: Dict[str, tf.Tensor],
    input_columns: Dict,
    masks: Dict[str, tf.Tensor],
    drop_ratio: float = 0.1,
) -> Tuple[Dict[str, tf.Tensor], Dict[str, tf.Tensor]]:
    dist = tfp.distributions.Bernoulli(probs=drop_ratio)

    modified_inputs = {}
    modified_masks = {}
    for key, column in input_columns.items():
        if not column["is_sequence"]:
            modified_masks[key] = masks[key]
            modified_inputs[key] = inputs[key]
            continue

        is_masked = masks[key]  # (B, S)
        is_unused = tf.cast(dist.sample(tf.shape(is_masked)[:1]), tf.bool)
        is_unused = is_unused[:, tf.newaxis, tf.newaxis]
        modified_masks[key] = tf.logical_and(is_masked, tf.logical_not(is_unused))
        modified_inputs[key] = apply_token(inputs[key], column, is_unused, "unused")

    return modified_inputs, masks


def rowcol_random_masking(
    inputs: Dict[str, tf.Tensor],
    input_columns: Dict,
    mask: tf.Tensor,
) -> Tuple[Dict[str, tf.Tensor], Dict[str, tf.Tensor]]:
    modified_inputs = {}
    masks = {}

    B = tf.shape(inputs["left"])[0]
    S = tf.shape(inputs["left"])[1]
    F = len(input_columns.values())
    p = MASK_PROB / 2.0
    col_mask = tf.random.uniform((B, S), minval=0.0, maxval=1.0) < p
    row_mask = tf.random.uniform((B, F), minval=0.0, maxval=1.0) < p

    for i, (key, column) in enumerate(input_columns.items()):
        # don't mask variables defined for canvas
        if not column["is_sequence"]:
            modified_inputs[key] = inputs[key]
            masks[key] = tf.fill(tf.shape(inputs[key]), True)
            continue

        # merge X-wise mask, and the latter steps are exactly the same as random
        mfp_mask = mask & (col_mask | row_mask[:, i : i + 1])

        # 80% mask, 10% random token, 10% unchanged
        chg_mask = mfp_mask & (
            tf.random.uniform(tf.shape(mfp_mask), minval=0.0, maxval=1.0) < CHANGE_PROB
        )
        rand_arr = tf.random.uniform(tf.shape(chg_mask), minval=0.0, maxval=1.0)
        masked_input = apply_token(
            inputs[key], column, chg_mask & (rand_arr >= THRESH), "masked"
        )
        masked_input = apply_token(
            masked_input, column, chg_mask & (rand_arr < THRESH), "random"
        )

        # update input
        modified_inputs[key] = masked_input
        masks[key] = mfp_mask

    return modified_inputs, masks


def random_masking(
    inputs: Dict[str, tf.Tensor],
    input_columns: Dict,
    mask: tf.Tensor,
) -> Tuple[Dict[str, tf.Tensor], Dict[str, tf.Tensor]]:
    """
    Like standard MLM training, do some operations for 15% of the tokens
    # 80% mask, 10% random token, 10% unchanged
    """

    # run in eager mode because of random sampling outside tf function
    modified_inputs = {}
    masks = {}

    # for random masking
    for key, column in input_columns.items():
        # don't mask variables defined for canvas
        if not column["is_sequence"]:
            modified_inputs[key] = inputs[key]
            masks[key] = tf.fill(tf.shape(inputs[key]), True)
            continue

        # create mask with shape (B, S) while ignoring padded region
        rand_arr = tf.random.uniform(tf.shape(inputs[key])[:-1], minval=0.0, maxval=1.0)
        mfp_mask = mask & (rand_arr < MASK_PROB)

        # 80% mask, 10% random token, 10% unchanged
        chg_mask = mfp_mask & (
            tf.random.uniform(tf.shape(mfp_mask), minval=0.0, maxval=1.0) < CHANGE_PROB
        )
        rand_arr = tf.random.uniform(tf.shape(chg_mask), minval=0.0, maxval=1.0)
        masked_input = apply_token(
            inputs[key], column, chg_mask & (rand_arr >= THRESH), "masked"
        )
        masked_input = apply_token(
            masked_input, column, chg_mask & (rand_arr < THRESH), "random"
        )

        # update input
        modified_inputs[key] = masked_input
        masks[key] = mfp_mask

    return modified_inputs, masks


===== dataforfd-spec.yml =====
name: crello
columns:
  id:
    dtype: string
    demo_only: true
  length:
    dtype: int64
    lookup:
      vocabulary:
        min: 1
        max: 50
      num_oov_indices: 0
      mask_value: null
  group:
    dtype: string
    lookup:
      mask_token: ''
      num_oov_indices: 0
  format:
    dtype: string
    lookup:
      mask_token: ''
      num_oov_indices: 0
  canvas_width:
    dtype: int64
    lookup:
      num_oov_indices: 0
  canvas_height:
    dtype: int64
    lookup:
      num_oov_indices: 0
  category:
    dtype: string
    lookup:
      mask_token: ''
      num_oov_indices: 0
  type:
    is_sequence: true
    dtype: string
    lookup:
      mask_token: ''
      num_oov_indices: 0
    primary_label:
      default: ''
  left:
    is_sequence: true
    dtype: float32
    discretize:
      min: 0.0
      max: 1.0
      bins: 64
  top:
    is_sequence: true
    dtype: float32
    discretize:
      min: 0.0
      max: 1.0
      bins: 64
  width:
    is_sequence: true
    dtype: float32
    discretize:
      min: 0.0
      max: 1.0
      bins: 64
  height:
    is_sequence: true
    dtype: float32
    discretize:
      min: 0.0
      max: 1.0
      bins: 64
  image_embedding:
    is_sequence: true
    shape: [512]
    dtype: float32
    loss_condition:
      key: type
      values:
        - humanElement

  uuid:
    is_sequence: true
    dtype: string
    demo_only: true

===== mask.py =====
import tensorflow as tf


class Unmask(tf.keras.layers.Layer):
    """Layer to stop mask propagation."""

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.supports_masking = True

    def compute_mask(self, inputs, mask=None):
        return None

    def call(self, inputs, mask=None):
        if hasattr(inputs, "_keras_mask"):
            delattr(inputs, "_keras_mask")
        return inputs


# @tf.function(experimental_relax_shapes=True)
def get_seq_mask(inputs, from_logits: bool = False, maxlen: int = None):
    """Generate mask from length."""
    if from_logits:
        length = tf.reshape(tf.argmax(inputs, axis=-1), (-1,))
    else:
        length = tf.reshape(inputs, (-1,))

    # Fix zero-based index.
    length += 1

    seq_mask = tf.sequence_mask(length, maxlen=maxlen)
    tf.debugging.assert_rank(seq_mask, 2)
    return seq_mask


===== utils.py =====
import logging

import tensorflow as tf

logger = logging.getLogger(__name__)


def make_dense_options(l2: float):
    if l2 is None:
        return {}
    return dict(
        kernel_regularizer=tf.keras.regularizers.l2(l2),
        bias_regularizer=tf.keras.regularizers.l2(l2),
    )


def make_emb_options(l2: float):
    if l2 is None:
        return {}
    return dict(
        embeddings_regularizer=tf.keras.regularizers.l2(l2),
    )


===== transformer.py =====
import tensorflow as tf
from mfp.models.architecture.utils import make_dense_options


class PositionEmbedding(tf.keras.layers.Layer):
    """Returns positional const embeddings."""

    def __init__(
        self,
        output_dim,
        maxlen,
        dropout=0.1,
        emb_options=None,
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.embeddings = tf.keras.layers.Embedding(
            maxlen + 1,
            output_dim,
            **(emb_options or {}),
        )
        self.dropout = tf.keras.layers.Dropout(dropout)

    def call(self, inputs, training=False):
        B = tf.shape(inputs)[0]
        positions = tf.range(tf.shape(inputs)[1])
        embeddings = self.embeddings(positions[tf.newaxis, :])
        embeddings = tf.tile(embeddings, [B, 1, 1])
        embeddings = self.dropout(embeddings, training=training)
        return embeddings


class MultiHeadSelfAttention(tf.keras.layers.Layer):
    """
    Taken from
    https://keras.io/examples/nlp/text_classification_with_transformer/

    :param emb_size: Size of the embedding.
    :param num_heads: Number of heads.
    :param lookahead: Allow attention to future tokens.
    """

    def __init__(self, emb_size, num_heads=8, lookahead=True, **dense_options):
        super().__init__()
        self.emb_size = emb_size
        self.num_heads = num_heads
        self.lookahead = lookahead
        if emb_size % num_heads != 0:
            raise ValueError(
                f"embedding dimension = {emb_size} should be divisible by "
                f"number of heads = {num_heads}."
            )
        self.projection_dim = emb_size // num_heads
        self.dense_query = tf.keras.layers.Dense(emb_size, **dense_options)
        self.dense_key = tf.keras.layers.Dense(emb_size, **dense_options)
        self.dense_value = tf.keras.layers.Dense(emb_size, **dense_options)
        self.combine_heads = tf.keras.layers.Dense(emb_size, **dense_options)
        self.supports_masking = True

    def attention(self, query, key, value, mask=None):
        score = tf.matmul(query, key, transpose_b=True)  # (B, H, S, projection_dim)
        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)  # (B, H, S, S)
        scaled_score = score / tf.math.sqrt(dim_key)  # (B, H, S, S)
        if mask is not None:
            # padding mask (B, 1, 1, S)
            mask = tf.cast(mask, tf.float32)[:, tf.newaxis, tf.newaxis, :]
            if not self.lookahead:
                size = tf.shape(mask)[-1]
                mask *= tf.linalg.band_part(tf.ones((size, size)), -1, 0)[
                    tf.newaxis, tf.newaxis, :, :
                ]
            # Force large negative for masks: (B, H, S, S).
            scaled_score += -1e9 * (1.0 - mask)
        weights = tf.nn.softmax(scaled_score, axis=-1)  # (B, H, S, S)
        output = tf.matmul(weights, value)  # (B, H, S, projection_dim)
        return output, weights

    def separate_heads(self, x, batch_size):
        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))
        return tf.transpose(x, perm=[0, 2, 1, 3])

    def call(self, inputs, mask=None):
        # inputs.shape = [B, S, embedding_dim]
        batch_size = tf.shape(inputs)[0]
        query = self.dense_query(inputs)  # (B, S, emb_size)
        query = self.separate_heads(query, batch_size)  # (B, H, S, projection_dim)
        key = self.dense_key(inputs)  # (B, S, emb_size)
        key = self.separate_heads(key, batch_size)  # (B, H, S, projection_dim)
        value = self.dense_value(inputs)  # (B, S, emb_size)
        value = self.separate_heads(value, batch_size)  # (B, H, S, projection_dim)
        attention, _ = self.attention(query, key, value, mask)
        attention = tf.transpose(
            attention, perm=[0, 2, 1, 3]
        )  # (B, S, H, projection_dim)
        concat_attention = tf.reshape(
            attention, (batch_size, -1, self.emb_size)
        )  # (B, S, emb_size)
        output = self.combine_heads(concat_attention)  # (B, S, emb_size)
        return output


class MultiHeadCrossAttention(MultiHeadSelfAttention):
    """
    Taken from
    https://keras.io/examples/nlp/text_classification_with_transformer/

    :param emb_size: Size of the embedding.
    :param num_heads: Number of heads.
    :param lookahead: Allow attention to future tokens.
    """

    def __init__(self, emb_size, num_heads=8, lookahead=True, **dense_options):
        super().__init__(emb_size, num_heads, lookahead, **dense_options)
        assert self.lookahead

    def call(self, inputs, mask=None):
        # inputs.shape = [B, S, embedding_dim]
        x, z = inputs
        # tgt_mask, memory_mask = masks

        batch_size = tf.shape(x)[0]
        query = self.dense_query(x)  # (B, S, emb_size)
        query = self.separate_heads(query, batch_size)  # (B, H, S, projection_dim)
        key = self.dense_key(z)  # (B, S, emb_size)
        key = self.separate_heads(key, batch_size)  # (B, H, S, projection_dim)
        value = self.dense_value(z)  # (B, S, emb_size)
        value = self.separate_heads(value, batch_size)  # (B, H, S, projection_dim)

        attention, _ = self.attention(query, key, value, mask)

        attention = tf.transpose(
            attention, perm=[0, 2, 1, 3]
        )  # (B, S, H, projection_dim)
        concat_attention = tf.reshape(
            attention, (batch_size, -1, self.emb_size)
        )  # (B, S, emb_size)
        output = self.combine_heads(concat_attention)  # (B, S, emb_size)
        return output


class TransformerBlock(tf.keras.layers.Layer):
    """Transformer block with optional global conditional."""

    def __init__(
        self,
        emb_size=64,
        num_heads=8,
        ff_dim=None,
        dropout=0.1,
        conditional=None,
        pooling=None,
        dense_options=None,
        lookahead=True,
        **kwargs,
    ):
        super().__init__(**kwargs)
        dense_options = dense_options or {}
        self.attn = MultiHeadSelfAttention(
            emb_size, num_heads, lookahead=lookahead, **dense_options
        )
        self.mlp = tf.keras.Sequential(
            [
                tf.keras.layers.Dense(
                    ff_dim or (2 * emb_size),
                    activation="relu",
                    **dense_options,
                ),
                # tf.keras.layers.ReLU(),
                tf.keras.layers.Dense(emb_size, **dense_options),
            ]
        )
        self.norm1 = tf.keras.layers.LayerNormalization()
        self.norm2 = tf.keras.layers.LayerNormalization()
        self.dropout1 = tf.keras.layers.Dropout(dropout)
        self.dropout2 = tf.keras.layers.Dropout(dropout)
        self.supports_masking = True
        self.conditional = None
        if conditional:
            self.norm3 = tf.keras.layers.LayerNormalization()
            self.conditional = tf.keras.layers.Dense(emb_size, **dense_options)

        self.pooling = None
        if pooling:
            self.relu = tf.keras.layers.Activation("relu")
            self.pooling = tf.keras.layers.GlobalAveragePooling1D()

    def call(self, inputs, training=False, mask=None):
        if self.conditional is not None:
            x = inputs[0]
            z = inputs[1]
        else:
            x = inputs
        y = self.attn(x, mask=mask)
        y = self.dropout1(y, training=training)
        x = self.norm1(x + y, training=training)
        if self.conditional is not None:
            z = tf.expand_dims(self.conditional(z), 1)
            x = self.norm3(x + z, training=training)
        y = self.mlp(x)
        y = self.dropout2(y, training=training)
        x = self.norm2(x + y, training=training)
        if self.pooling is not None:
            x = self.relu(x)
            return self.pooling(x, mask=mask)
        return x


class DeepSVGBlock(TransformerBlock):
    """DeepSVG transformer block."""

    def call(self, inputs, training=False, mask=None):
        if self.conditional is not None:
            x, z = inputs
        else:
            x = inputs
        y = self.norm1(x, training=training)
        y = self.attn(y, mask=mask)
        y = self.dropout1(y, training=training)
        x += y
        if self.conditional is not None:
            x += tf.expand_dims(self.conditional(z), 1)
        y = self.norm2(x, training=training)
        y = self.mlp(y)
        y = self.dropout2(y, training=training)
        x = x + y
        if self.pooling is not None:
            x = self.relu(x)
            return self.pooling(x, mask=mask)
        return x


def get_seq_block(layer_type):
    return {
        "transformer": TransformerBlock,
        "deepsvg": DeepSVGBlock,
    }[layer_type]


class Blocks(tf.keras.layers.Layer):
    """
    Stack of transformer layers implementation.
    """

    def __init__(
        self,
        latent_dim=128,
        num_blocks=1,
        block_type="deepsvg",
        conditional=None,
        lookahead=True,  # False if using auto-regressive models
        dropout=0.1,
        l2=None,
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.seq2seq = {}
        self.latent_dim = latent_dim
        self.num_blocks = num_blocks
        self.conditional = conditional

        layer_fn = get_seq_block(block_type)
        for i in range(num_blocks):
            self.seq2seq["seq2seq_%d" % i] = layer_fn(
                latent_dim,
                dropout=dropout,
                conditional=conditional,
                dense_options=make_dense_options(l2),
                lookahead=lookahead,
                name="seq2seq_%d" % i,
            )

    def __call__(self, seq, mask, training=False):
        if self.conditional:
            seq, z = seq[0], seq[1]
            for layer in self.seq2seq.values():
                seq = layer((seq, z), training=training, mask=mask)
        else:
            for layer in self.seq2seq.values():
                seq = layer(seq, training=training, mask=mask)
        return seq


class CrossBlocks(Blocks):
    """
    Stack of transformer layers implementation.
    """

    def __init__(
        self,
        **kwargs,
    ):
        super().__init__(**kwargs)

    def __call__(self, inputs, masks, training=False):
        tgt, memory = inputs
        for layer in self.seq2seq.values():
            tgt = layer((tgt, memory), training=training, masks=masks)
        return tgt


demo_crello.ipynb

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crello analysis\n",
    "This notebook qualitatively analyzes learned models in Crello dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Editable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please edit these parameters\n",
    "ckpt_dir = \"/home/dell/Project-HCL/BaseLine/flex-dm/results/crello/ours-exp-ft/checkpoints\"\n",
    "dataset_name = \"crello\"\n",
    "db_root = \"/home/dell/Project-HCL/BaseLine/flex-dm/data/crello\"\n",
    "batch_size = 20\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 01:01:30.827357: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-10 01:01:31.031356: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-10 01:01:31.037691: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2025-10-10 01:01:31.037723: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2025-10-10 01:01:32.585897: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2025-10-10 01:01:32.585970: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2025-10-10 01:01:32.585978: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import logging\n",
    "import sys\n",
    "from typing import Dict, List\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "from IPython.display import display, HTML\n",
    "%matplotlib inline\n",
    "import os\n",
    "# current_path = os.getcwd()\n",
    "# print(\"\", current_path)\n",
    "\n",
    "sys.path.append(\"../src/mfp\")\n",
    "\n",
    "# current_path = os.getcwd()\n",
    "# print(\"\", current_path)\n",
    "# os.chdir(\"/storage/homes/hzj/hzj/Project-HCL/BaseLine/flex-dm/src/mfp\")\n",
    "os.chdir(\"/home/dell/Project-HCL/BaseLine/flex-dm/src/mfp\")\n",
    "\n",
    "\n",
    "# /home/dell/Project-HCL/BaseLine/flex-dm\n",
    "\n",
    "from mfp.data.spec import ATTRIBUTE_GROUPS, DataSpec, set_visual_default\n",
    "from mfp.helpers.retrieve import ImageRetriever, TextRetriever\n",
    "from mfp.helpers.svg_crello import SVGBuilder\n",
    "from mfp.models.mfp import MFP\n",
    "from mfp.models.architecture.mask import get_seq_mask\n",
    "from mfp.models.masking import get_initial_masks\n",
    "from util import grouper, load_model\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# fix seed for debug\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mfp.data.spec:Loading resource at /home/dell/Project-HCL/BaseLine/flex-dm/src/mfp/mfp/data/crello-spec.yml\n",
      "INFO:mfp.data.spec:Loading resource at /home/dell/Project-HCL/BaseLine/flex-dm/data/crello/count.json\n",
      "INFO:mfp.data.spec:Loading resource at /home/dell/Project-HCL/BaseLine/flex-dm/data/crello/vocabulary.json\n",
      "INFO:mfp.data.spec:Lookup for length: vocabulary_size=50, options={'num_oov_indices': 0, 'mask_value': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:mask_value is deprecated, use mask_token instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:mask_value is deprecated, use mask_token instead.\n",
      "2025-10-10 01:01:36.125741: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2025-10-10 01:01:36.125835: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2025-10-10 01:01:36.125885: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2025-10-10 01:01:36.178578: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2025-10-10 01:01:36.178772: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2025-10-10 01:01:36.180065: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "INFO:mfp.data.spec:Lookup for group: vocabulary_size=6, options={'mask_token': '', 'num_oov_indices': 0}\n",
      "INFO:mfp.data.spec:Lookup for format: vocabulary_size=67, options={'mask_token': '', 'num_oov_indices': 0}\n",
      "INFO:mfp.data.spec:Lookup for canvas_width: vocabulary_size=41, options={'num_oov_indices': 0}\n",
      "INFO:mfp.data.spec:Lookup for canvas_height: vocabulary_size=46, options={'num_oov_indices': 0}\n",
      "INFO:mfp.data.spec:Lookup for category: vocabulary_size=23, options={'mask_token': '', 'num_oov_indices': 0}\n",
      "INFO:mfp.data.spec:Lookup for type: vocabulary_size=5, options={'mask_token': '', 'num_oov_indices': 0}\n",
      "INFO:mfp.data.spec:Discretizer for left: bins=64\n",
      "INFO:mfp.data.spec:Discretizer for top: bins=64\n",
      "INFO:mfp.data.spec:Discretizer for width: bins=64\n",
      "INFO:mfp.data.spec:Discretizer for height: bins=64\n",
      "INFO:mfp.data.spec:Discretizer for opacity: bins=8\n",
      "INFO:mfp.data.spec:Discretizer for color: bins=16\n",
      "INFO:mfp.data.spec:Lookup for font_family: vocabulary_size=34, options={'num_oov_indices': 1, 'mask_token': None}\n",
      "INFO:mfp.data.spec:TFRecord from /home/dell/Project-HCL/BaseLine/flex-dm/data/crello/test-*.tfrecord\n",
      "/home/dell/anaconda3/envs/fd/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py:2292: UserWarning: The `deterministic` argument has no effect unless the `num_parallel_calls` argument is specified.\n",
      "  warnings.warn(\"The `deterministic` argument has no effect unless the \"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataspec = DataSpec(dataset_name, db_root, batch_size)\n",
    "test_dataset = dataspec.make_dataset(\"test\", shuffle=False)\n",
    "\n",
    "iterator = iter(test_dataset.take(1))\n",
    "example = next(iterator)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load pre-trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mfp.data.spec:Loss condition for color: type included in ['textElement', 'coloredBackground']\n",
      "INFO:mfp.data.spec:Loss condition for image_embedding: type included in ['svgElement', 'imageElement', 'maskElement']\n",
      "INFO:mfp.data.spec:Loss condition for text_embedding: type included in ['textElement']\n",
      "INFO:mfp.data.spec:Loss condition for font_family: type included in ['textElement']\n",
      "INFO:mfp.models.mfp:[('random', 1.0), ('elem', 0.0), ('type', 0.0), ('pos', 0.0), ('img', 0.0)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  type: Linear(256, 6) -> (1, 6)\n",
      "  left: Linear(256, 64) -> (1, 64)\n",
      "  top: Linear(256, 64) -> (1, 64)\n",
      "  width: Linear(256, 64) -> (1, 64)\n",
      "  height: Linear(256, 64) -> (1, 64)\n",
      "  opacity: Linear(256, 8) -> (1, 8)\n",
      "  color: Linear(256, 48) -> (3, 48)\n",
      "  image_embedding: Linear(256, 512) -> (512,)\n",
      "  text_embedding: Linear(256, 512) -> (512,)\n",
      "  font_family: Linear(256, 35) -> (1, 35)\n"
     ]
    }
   ],
   "source": [
    "input_columns = dataspec.make_input_columns()\n",
    "models = {\"main\": load_model(ckpt_dir, input_columns=input_columns)}\n",
    "\n",
    "# models = {\"main\": load_model(ckpt_dir, input_columns=input_columns, compile=False)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build DB for image/text retrieval and visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mfp.data.spec:Loading resource at /home/dell/Project-HCL/BaseLine/flex-dm/src/mfp/mfp/data/crello-images-spec.yml\n",
      "INFO:mfp.data.spec:Loading resource at /home/dell/Project-HCL/BaseLine/flex-dm/data/crello/count.json\n",
      "INFO:mfp.data.spec:Loading resource at /home/dell/Project-HCL/BaseLine/flex-dm/data/crello/vocabulary.json\n",
      "INFO:mfp.helpers.retrieve:Fetching image embeddings...\n",
      "INFO:mfp.data.spec:TFRecord from /home/dell/Project-HCL/BaseLine/flex-dm/data/crello/test-*.tfrecord\n",
      "INFO:mfp.helpers.retrieve:Building image index...\n",
      "INFO:mfp.data.spec:Loading resource at /home/dell/Project-HCL/BaseLine/flex-dm/src/mfp/mfp/data/crello-texts-spec.yml\n",
      "INFO:mfp.data.spec:Loading resource at /home/dell/Project-HCL/BaseLine/flex-dm/data/crello/count.json\n",
      "INFO:mfp.data.spec:Loading resource at /home/dell/Project-HCL/BaseLine/flex-dm/data/crello/vocabulary.json\n",
      "INFO:mfp.helpers.retrieve:Fetching image embeddings...\n",
      "INFO:mfp.data.spec:TFRecord from /home/dell/Project-HCL/BaseLine/flex-dm/data/crello/test-*.tfrecord\n",
      "INFO:mfp.helpers.retrieve:Building image index...\n"
     ]
    }
   ],
   "source": [
    "# Note: this part takes several minutes due to building search index\n",
    "db_root = Path(db_root)\n",
    "image_db = ImageRetriever(db_root, image_path=db_root / \"images\")\n",
    "image_db.build(\"test\")\n",
    "text_db = TextRetriever(db_root, text_path=db_root / \"texts\")\n",
    "text_db.build(\"test\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "builders = {}\n",
    "builders[\"layout\"] = SVGBuilder(\n",
    "    max_width=128,\n",
    "    max_height=192,\n",
    "    key=\"type\",\n",
    "    preprocessor=dataspec.preprocessor,\n",
    ")\n",
    "patterns = (\n",
    "    (\"visual\", image_db, text_db),\n",
    "    (\"visual_wo_text\", image_db, None),\n",
    "    (\"visual_wo_image\", None, text_db),\n",
    ")\n",
    "\n",
    "for (name, idb, tdb) in patterns:\n",
    "    builders[name] = SVGBuilder(\n",
    "        max_width=128,\n",
    "        max_height=192,\n",
    "        key=\"color\",\n",
    "        preprocessor=dataspec.preprocessor,\n",
    "        image_db=idb,\n",
    "        text_db=tdb,\n",
    "        render_text=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mfp.helpers.svg_crello:#elements mismatch 8,7 for pkl, tfr\n",
      "WARNING:mfp.helpers.svg_crello:#elements mismatch 9,8 for pkl, tfr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From left to right: gt-layout,gt-visual,pred-layout,pred-visual\n",
      "type\n",
      "type\n",
      "type\n",
      "type\n",
      "type\n",
      "type\n",
      "type\n",
      "type\n",
      "type\n",
      "type\n",
      "type\n",
      "type\n",
      "type\n",
      "type\n",
      "type\n",
      "type\n",
      "type\n",
      "type\n",
      "type\n",
      "type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mfp.helpers.svg_crello:#elements mismatch 11,10 for pkl, tfr\n",
      "WARNING:mfp.helpers.svg_crello:#elements mismatch 5,4 for pkl, tfr\n",
      "WARNING:mfp.helpers.svg_crello:#elements mismatch 7,6 for pkl, tfr\n",
      "WARNING:mfp.helpers.svg_crello:#elements mismatch 5,4 for pkl, tfr\n",
      "WARNING:mfp.helpers.svg_crello:#elements mismatch 9,8 for pkl, tfr\n",
      "WARNING:mfp.helpers.svg_crello:#elements mismatch 12,11 for pkl, tfr\n",
      "WARNING:mfp.helpers.svg_crello:#elements mismatch 9,8 for pkl, tfr\n",
      "WARNING:mfp.helpers.svg_crello:#elements mismatch 6,5 for pkl, tfr\n",
      "WARNING:mfp.helpers.svg_crello:#elements mismatch 12,11 for pkl, tfr\n",
      "WARNING:mfp.helpers.svg_crello:#elements mismatch 7,6 for pkl, tfr\n",
      "WARNING:mfp.helpers.svg_crello:#elements mismatch 7,6 for pkl, tfr\n",
      "WARNING:mfp.helpers.svg_crello:#elements mismatch 20,19 for pkl, tfr\n",
      "WARNING:mfp.helpers.svg_crello:#elements mismatch 7,6 for pkl, tfr\n",
      "WARNING:mfp.helpers.svg_crello:#elements mismatch 13,12 for pkl, tfr\n",
      "WARNING:mfp.helpers.svg_crello:#elements mismatch 12,11 for pkl, tfr\n",
      "WARNING:mfp.helpers.svg_crello:#elements mismatch 17,16 for pkl, tfr\n",
      "WARNING:mfp.helpers.svg_crello:#elements mismatch 8,7 for pkl, tfr\n",
      "WARNING:mfp.helpers.svg_crello:#elements mismatch 9,8 for pkl, tfr\n",
      "WARNING:mfp.helpers.svg_crello:#elements mismatch 11,10 for pkl, tfr\n",
      "WARNING:mfp.helpers.svg_crello:#elements mismatch 5,4 for pkl, tfr\n",
      "WARNING:mfp.helpers.svg_crello:#elements mismatch 7,6 for pkl, tfr\n",
      "WARNING:mfp.helpers.svg_crello:#elements mismatch 5,4 for pkl, tfr\n",
      "WARNING:mfp.helpers.svg_crello:#elements mismatch 9,8 for pkl, tfr\n",
      "WARNING:mfp.helpers.svg_crello:#elements mismatch 12,11 for pkl, tfr\n",
      "WARNING:mfp.helpers.svg_crello:#elements mismatch 9,8 for pkl, tfr\n",
      "WARNING:mfp.helpers.svg_crello:#elements mismatch 6,5 for pkl, tfr\n",
      "WARNING:mfp.helpers.svg_crello:#elements mismatch 12,11 for pkl, tfr\n",
      "WARNING:mfp.helpers.svg_crello:#elements mismatch 7,6 for pkl, tfr\n",
      "WARNING:mfp.helpers.svg_crello:#elements mismatch 7,6 for pkl, tfr\n",
      "WARNING:mfp.helpers.svg_crello:#elements mismatch 20,19 for pkl, tfr\n",
      "WARNING:mfp.helpers.svg_crello:#elements mismatch 7,6 for pkl, tfr\n",
      "WARNING:mfp.helpers.svg_crello:#elements mismatch 13,12 for pkl, tfr\n",
      "WARNING:mfp.helpers.svg_crello:#elements mismatch 12,11 for pkl, tfr\n",
      "WARNING:mfp.helpers.svg_crello:#elements mismatch 17,16 for pkl, tfr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n"
     ]
    }
   ],
   "source": [
    "target_task = \"pos\"  # choose from: elem, pos, attr, txt, img\n",
    "column_names = {\n",
    "    \"txt\": [\"gt-layout\", \"gt-visual\", \"input\", \"pred\"],\n",
    "    \"img\": [\"gt-layout\", \"gt-visual\", \"input\", \"pred\"],\n",
    "    \"attr\": [\"gt-layout\", \"gt-visual\", \"input\", \"pred\"],\n",
    "    \"pos\": [\"gt-layout\", \"gt-visual\", \"pred-layout\", \"pred-visual\"],\n",
    "    \"elem\": [\"gt-layout\", \"gt-visual\", \"input-layout\", \"input-visual\", \"pred-layout\", \"pred-visual\"],\n",
    "}\n",
    "\n",
    "def visualize_reconstruction(\n",
    "    models: List[tf.keras.Model],\n",
    "    example: Dict,\n",
    "    dataspec: DataSpec\n",
    "):\n",
    "    svgs = []\n",
    "    items = dataspec.unbatch(example)\n",
    "    svgs.append(list(map(builders[\"layout\"], items)))\n",
    "    svgs.append(list(map(builders[\"visual\"], items)))\n",
    "    if target_task == \"txt\":\n",
    "        svgs.append(list(map(builders[\"visual_wo_text\"], items)))\n",
    "    elif target_task == \"img\":\n",
    "        svgs.append(list(map(builders[\"visual_wo_image\"], items)))\n",
    "    elif target_task == \"attr\":\n",
    "        svgs.append(list(map(builders[\"visual\"], [set_visual_default(x) for x in items])))\n",
    "\n",
    "    seq_mask = get_seq_mask(example[\"length\"])\n",
    "    mfp_masks = get_initial_masks(input_columns, seq_mask)\n",
    "\n",
    "    for key in mfp_masks.keys():\n",
    "        if not input_columns[key][\"is_sequence\"]:\n",
    "            continue\n",
    "        mask = mfp_masks[key].numpy()\n",
    "\n",
    "        if target_task == \"elem\":\n",
    "            target_indices = [0]  # hide first\n",
    "            for i in range(len(target_indices)):\n",
    "                mask[i, target_indices[i]] = True\n",
    "        else:\n",
    "            if key == \"type\":\n",
    "                continue\n",
    "            attr_groups = ATTRIBUTE_GROUPS[\"crello\"][target_task]\n",
    "            if key in attr_groups:\n",
    "                mask = seq_mask\n",
    "\n",
    "        mfp_masks[key] = tf.convert_to_tensor(mask)\n",
    "\n",
    "    if target_task == \"elem\":\n",
    "        example_copy = {}\n",
    "        for key in example.keys():\n",
    "            # note: assuming similar mask place in a batch\n",
    "            if example[key].shape[1] > 1:\n",
    "                B, S = example[key].shape[:2]\n",
    "                indices = tf.where(~mfp_masks[key][0, :])[:, 0]\n",
    "                example_copy[key] = tf.gather(\n",
    "                    example[key], indices, axis=1\n",
    "                )\n",
    "                # print(key, example_copy[key].shape)\n",
    "            else:\n",
    "                example_copy[key] = example[key]\n",
    "        example_copy[\"length\"] -= 1\n",
    "        items = dataspec.unbatch(example_copy)\n",
    "        svgs.append(list(map(builders[\"layout\"], items)))\n",
    "        svgs.append(list(map(builders[\"visual\"], items)))\n",
    "\n",
    "    for model in models:\n",
    "        pred = model(example, training=False, demo_args={\"masks\": mfp_masks})\n",
    "        for key in example:\n",
    "            if key not in pred:\n",
    "                pred[key] = example[key]\n",
    "                print(example['id'])\n",
    "\n",
    "        if target_task in [\"pos\", \"elem\"]:\n",
    "            svgs.append(list(map(builders[\"layout\"], dataspec.unbatch(pred))))\n",
    "        svgs.append(list(map(builders[\"visual\"], dataspec.unbatch(pred))))\n",
    "\n",
    "    return [list(grouper(row, len(column_names[target_task]))) for row in zip(*svgs)]\n",
    "\n",
    "iterator = iter(test_dataset.take(1))\n",
    "example = next(iterator)\n",
    "\n",
    "print(f\"From left to right: {','.join(column_names[target_task])}\")\n",
    "svgs = visualize_reconstruction(models.values(), example, dataspec)\n",
    "for i, row in enumerate(svgs):\n",
    "    print(i)\n",
    "    display(HTML(\"<div>%s</div>\" % \" \".join(itertools.chain.from_iterable(row))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}


tensorflowmaskdemo_crello.ipynb


(pytorch)

===== models_pytorch.py =====
"""
PyTorch - 
TensorFlow
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, Optional
import math


# ==================== Transformer Components ====================

class MultiHeadSelfAttention(nn.Module):
    """"""
    
    def __init__(
        self,
        embed_dim: int,
        num_heads: int = 8,
        dropout: float = 0.1,
        lookahead: bool = True,
    ):
        super().__init__()
        assert embed_dim % num_heads == 0
        
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        self.lookahead = lookahead
        
        self.q_proj = nn.Linear(embed_dim, embed_dim)
        self.k_proj = nn.Linear(embed_dim, embed_dim)
        self.v_proj = nn.Linear(embed_dim, embed_dim)
        self.out_proj = nn.Linear(embed_dim, embed_dim)
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None):
        B, S, D = x.shape
        
        q = self.q_proj(x).view(B, S, self.num_heads, self.head_dim).transpose(1, 2)
        k = self.k_proj(x).view(B, S, self.num_heads, self.head_dim).transpose(1, 2)
        v = self.v_proj(x).view(B, S, self.num_heads, self.head_dim).transpose(1, 2)
        
        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)
        
        if mask is not None:
            mask = mask.unsqueeze(1).unsqueeze(2)
            scores = scores.masked_fill(~mask, float('-inf'))
            
            if not self.lookahead:
                causal_mask = torch.triu(
                    torch.ones(S, S, device=x.device, dtype=torch.bool),
                    diagonal=1
                )
                scores = scores.masked_fill(causal_mask, float('-inf'))
        
        attn_weights = F.softmax(scores, dim=-1)
        attn_weights = self.dropout(attn_weights)
        
        out = torch.matmul(attn_weights, v)
        out = out.transpose(1, 2).contiguous().view(B, S, D)
        out = self.out_proj(out)
        return out


class TransformerBlock(nn.Module):
    """Transformer(DeepSVG)"""
    
    def __init__(
        self,
        embed_dim: int = 128,
        num_heads: int = 8,
        ff_dim: Optional[int] = None,
        dropout: float = 0.1,
        lookahead: bool = True,
    ):
        super().__init__()
        ff_dim = ff_dim or (2 * embed_dim)
        
        self.norm1 = nn.LayerNorm(embed_dim)
        self.attn = MultiHeadSelfAttention(
            embed_dim, num_heads, dropout, lookahead
        )
        self.dropout1 = nn.Dropout(dropout)
        
        self.norm2 = nn.LayerNorm(embed_dim)
        self.ffn = nn.Sequential(
            nn.Linear(embed_dim, ff_dim),
            nn.ReLU(),
            nn.Linear(ff_dim, embed_dim),
        )
        self.dropout2 = nn.Dropout(dropout)
    
    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None):
        residual = x
        x = self.norm1(x)
        x = self.attn(x, mask)
        x = self.dropout1(x)
        x = residual + x
        
        residual = x
        x = self.norm2(x)
        x = self.ffn(x)
        x = self.dropout2(x)
        x = residual + x
        
        return x


class TransformerBlocks(nn.Module):
    """Transformer"""
    
    def __init__(
        self,
        num_blocks: int = 4,
        embed_dim: int = 128,
        num_heads: int = 8,
        dropout: float = 0.1,
        lookahead: bool = True,
    ):
        super().__init__()
        self.blocks = nn.ModuleList([
            TransformerBlock(embed_dim, num_heads, dropout=dropout, lookahead=lookahead)
            for _ in range(num_blocks)
        ])
    
    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None):
        for block in self.blocks:
            x = block(x, mask)
        return x


# ==================== Encoder()====================

class Encoder(nn.Module):
    """ - TF"""
    
    def __init__(
        self,
        input_columns: Dict,
        embed_dim: int = 128,
        dropout: float = 0.1,
        max_length: int = 50,
    ):
        super().__init__()
        self.input_columns = input_columns
        self.embed_dim = embed_dim
        
        # 
        self.emb_layers = nn.ModuleList()
        self.emb_keys = []
        
        print("Encoder:")
        for key, column in input_columns.items():
            # demo_only
            if not column.get('is_sequence', False):
                continue
            if column.get('demo_only', False):
                continue
            
            self.emb_keys.append(key)
            
            if column['type'] == 'categorical':
                # +2 <MASK><UNUSED>
                vocab_size = column['input_dim'] + 2
                self.emb_layers.append(nn.Embedding(vocab_size, embed_dim))
                print(f"  {key}: Embedding({vocab_size}, {embed_dim})")
            elif column['type'] == 'numerical':
                # 
                input_size = column['shape'][-1] if 'shape' in column else 1
                self.emb_layers.append(nn.Linear(input_size, embed_dim))
                print(f"  {key}: Linear({input_size}, {embed_dim})")
        
        print(f": {len(self.emb_keys)} ")
        
        self.pos_embedding = nn.Embedding(max_length + 1, embed_dim)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, inputs: Dict[str, torch.Tensor]) -> tuple:
        """"""
        batch_size = inputs['length'].size(0)
        
        # 
        seq_len = None
        for key in self.emb_keys:
            if key in inputs:
                seq_len = inputs[key].size(1)
                break
        
        if seq_len is None:
            raise ValueError("")
        
        # 
        seq_embs = []
        for idx, key in enumerate(self.emb_keys):
            if key not in inputs:
                continue
            
            x = inputs[key]
            layer = self.emb_layers[idx]
            
            # 
            if isinstance(layer, nn.Embedding):
                if x.dtype != torch.long:
                    x = x.long()
            elif isinstance(layer, nn.Linear):
                if x.dtype != torch.float:
                    x = x.float()
            
            emb = layer(x)
            
            # (RGB) - sum across feature dimension
            if len(emb.shape) == 4:  # (B, S, 3, D)
                emb = emb.sum(dim=2)  # -> (B, S, D)
            
            seq_embs.append(emb)
        
        #  - element-wise addition
        seq = torch.stack(seq_embs).sum(dim=0)
        
        # 
        positions = torch.arange(seq_len, device=seq.device).unsqueeze(0).expand(batch_size, -1)
        seq = seq + self.pos_embedding(positions)
        seq = self.dropout(seq)
        
        # 
        lengths = inputs['length'].squeeze(-1)
        mask = torch.arange(seq_len, device=seq.device).unsqueeze(0) < lengths.unsqueeze(1)
        
        return seq, mask


# ==================== Decoder()====================

class Decoder(nn.Module):
    """ - TF"""
    
    def __init__(self, input_columns: Dict, embed_dim: int = 128):
        super().__init__()
        self.input_columns = input_columns
        
        # 
        self.head_layers = nn.ModuleList()
        self.head_keys = []
        self.head_configs = []
        
        print("Decoder:")
        for key, column in input_columns.items():
            # demo_only
            if not column.get('is_sequence', False):
                continue
            if column.get('demo_only', False):
                continue
            
            self.head_keys.append(key)
            self.head_configs.append(column)
            
            if column['type'] == 'categorical':
                shape = column.get('shape', [1])
                #  = shape[-1] * input_dim
                output_dim = shape[-1] * column['input_dim']
                self.head_layers.append(nn.Linear(embed_dim, output_dim))
                print(f"  {key}: Linear({embed_dim}, {output_dim}) -> ({shape[-1]}, {column['input_dim']})")
            else:
                shape = column.get('shape', [1])
                output_dim = shape[-1]
                self.head_layers.append(nn.Linear(embed_dim, output_dim))
                print(f"  {key}: Linear({embed_dim}, {output_dim})")
        
        print(f": {len(self.head_keys)} ")
    
    def forward(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:
        outputs = {}
        batch_size, seq_len, _ = x.shape
        
        for idx, key in enumerate(self.head_keys):
            column = self.head_configs[idx]
            pred = self.head_layers[idx](x)
            
            if column['type'] == 'categorical':
                shape = column.get('shape', [1])
                num_features = shape[-1]
                vocab_size = column['input_dim']
                # Reshape: (B, S, num_features*vocab_size) -> (B, S, num_features, vocab_size)
                pred = pred.view(batch_size, seq_len, num_features, vocab_size)
            
            outputs[key] = pred
        
        return outputs


# ==================== MFP Model ====================

class MFP(nn.Module):
    """Masked Field Prediction"""
    
    def __init__(
        self,
        input_columns: Dict,
        embed_dim: int = 128,
        num_blocks: int = 4,
        num_heads: int = 8,
        dropout: float = 0.1,
        max_length: int = 50,
    ):
        super().__init__()
        self.input_columns = input_columns
        
        print("\n" + "="*60)
        print("MFP")
        print("="*60)
        
        self.encoder = Encoder(
            input_columns, embed_dim, dropout, max_length
        )
        
        print("\nTransformer:")
        print(f"  blocks={num_blocks}, embed_dim={embed_dim}, num_heads={num_heads}")
        self.transformer = TransformerBlocks(
            num_blocks, embed_dim, num_heads, dropout, lookahead=True
        )
        
        print("")
        self.decoder = Decoder(input_columns, embed_dim)
        
        total_params = sum(p.numel() for p in self.parameters())
        print(f"\n: {total_params:,}")
        print("="*60 + "\n")
    
    def forward(self, inputs: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        x, mask = self.encoder(inputs) #torch.Size([20, 20, 256])
        x = self.transformer(x, mask) #torch.Size([20, 20, 256])
        outputs = self.decoder(x)
        return outputs
    
    def load_converted_weights(self, checkpoint_path: str):
        """"""
        checkpoint = torch.load(checkpoint_path, map_location='cpu')
        state_dict = checkpoint['state_dict']
        
        missing_keys, unexpected_keys = self.load_state_dict(state_dict, strict=False)
        
        if missing_keys:
            print(f":  {len(missing_keys)} ")
            for key in missing_keys[:5]:
                print(f"  - {key}")
        if unexpected_keys:
            print(f":  {len(unexpected_keys)} ")
            for key in unexpected_keys[:5]:
                print(f"  - {key}")
        
        print(" ")


# ====================  ====================

if __name__ == "__main__":
    print("MFP()\n")
    
    # TFinput_columns
    input_columns = {
        'type': {
            'is_sequence': True, 
            'type': 'categorical', 
            'input_dim': 6, 
            'shape': [1]
        },
        'left': {
            'is_sequence': True, 
            'type': 'categorical', 
            'input_dim': 64, 
            'shape': [1]
        },
        'top': {
            'is_sequence': True, 
            'type': 'categorical', 
            'input_dim': 64, 
            'shape': [1]
        },
        'width': {
            'is_sequence': True, 
            'type': 'categorical', 
            'input_dim': 64, 
            'shape': [1]
        },
        'height': {
            'is_sequence': True, 
            'type': 'categorical', 
            'input_dim': 64, 
            'shape': [1]
        },
        'opacity': {
            'is_sequence': True, 
            'type': 'categorical', 
            'input_dim': 8, 
            'shape': [1]
        },
        'color': {
            'is_sequence': True, 
            'type': 'categorical', 
            'input_dim': 16, 
            'shape': [3]
        },
        'image_embedding': {
            'is_sequence': True, 
            'type': 'numerical', 
            'shape': [512]
        },
        'text_embedding': {
            'is_sequence': True, 
            'type': 'numerical', 
            'shape': [512]
        },
        'font_family': {
            'is_sequence': True, 
            'type': 'categorical', 
            'input_dim': 35, 
            'shape': [1]
        },
        'uuid': {
            'is_sequence': True, 
            'demo_only': True,
            'type': 'categorical', 
            'input_dim': 1215, 
            'shape': [1]
        },
    }
    
    model = MFP(input_columns, embed_dim=256, num_blocks=4)
    
    # 
    batch_size = 2
    seq_len = 10
    
    test_input = {
        'length': torch.tensor([[5], [7]], dtype=torch.long),
        'type': torch.randint(0, 6, (batch_size, seq_len, 1)),
        'left': torch.randint(0, 64, (batch_size, seq_len, 1)),
        'top': torch.randint(0, 64, (batch_size, seq_len, 1)),
        'width': torch.randint(0, 64, (batch_size, seq_len, 1)),
        'height': torch.randint(0, 64, (batch_size, seq_len, 1)),
        'opacity': torch.randint(0, 8, (batch_size, seq_len, 1)),
        'color': torch.randint(0, 16, (batch_size, seq_len, 3)),
        'image_embedding': torch.randn(batch_size, seq_len, 512),
        'text_embedding': torch.randn(batch_size, seq_len, 512),
        'font_family': torch.randint(0, 35, (batch_size, seq_len, 1)),
    }
    
    print("...")
    with torch.no_grad():
        outputs = model(test_input)
    
    print("\n !")
    print("\n:")
    for key, value in outputs.items():
        print(f"  {key:20s}: {list(value.shape)}")
    
    print("\n:")
    print("  type:  [2, 10, 1, 6]")
    print("  color: [2, 10, 3, 16]")
    print("  opacity: [2, 10, 1, 8]")

===== retriever_pytorch.py =====
"""
PyTorch

"""

import json
import logging
from pathlib import Path
from typing import Any, Dict, List
from base64 import b64encode

import torch
import numpy as np

# faiss
try:
    import faiss
    FAISS_AVAILABLE = True
except ImportError:
    FAISS_AVAILABLE = False
    logging.warning("Faiss not available, using brute force search")

logger = logging.getLogger(__name__)


class BaseRetriever:
    """"""
    
    def __init__(
        self,
        data_path: Path,
        key: str,
        value: str,
        condition: Dict[str, Any] = None,
        dim: int = 512,
    ):
        """
        Args:
            data_path: 
            key: 
            value: 
            condition: 
            dim: 
        """
        self.data_path = Path(data_path)
        self.key = key
        self.value = value
        self.condition = condition
        self.dim = dim
        
        self.labels = None
        self.db = None
    
    def build(self, split: str = 'train'):
        """
        
        
        Args:
            split: 
        """
        logger.info(f"Building {self.__class__.__name__} index for {split}...")
        
        # 
        json_file = self.data_path / f"{split}.json"
        if not json_file.exists():
            logger.warning(f"Data file not found: {json_file}")
            return
        
        with open(json_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # 
        embeddings = []
        labels = []
        
        for item in data:
            length = item['length']
            
            for i in range(length):
                # 
                if self.condition:
                    cond_key = self.condition['key']
                    cond_values = self.condition['values']
                    if item[cond_key][i] not in cond_values:
                        continue
                
                # 
                if self.key in item and self.value in item:
                    key_val = item[self.key][i]
                    value_val = item[self.value][i]
                    
                    if isinstance(value_val, list) and len(value_val) == self.dim:
                        embeddings.append(value_val)
                        labels.append(key_val)
        
        if not embeddings:
            logger.warning(f"No embeddings found for {split}")
            return
        
        # 
        unique_data = {}
        for label, emb in zip(labels, embeddings):
            if label not in unique_data:
                unique_data[label] = emb
        
        self.labels = np.array(list(unique_data.keys()))
        embeddings_list = list(unique_data.values())
        
        # 2D numpy
        if embeddings_list:
            #  numpy 
            embeddings = np.array(embeddings_list, dtype=np.float32)
            
            #  C-contiguous 
            if not embeddings.flags['C_CONTIGUOUS']:
                embeddings = np.ascontiguousarray(embeddings)
            
            # 
            assert embeddings.ndim == 2, f"Expected 2D array, got {embeddings.ndim}D"
            assert embeddings.shape[1] == self.dim, f"Expected dim={self.dim}, got {embeddings.shape[1]}"
            assert embeddings.dtype == np.float32, f"Expected float32, got {embeddings.dtype}"
            
            logger.info(f"Embeddings shape: {embeddings.shape}, dtype: {embeddings.dtype}, "
                       f"C-contiguous: {embeddings.flags['C_CONTIGUOUS']}")
        else:
            logger.warning(f"No embeddings after deduplication for {split}")
            return
        
        # 
        if FAISS_AVAILABLE:
            try:
                self.db = faiss.IndexFlatL2(self.dim)
                #  embeddings 
                embeddings_copy = np.copy(embeddings, order='C')
                self.db.add(embeddings_copy)
            except Exception as e:
                logger.error(f"Faiss error: {e}")
                logger.info("Falling back to PyTorch brute force search")
                self.db = torch.from_numpy(embeddings)
        else:
            # PyTorch
            self.db = torch.from_numpy(embeddings)
        
        logger.info(f" Built index with {len(self.labels)} items")
    
    def search(self, query, k: int = 1):
        """
        
        
        Args:
            query: 
            k: 
        
        Returns:
            
        """
        if self.labels is None or self.db is None:
            return self.get_default_result()
        
        # numpy
        if torch.is_tensor(query):
            query = query.cpu().numpy()
        if not isinstance(query, np.ndarray):
            query = np.array(query, dtype=np.float32)
        
        if query.ndim == 1:
            query = query.reshape(1, -1)
        
        #  C-contiguous  float32
        query = np.ascontiguousarray(query, dtype=np.float32)
        
        # 
        if FAISS_AVAILABLE:
            _, indices = self.db.search(query, k)
        else:
            # PyTorch
            query_torch = torch.from_numpy(query)
            distances = torch.cdist(query_torch, self.db)
            _, indices = distances.topk(k, largest=False)
            indices = indices.cpu().numpy()
        
        # 
        results = [self.get_url(idx) for idx in indices[0]]
        
        return results[0] if k == 1 else results
    
    def get_url(self, index: int) -> str:
        """URL"""
        raise NotImplementedError
    
    def get_default_result(self):
        """"""
        return ""


class ImageRetriever(BaseRetriever):
    """"""
    
    def __init__(
        self,
        data_path: Path,
        key: str = 'image_hash',
        value: str = 'image_embedding',
        condition: Dict[str, Any] = None,
        image_path: Path = None,
        dim: int = 512,
    ):
        """
        Args:
            data_path: 
            key: 
            value: 
            condition: 
            image_path: 
            dim: 
        """
        super().__init__(data_path, key, value, condition, dim)
        
        if condition is None:
            self.condition = {
                'key': 'type',
                'values': ['imageElement', 'maskElement', 'svgElement', 'humanElement'],
            }
        
        self.image_path = image_path or self.data_path / 'images'
    
    def get_url(self, index: int) -> str:
        """URL"""
        label = self.labels[index]
        
        if isinstance(label, bytes):
            label = label.decode('utf-8')
        
        if label:
            image_file = self.image_path / f"{label}.png"
            if image_file.exists():
                return self._make_data_uri(image_file)
        
        return ""
    
    def _make_data_uri(self, file_path: Path, mime_type: str = 'image/png') -> str:
        """data URI"""
        try:
            with open(file_path, 'rb') as f:
                image_bytes = f.read()
            data = b64encode(image_bytes).decode('ascii')
            return f"data:{mime_type};base64,{data}"
        except Exception as e:
            logger.warning(f"Failed to read image {file_path}: {e}")
            return ""


class TextRetriever(BaseRetriever):
    """"""
    
    def __init__(
        self,
        data_path: Path,
        key: str = 'text_hash',
        value: str = 'text_embedding',
        condition: Dict[str, Any] = None,
        text_path: Path = None,
        dim: int = 512,
    ):
        """
        Args:
            data_path: 
            key: 
            value: 
            condition: 
            text_path: 
            dim: 
        """
        super().__init__(data_path, key, value, condition, dim)
        
        if condition is None:
            self.condition = {
                'key': 'type',
                'values': ['textElement'],
            }
        
        self.text_path = text_path or self.data_path / 'texts'
    
    def get_url(self, index: int) -> str:
        """"""
        label = self.labels[index]
        
        if isinstance(label, bytes):
            label = label.decode('utf-8')
        
        if label:
            text_file = self.text_path / f"{label}.txt"
            if text_file.exists():
                try:
                    with open(text_file, 'r', encoding='utf-8') as f:
                        return f.read()
                except Exception as e:
                    logger.warning(f"Failed to read text {text_file}: {e}")
        
        return "TEXT"


# 
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    
    # 
    data_path = Path("./data/crello_json")
    
    if data_path.exists():
        # 
        print("...")
        image_retriever = ImageRetriever(
            data_path,
            image_path=data_path.parent / "crello" / "images"
        )
        image_retriever.build("test")
        
        # 
        if image_retriever.labels is not None:
            test_query = np.random.randn(512).astype(np.float32)
            result = image_retriever.search(test_query)
            print(f": {len(result)}")
        
        # 
        print("\n...")
        text_retriever = TextRetriever(
            data_path,
            text_path=data_path.parent / "crello" / "texts"
        )
        text_retriever.build("test")
        
        # 
        if text_retriever.labels is not None:
            test_query = np.random.randn(512).astype(np.float32)
            result = text_retriever.search(test_query)
            print(f": {result[:50]}...")
        
        print("\n !")
    else:
        print(f": {data_path}")

===== train_pytorch.py =====
"""
PyTorch

"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
from pathlib import Path
import argparse
import json
from tqdm import tqdm
import time
import os
from typing import Dict

from dataset import create_dataloader, DesignLayoutDataset
from models_pytorch import MFP


class ImprovedMFPTrainer:
    """MFP"""
    
    def __init__(
        self,
        model: nn.Module,
        train_loader: DataLoader,
        val_loader: DataLoader,
        config: Dict,
        device: str = 'cuda',
        save_dir: str = './checkpoints',
        log_dir: str = './logs',
        resume_path: str = None,
    ):
        self.model = model.to(device)
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.config = config
        self.device = device
        
        # 
        train_cfg = config.get('training', {})
        self.num_epochs = train_cfg.get('num_epochs', 100)
        self.gradient_clip = train_cfg.get('gradient_clip', 1.0)
        self.accumulation_steps = train_cfg.get('accumulation_steps', 1)
        
        # 
        self.loss_weights = config.get('loss_weights', {})
        
        # 
        self.optimizer = optim.AdamW(
            model.parameters(),
            lr=train_cfg.get('learning_rate', 1e-4),
            betas=(0.9, 0.999),
            weight_decay=train_cfg.get('weight_decay', 0.01),
        )
        
        # 
        scheduler_cfg = config.get('scheduler', {})
        if scheduler_cfg.get('type') == 'ReduceLROnPlateau':
            self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
                self.optimizer,
                mode=scheduler_cfg.get('mode', 'min'),
                factor=scheduler_cfg.get('factor', 0.5),
                patience=scheduler_cfg.get('patience', 5),
                min_lr=scheduler_cfg.get('min_lr', 1e-6),
                verbose=True,
            )
        else:
            self.scheduler = None
        
        # 
        self.save_dir = Path(save_dir)
        self.save_dir.mkdir(parents=True, exist_ok=True)
        
        # TensorBoard
        self.writer = SummaryWriter(log_dir)
        
        # 
        self.start_epoch = 0
        self.best_val_loss = float('inf')
        self.global_step = 0
        
        # 
        if resume_path and Path(resume_path).exists():
            self.load_checkpoint(resume_path)
        
        print(f" ")
        print(f"  : {device}")
        print(f"  : {len(train_loader)}")
        print(f"  : {len(val_loader)}")
        print(f"  epoch: {self.start_epoch}")
    
    def compute_loss(
        self,
        predictions: Dict[str, torch.Tensor],
        targets: Dict[str, torch.Tensor],
        mask: torch.Tensor,
    ) -> Dict[str, torch.Tensor]:
        """
        
        
        Args:
            predictions: 
            targets: 
            mask:  (B, S)
        
        Returns:
            
        """
        losses = {}
        total_loss = 0.0
        
        for key in predictions.keys():
            if key not in targets:
                continue
            
            pred = predictions[key]
            target = targets[key]
            
            # 
            column = self.model.input_columns.get(key, {})
            weight = self.loss_weights.get(key, 1.0)
            
            if column.get('type') == 'categorical':
                # 
                # pred: (B, S, num_feat, C), target: (B, S, num_feat)
                if pred.dim() == 4:  # (B, S, num_feat, C)
                    B, S, num_feat, C = pred.shape
                    pred = pred.reshape(B * S * num_feat, C)
                    target = target.reshape(B * S * num_feat).long()
                    
                    loss = F.cross_entropy(pred, target, reduction='none')
                    loss = loss.reshape(B, S, num_feat)
                    
                    # 
                    mask_expanded = mask.unsqueeze(-1).expand_as(loss)
                    loss = (loss * mask_expanded.float()).sum() / (mask_expanded.sum() + 1e-8)
                    
                elif pred.dim() == 3:  # (B, S, C)
                    B, S, C = pred.shape
                    pred = pred.reshape(B * S, C)
                    target = target.reshape(B * S).long()
                    
                    loss = F.cross_entropy(pred, target, reduction='none')
                    loss = loss.reshape(B, S)
                    
                    # 
                    loss = (loss * mask.float()).sum() / (mask.sum() + 1e-8)
                else:
                    continue
            
            elif column.get('type') == 'numerical':
                # MSE
                # pred: (B, S, D), target: (B, S, D)
                loss = F.mse_loss(pred, target, reduction='none')
                
                # 
                mask_expanded = mask.unsqueeze(-1).expand_as(loss)
                loss = (loss * mask_expanded.float()).sum() / (mask_expanded.sum() + 1e-8)
            else:
                continue
            
            # 
            weighted_loss = loss * weight
            losses[f'{key}_loss'] = loss.detach()
            total_loss += weighted_loss
        
        losses['total_loss'] = total_loss
        return losses
    
    def train_epoch(self, epoch: int):
        """epoch"""
        self.model.train()
        epoch_losses = {}
        
        pbar = tqdm(self.train_loader, desc=f'Epoch {epoch}/{self.num_epochs}')
        self.optimizer.zero_grad()
        
        for batch_idx, batch in enumerate(pbar):
            # 
            inputs = {k: v.to(self.device) if torch.is_tensor(v) else v 
                     for k, v in batch.items()}
            
            # 
            outputs = self.model(inputs)
            
            # 
            lengths = inputs['length'].squeeze(-1)
            max_len = inputs['left'].size(1)
            mask = torch.arange(max_len, device=self.device).unsqueeze(0) < lengths.unsqueeze(1)
            
            # 
            losses = self.compute_loss(outputs, inputs, mask)
            loss = losses['total_loss'] / self.accumulation_steps
            
            # 
            loss.backward()
            
            # 
            if (batch_idx + 1) % self.accumulation_steps == 0:
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.gradient_clip)
                self.optimizer.step()
                self.optimizer.zero_grad()
            
            # 
            for key, value in losses.items():
                if key not in epoch_losses:
                    epoch_losses[key] = []
                epoch_losses[key].append(value.item() if torch.is_tensor(value) else value)
            
            # 
            pbar.set_postfix({
                'loss': f"{losses['total_loss'].item():.4f}",
                'lr': f"{self.optimizer.param_groups[0]['lr']:.6f}"
            })
            
            # TensorBoard
            log_cfg = self.config.get('logging', {})
            if self.global_step % log_cfg.get('log_interval', 100) == 0:
                for key, value in losses.items():
                    self.writer.add_scalar(
                        f'train/{key}', 
                        value.item() if torch.is_tensor(value) else value, 
                        self.global_step
                    )
                self.writer.add_scalar('train/lr', self.optimizer.param_groups[0]['lr'], self.global_step)
            
            self.global_step += 1
        
        # epoch
        avg_losses = {k: sum(v) / len(v) for k, v in epoch_losses.items()}
        return avg_losses
    
    @torch.no_grad()
    def validate(self, epoch: int):
        """"""
        self.model.eval()
        epoch_losses = {}
        
        for batch in tqdm(self.val_loader, desc='Validation'):
            # 
            inputs = {k: v.to(self.device) if torch.is_tensor(v) else v 
                     for k, v in batch.items()}
            
            # 
            outputs = self.model(inputs)
            
            # 
            lengths = inputs['length'].squeeze(-1)
            max_len = inputs['left'].size(1)
            mask = torch.arange(max_len, device=self.device).unsqueeze(0) < lengths.unsqueeze(1)
            
            # 
            losses = self.compute_loss(outputs, inputs, mask)
            
            # 
            for key, value in losses.items():
                if key not in epoch_losses:
                    epoch_losses[key] = []
                val = value.item() if torch.is_tensor(value) else value
                epoch_losses[key].append(val)
        
        # 
        avg_losses = {k: sum(v) / len(v) for k, v in epoch_losses.items()}
        
        # TensorBoard
        for key, value in avg_losses.items():
            self.writer.add_scalar(f'val/{key}', value, epoch)
        
        return avg_losses
    
    def save_checkpoint(self, epoch: int, val_loss: float, is_best: bool = False):
        """checkpoint"""
        checkpoint = {
            'epoch': epoch,
            'global_step': self.global_step,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'best_val_loss': self.best_val_loss,
            'val_loss': val_loss,
            'config': self.config,
        }
        
        if self.scheduler is not None:
            checkpoint['scheduler_state_dict'] = self.scheduler.state_dict()
        
        # 
        torch.save(checkpoint, self.save_dir / 'latest.pth')
        
        # 
        if is_best:
            torch.save(checkpoint, self.save_dir / 'best.pth')
            print(f"  (epoch {epoch}, val_loss: {val_loss:.4f})")
        
        # 
        ckpt_cfg = self.config.get('checkpoint', {})
        if epoch % ckpt_cfg.get('save_interval', 5) == 0:
            torch.save(checkpoint, self.save_dir / f'checkpoint_epoch_{epoch}.pth')
        
        # checkpoint
        self._cleanup_checkpoints(ckpt_cfg.get('keep_last_n', 3))
    
    def _cleanup_checkpoints(self, keep_last_n: int):
        """checkpoint"""
        checkpoints = sorted(self.save_dir.glob('checkpoint_epoch_*.pth'))
        if len(checkpoints) > keep_last_n:
            for ckpt in checkpoints[:-keep_last_n]:
                ckpt.unlink()
    
    def load_checkpoint(self, checkpoint_path: str):
        """checkpoint"""
        print(f"checkpoint: {checkpoint_path}")
        checkpoint = torch.load(checkpoint_path, map_location=self.device)
        
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        
        if 'scheduler_state_dict' in checkpoint and self.scheduler is not None:
            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        
        self.start_epoch = checkpoint.get('epoch', 0) + 1
        self.global_step = checkpoint.get('global_step', 0)
        self.best_val_loss = checkpoint.get('best_val_loss', float('inf'))
        
        print(f"  epoch {self.start_epoch}")
    
    def train(self):
        """"""
        print("\n" + "="*60)
        print("")
        print("="*60)
        
        for epoch in range(self.start_epoch, self.num_epochs):
            start_time = time.time()
            
            # 
            train_losses = self.train_epoch(epoch)
            
            # 
            log_cfg = self.config.get('logging', {})
            if epoch % log_cfg.get('val_interval', 1) == 0:
                val_losses = self.validate(epoch)
            else:
                val_losses = {'total_loss': float('inf')}
            
            # 
            if self.scheduler is not None:
                self.scheduler.step(val_losses['total_loss'])
            
            # 
            epoch_time = time.time() - start_time
            print(f"\n{'='*60}")
            print(f"Epoch {epoch}/{self.num_epochs - 1} ({epoch_time:.1f}s)")
            print(f"{'='*60}")
            print(f"Train Loss: {train_losses['total_loss']:.4f}")
            if 'total_loss' in val_losses and val_losses['total_loss'] != float('inf'):
                print(f"Val Loss:   {val_losses['total_loss']:.4f}")
            print(f"LR:         {self.optimizer.param_groups[0]['lr']:.6f}")
            print(f"Best Val:   {self.best_val_loss:.4f}")
            
            # checkpoint
            is_best = val_losses['total_loss'] < self.best_val_loss
            if is_best:
                self.best_val_loss = val_losses['total_loss']
            
            self.save_checkpoint(epoch, val_losses['total_loss'], is_best)
        
        print("\n" + "="*60)
        print("!")
        print(f": {self.best_val_loss:.4f}")
        print("="*60)
        self.writer.close()


def main():
    parser = argparse.ArgumentParser(description='Train MFP Model')
    
    # 
    parser.add_argument('--data_dir', type=str, required=True,
                       help='JSON')
    parser.add_argument('--config', type=str, default='config/train_config.json',
                       help='')
    
    # 
    parser.add_argument('--batch_size', type=int, default=None)
    parser.add_argument('--num_epochs', type=int, default=None)
    parser.add_argument('--learning_rate', type=float, default=None)
    parser.add_argument('--embed_dim', type=int, default=None)
    parser.add_argument('--num_blocks', type=int, default=None)
    parser.add_argument('--num_heads', type=int, default=None)
    parser.add_argument('--num_workers', type=int, default=None)
    
    # 
    parser.add_argument('--device', type=str, default='cuda')
    parser.add_argument('--save_dir', type=str, default='./checkpoints')
    parser.add_argument('--log_dir', type=str, default='./logs')
    parser.add_argument('--resume', type=str, default=None,
                       help='checkpoint')
    
    args = parser.parse_args()
    
    # 
    print("...")
    with open(args.config, 'r') as f:
        config = json.load(f)
    
    # 
    if args.batch_size is not None:
        config['training']['batch_size'] = args.batch_size
    if args.num_epochs is not None:
        config['training']['num_epochs'] = args.num_epochs
    if args.learning_rate is not None:
        config['training']['learning_rate'] = args.learning_rate
    if args.embed_dim is not None:
        config['model']['embed_dim'] = args.embed_dim
    if args.num_blocks is not None:
        config['model']['num_blocks'] = args.num_blocks
    if args.num_heads is not None:
        config['model']['num_heads'] = args.num_heads
    if args.num_workers is not None:
        config['data']['num_workers'] = args.num_workers
    
    print(json.dumps(config, indent=2))
    
    # 
    print("\n...")
    train_loader = create_dataloader(
        args.data_dir, 'train',
        batch_size=config['training']['batch_size'],
        shuffle=True,
        num_workers=config['data'].get('num_workers', 4),
        max_length=config['data'].get('max_length', 20),
        bins=config['data'].get('bins', 64),
        min_font_freq=config['data'].get('min_font_freq', 500),
    )
    
    val_loader = create_dataloader(
        args.data_dir, 'val',
        batch_size=config['training']['batch_size'],
        shuffle=False,
        num_workers=config['data'].get('num_workers', 4),
        max_length=config['data'].get('max_length', 20),
        bins=config['data'].get('bins', 64),
        min_font_freq=config['data'].get('min_font_freq', 500),
    )
    
    # input_columns
    dataset = train_loader.dataset
    input_columns = dataset.get_input_columns()
    
    # input_columns
    input_columns_path = Path(args.save_dir).parent / 'input_columns_used.json'
    input_columns_path.parent.mkdir(parents=True, exist_ok=True)
    with open(input_columns_path, 'w') as f:
        json.dump(input_columns, f, indent=2)
    print(f" Input columns: {input_columns_path}")
    
    # 
    print("\n...")
    model = MFP(
        input_columns=input_columns,
        embed_dim=config['model']['embed_dim'],
        num_blocks=config['model']['num_blocks'],
        num_heads=config['model']['num_heads'],
        dropout=config['model']['dropout'],
        max_length=config['model']['max_length'],
    )
    
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f": {total_params:,}")
    print(f": {trainable_params:,}")
    
    # 
    trainer = ImprovedMFPTrainer(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        config=config,
        device=args.device,
        save_dir=args.save_dir,
        log_dir=args.log_dir,
        resume_path=args.resume,
    )
    
    # 
    trainer.train()


if __name__ == "__main__":
    main()

===== svg_builder_pytorch.py =====
"""
PyTorchSVG - 
"""

from typing import Dict, Optional, List
import numpy as np


class SVGBuilder:
    """SVG - XML"""
    
    def __init__(
        self,
        key: str = 'type',
        colormap: Optional[Dict] = None,
        preprocessor = None,
        canvas_width: int = 256,
        canvas_height: int = 256,
        max_width: Optional[int] = None,
        max_height: Optional[int] = None,
        opacity: float = 0.5,
        image_db = None,
        text_db = None,
        render_text: bool = False,
        **kwargs
    ):
        assert key, "key"
        
        self.key = key
        self.canvas_width = canvas_width
        self.canvas_height = canvas_height
        self.max_width = max_width
        self.max_height = max_height
        self.opacity = opacity
        self.image_db = image_db
        self.text_db = text_db
        self.render_text = render_text
        
        # 
        if key == 'color':
            self.colormap = None
        elif preprocessor is not None:
            vocabulary = preprocessor.get(key, {}).get('vocabulary', [])
            self.colormap = self._make_colormap(vocabulary, colormap)
        elif colormap is not None:
            self.colormap = colormap
        else:
            self.colormap = self._make_default_colormap()
    
    def _make_default_colormap(self) -> Dict:
        """"""
        return {
            '': 'none',
            'svgElement': 'rgb(66, 166, 246)',
            'textElement': 'rgb(241, 98, 147)',
            'imageElement': 'rgb(175, 214, 130)',
            'maskElement': 'rgb(79, 196, 248)',
            'coloredBackground': 'rgb(226, 191, 232)',
            'videoElement': 'rgb(255, 207, 102)',
            'humanElement': 'rgb(255, 139, 101)',
        }
    
    def _make_colormap(self, vocabulary: List[str], base_colormap=None) -> Dict:
        """"""
        if base_colormap:
            return base_colormap
        
        try:
            from matplotlib import cm
            vocab_size = len(vocabulary)
            cmap = cm.get_cmap('tab20', vocab_size)
            return {
                label: f'rgb({int(c[0]*255)},{int(c[1]*255)},{int(c[2]*255)})'
                for label, c in zip(vocabulary, cmap(range(vocab_size)))
            }
        except ImportError:
            return self._make_default_colormap()
    
    def compute_canvas_size(self, document: Dict):
        """"""
        canvas_width = document.get('canvas_width', self.canvas_width)
        canvas_height = document.get('canvas_height', self.canvas_height)
        
        scale = 1.0
        if self.max_width is not None:
            scale = min(self.max_width / canvas_width, scale)
        if self.max_height is not None:
            scale = min(self.max_height / canvas_height, scale)
        
        return canvas_width * scale, canvas_height * scale
    
    def __call__(self, document: Dict) -> str:
        """SVG"""
        canvas_width, canvas_height = self.compute_canvas_size(document)
        
        # SVG
        svg_parts = [
            f'<svg width="{int(canvas_width)}" height="{int(canvas_height)}" '
            f'viewBox="0 0 1 1" style="background-color: #FFF" '
            f'preserveAspectRatio="none" '
            f'xmlns="http://www.w3.org/2000/svg" '
            f'xmlns:xlink="http://www.w3.org/1999/xlink">'
        ]
        
        # 
        elements = document.get('elements', [])
        for i, element in enumerate(elements):
            svg_parts.append(self._element_to_svg(element, i))
        
        # SVG
        svg_parts.append('</svg>')
        
        return '\n'.join(svg_parts)
    
    def _element_to_svg(self, element: Dict, index: int) -> str:
        """SVG"""
        # 
        if self.key == 'color':
            color = element.get('color', [0, 0, 0])
            if isinstance(color, (list, tuple, np.ndarray)):
                fill = f'rgb({int(color[0])},{int(color[1])},{int(color[2])})'
            else:
                fill = 'rgb(0,0,0)'
        else:
            element_type = element.get(self.key, '')
            if isinstance(element_type, (list, tuple)):
                element_type = element_type[0] if len(element_type) > 0 else ''
            if isinstance(element_type, (int, float, np.integer, np.floating)):
                element_type = str(int(element_type))
            if isinstance(element_type, bytes):
                element_type = element_type.decode('utf-8')
            fill = self.colormap.get(element_type, 'rgb(128,128,128)')
        
        # 
        left = float(element.get('left', 0))
        top = float(element.get('top', 0))
        width = float(element.get('width', 0.1))
        height = float(element.get('height', 0.1))
        opacity_val = float(element.get('opacity', 1.0))
        
        # ID
        uuid = element.get('uuid', f'elem_{index}')
        if isinstance(uuid, bytes):
            uuid = uuid.decode('utf-8')
        elem_type = str(element.get('type', ''))
        
        # 
        image_url = None
        if self.image_db:
            et = element.get('type', '')
            if isinstance(et, bytes):
                et = et.decode('utf-8')
            if et in self.image_db.condition.get('values', []):
                if self.image_db.value in element:
                    image_url = self.image_db.search(element[self.image_db.value])
        
        # 
        text_content = None
        if self.text_db or self.render_text:
            et = element.get('type', '')
            if isinstance(et, bytes):
                et = et.decode('utf-8')
            if et == 'textElement':
                if self.text_db and self.text_db.value in element:
                    text_content = self.text_db.search(element[self.text_db.value])
                else:
                    text_content = "TEXT TEXT TEXT"
        
        # SVG
        if image_url and image_url != '':
            return self._make_image_svg(uuid, elem_type, left, top, width, height, opacity_val, image_url)
        elif self.render_text and text_content:
            return self._make_text_svg(uuid, elem_type, left, top, width, height, opacity_val, fill, text_content, element)
        else:
            return self._make_rect_svg(uuid, elem_type, left, top, width, height, opacity_val * self.opacity, fill)
    
    def _make_rect_svg(self, uuid: str, elem_type: str, left: float, top: float, 
                       width: float, height: float, opacity: float, fill: str) -> str:
        """SVG"""
        return (
            f'<rect id="{uuid}" class="{elem_type}" '
            f'x="{left:.6f}" y="{top:.6f}" '
            f'width="{width:.6f}" height="{height:.6f}" '
            f'fill="{fill}" opacity="{opacity:.3f}"/>'
        )
    
    def _make_image_svg(self, uuid: str, elem_type: str, left: float, top: float,
                        width: float, height: float, opacity: float, url: str) -> str:
        """SVG"""
        return (
            f'<image id="{uuid}" class="{elem_type}" '
            f'x="{left:.6f}" y="{top:.6f}" '
            f'width="{width:.6f}" height="{height:.6f}" '
            f'xlink:href="{url}" opacity="{opacity:.3f}" '
            f'preserveAspectRatio="none"/>'
        )
    
    def _make_text_svg(self, uuid: str, elem_type: str, left: float, top: float,
                       width: float, height: float, opacity: float, fill: str,
                       text: str, element: Dict) -> str:
        """SVG"""
        margin = height * 0.1
        font_size = height * 0.8
        font_family = element.get('font_family', 'Arial')
        if isinstance(font_family, bytes):
            font_family = font_family.decode('utf-8')
        
        display_text = str(text)[:100].strip()
        # XML
        display_text = display_text.replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;')
        
        opacity_str = f' opacity="{opacity:.3f}"' if opacity < 1 else ''
        
        return (
            f'<svg id="{uuid}" class="{elem_type}" '
            f'x="{left:.6f}" y="{(top - margin):.6f}" '
            f'width="{width:.6f}" height="{(height + margin * 2):.6f}" '
            f'overflow="visible"{opacity_str}>'
            f'<text x="50%" y="50%" text-anchor="middle" dominant-baseline="central" '
            f'fill="{fill}" font-size="{font_size:.6f}" font-family="{font_family}">'
            f'{display_text}</text></svg>'
        )


# 
if __name__ == "__main__":
    print("="*60)
    print("SVG Builder ")
    print("="*60)
    
    test_doc = {
        'id': 'test_001',
        'canvas_width': 800,
        'canvas_height': 600,
        'elements': [
            {
                'type': 'coloredBackground',
                'left': 0.0,
                'top': 0.0,
                'width': 1.0,
                'height': 1.0,
                'color': [240, 240, 240],
                'opacity': 1.0,
            },
            {
                'type': 'imageElement',
                'left': 0.1,
                'top': 0.1,
                'width': 0.3,
                'height': 0.4,
                'color': [255, 100, 100],
                'opacity': 1.0,
            },
            {
                'type': 'textElement',
                'left': 0.5,
                'top': 0.2,
                'width': 0.4,
                'height': 0.1,
                'color': [100, 100, 255],
                'opacity': 1.0,
                'font_family': 'Arial',
            },
        ]
    }
    
    print("\n1: Layout")
    builder_layout = SVGBuilder(key='type', max_width=400, opacity=0.8)
    svg_layout = builder_layout(test_doc)
    with open('test_layout.svg', 'w', encoding='utf-8') as f:
        f.write(svg_layout)
    print(f"  {len(svg_layout)} ")
    
    print("\n2: Visual")
    builder_visual = SVGBuilder(key='color', max_width=400, opacity=1.0, render_text=True)
    svg_visual = builder_visual(test_doc)
    with open('test_visual.svg', 'w', encoding='utf-8') as f:
        f.write(svg_visual)
    print(f"  {len(svg_visual)} ")
    
    print("\n" + "="*60)
    print(" ")
    print("="*60)

===== dataset.py =====
"""
 Dataset - 
"""

import json
import torch
from torch.utils.data import Dataset, DataLoader
from pathlib import Path
import numpy as np
from typing import Dict, List, Optional


class DesignLayoutDataset(Dataset):
    """ - """
    
    def __init__(
        self,
        data_path: str,
        split: str = 'train',
        max_length: int = 20,
        bins: int = 64,
        min_font_freq: int = 500,
    ):
        self.data_path = Path(data_path)
        self.split = split
        self.max_length = max_length
        self.bins = bins
        self.min_font_freq = min_font_freq
        
        # 
        json_file = self.data_path / f"{split}.json"
        print(f": {json_file}")
        with open(json_file, 'r', encoding='utf-8') as f:
            self.data = json.load(f)
        
        print(f"  {len(self.data)} ")
        
        # 
        vocab_file = self.data_path.parent / "vocabulary.json"
        with open(vocab_file, 'r', encoding='utf-8') as f:
            self.vocabulary = json.load(f)
        
        # 
        self._build_lookups()
    
    def _build_lookups(self):
        """"""
        print("\n...")
        
        # === Type - token ===
        type_vocab = self.vocabulary['type']
        if isinstance(type_vocab, list):
            # 0
            self.type_to_idx = {v: i for i, v in enumerate(type_vocab)}
        else:
            self.type_to_idx = {k: i for i, k in enumerate(type_vocab.keys())}
        
        # 0
        self.type_to_idx['<UNKNOWN>'] = 0
        self.type_vocab_size = len(type_vocab)  # token
        
        print(f"  Type: {self.type_vocab_size} ")
        print(f"  Type: {self.type_to_idx}")
        
        # === Canvas Width ===
        if 'canvas_width' in self.vocabulary:
            width_vocab = self.vocabulary['canvas_width']
            if isinstance(width_vocab, dict):
                widths = sorted([int(k) for k in width_vocab.keys()])
            elif isinstance(width_vocab, list):
                widths = sorted([int(v) for v in width_vocab])
            else:
                widths = list(range(200, 2001, 100))
            
            self.width_to_idx = {w: i for i, w in enumerate(widths)}
            self.idx_to_width = {i: w for i, w in enumerate(widths)}
            self.idx_to_width[-1] = widths[0] if widths else 800  # 
            
            self.width_vocab_size = len(widths)
            print(f"  Canvas Width: {len(widths)} ")
        else:
            self.width_to_idx = {}
            self.idx_to_width = {0: 800}
            self.width_vocab_size = 1
        
        # === Canvas Height ===
        if 'canvas_height' in self.vocabulary:
            height_vocab = self.vocabulary['canvas_height']
            if isinstance(height_vocab, dict):
                heights = sorted([int(k) for k in height_vocab.keys()])
            elif isinstance(height_vocab, list):
                heights = sorted([int(v) for v in height_vocab])
            else:
                heights = list(range(200, 2001, 100))
            
            self.height_to_idx = {h: i for i, h in enumerate(heights)}
            self.idx_to_height = {i: h for i, h in enumerate(heights)}
            self.idx_to_height[-1] = heights[0] if heights else 600
            
            self.height_vocab_size = len(heights)
            print(f"  Canvas Height: {len(heights)} ")
        else:
            self.height_to_idx = {}
            self.idx_to_height = {0: 600}
            self.height_vocab_size = 1
        
        # === Font ===
        if 'font_family' in self.vocabulary:
            font_vocab = self.vocabulary['font_family']
            
            if isinstance(font_vocab, dict):
                filtered_fonts = [
                    font for font, count in font_vocab.items() 
                    if count >= self.min_font_freq
                ]
                filtered_fonts.sort()
                self.font_to_idx = {font: i for i, font in enumerate(filtered_fonts)}
            else:
                self.font_to_idx = {v: i for i, v in enumerate(font_vocab)}
            
            # OOV0
            self.font_vocab_size = len(self.font_to_idx)
            # OOV0
            self.font_oov_idx = 0
            
            print(f"  Font: {self.font_vocab_size}  (OOV->0)")
        else:
            self.font_to_idx = {}
            self.font_oov_idx = 0
            self.font_vocab_size = 0
        
        # 
        self.idx_to_type = {v: k for k, v in self.type_to_idx.items()}
        self.idx_to_font = {v: k for k, v in self.font_to_idx.items()}
    
    def discretize(self, value: float, min_val: float = 0.0, max_val: float = 1.0) -> int:
        """bins [0, bins-1] """
        value = np.clip(value, min_val, max_val)
        discrete = int((value - min_val) / (max_val - min_val) * (self.bins - 1))
        return np.clip(discrete, 0, self.bins - 1)  # 
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx) -> Dict[str, torch.Tensor]:
        """ - """
        item = self.data[idx]
        length = min(item['length'], self.max_length)
        
        # Canvas
        canvas_w = item['canvas_width']
        canvas_h = item['canvas_height']
        
        width_idx = self.width_to_idx.get(canvas_w, 0)
        height_idx = self.height_to_idx.get(canvas_h, 0)
        
        if width_idx == 0 and canvas_w not in self.width_to_idx:
            closest_w = min(self.width_to_idx.keys(), 
                          key=lambda x: abs(x - canvas_w)) if self.width_to_idx else 800
            width_idx = self.width_to_idx.get(closest_w, 0)
            
        if height_idx == 0 and canvas_h not in self.height_to_idx:
            closest_h = min(self.height_to_idx.keys(), 
                          key=lambda x: abs(x - canvas_h)) if self.height_to_idx else 600
            height_idx = self.height_to_idx.get(closest_h, 0)
        
        sample = {
            'id': item['id'],
            'length': torch.tensor([length], dtype=torch.long),
            'canvas_width': torch.tensor([width_idx], dtype=torch.long),
            'canvas_height': torch.tensor([height_idx], dtype=torch.long),
        }
        
        #  -  [0, bins-1] 
        for key in ['left', 'top', 'width', 'height']:
            values = [self.discretize(item[key][i]) for i in range(length)]
            values += [0] * (self.max_length - length)
            sample[key] = torch.tensor(values, dtype=torch.long).unsqueeze(-1)
        
        #  -  [0, type_vocab_size-1] 
        type_ids = []
        for i in range(length):
            type_name = item['type'][i]
            type_id = self.type_to_idx.get(type_name, 0)  # 0
            type_id = min(type_id, self.type_vocab_size - 1)  # 
            type_ids.append(type_id)
        type_ids += [0] * (self.max_length - length)
        sample['type'] = torch.tensor(type_ids, dtype=torch.long).unsqueeze(-1)
        
        #  -  [0, 7] 
        if 'opacity' in item:
            opacity_values = []
            for i in range(length):
                # 8bins: 0.0-1.0 -> 0-7
                opacity = np.clip(item['opacity'][i], 0.0, 1.0)
                discrete_val = int(opacity * 7)
                discrete_val = min(discrete_val, 7)  # 7
                opacity_values.append(discrete_val)
            opacity_values += [0] * (self.max_length - length)
            sample['opacity'] = torch.tensor(opacity_values, dtype=torch.long).unsqueeze(-1)
        
        #  -  [0, 15] 
        if 'color' in item:
            colors = []
            for i in range(length):
                rgb = item['color'][i]
                # 0-255 -> 0-15
                discrete_rgb = []
                for c in rgb:
                    c = np.clip(c, 0, 255)
                    discrete_c = int(c * 15 / 255)
                    discrete_c = min(discrete_c, 15)  # 15
                    discrete_rgb.append(discrete_c)
                colors.append(discrete_rgb)
            for _ in range(self.max_length - length):
                colors.append([0, 0, 0])
            sample['color'] = torch.tensor(colors, dtype=torch.long)
        
        #  -  [0, font_vocab_size-1] 
        if 'font_family' in item and self.font_to_idx:
            font_ids = []
            for i in range(length):
                font_name = item['font_family'][i]
                font_id = self.font_to_idx.get(font_name, self.font_oov_idx)
                #  [0, font_vocab_size-1]
                font_id = np.clip(font_id, 0, self.font_vocab_size - 1)
                font_ids.append(font_id)
            
            font_ids += [0] * (self.max_length - length)
            sample['font_family'] = torch.tensor(font_ids, dtype=torch.long).unsqueeze(-1)
        
        # UUID - demo
        if 'uuid' in item:
            # demo_only
            uuid_vals = item['uuid'][:length] + [''] * (self.max_length - length)
            sample['uuid'] = uuid_vals  # tensor
        
        # 
        if 'image_embedding' in item:
            image_embs = item['image_embedding'][:length]
            for _ in range(self.max_length - length):
                image_embs.append([0.0] * 512)
            sample['image_embedding'] = torch.tensor(image_embs, dtype=torch.float32)
        
        # 
        if 'text_embedding' in item:
            text_embs = item['text_embedding'][:length]
            for _ in range(self.max_length - length):
                text_embs.append([0.0] * 512)
            sample['text_embedding'] = torch.tensor(text_embs, dtype=torch.float32)
        
        return sample
    
    def get_input_columns(self) -> Dict:
        """
        input_columns
        input_dim Encodertoken
        """
        input_columns = {
            'id': {
                'demo_only': True,
                'shape': [1],
                'is_sequence': False,
                'primary_label': None,
            },
            'length': {
                'type': 'categorical',
                'input_dim': 50,
                'shape': [1],
                'is_sequence': False,
                'primary_label': None,
            },
            'canvas_width': {
                'type': 'categorical',
                'input_dim': self.width_vocab_size,
                'shape': [1],
                'is_sequence': False,
                'primary_label': None,
            },
            'canvas_height': {
                'type': 'categorical',
                'input_dim': self.height_vocab_size,
                'shape': [1],
                'is_sequence': False,
                'primary_label': None,
            },
            'type': {
                'type': 'categorical',
                'input_dim': self.type_vocab_size,  # 
                'shape': [1],
                'is_sequence': True,
                'primary_label': 0,
            },
            'left': {
                'type': 'categorical',
                'input_dim': self.bins,  # 64
                'shape': [1],
                'is_sequence': True,
                'primary_label': None,
            },
            'top': {
                'type': 'categorical',
                'input_dim': self.bins,  # 64
                'shape': [1],
                'is_sequence': True,
                'primary_label': None,
            },
            'width': {
                'type': 'categorical',
                'input_dim': self.bins,  # 64
                'shape': [1],
                'is_sequence': True,
                'primary_label': None,
            },
            'height': {
                'type': 'categorical',
                'input_dim': self.bins,  # 64
                'shape': [1],
                'is_sequence': True,
                'primary_label': None,
            },
            'opacity': {
                'type': 'categorical',
                'input_dim': 8,  # 0-7
                'shape': [1],
                'is_sequence': True,
                'primary_label': None,
            },
            'color': {
                'type': 'categorical',
                'input_dim': 16,  # 0-15 
                'shape': [3],
                'is_sequence': True,
                'primary_label': None,
            },
            'image_embedding': {
                'type': 'numerical',
                'shape': [512],
                'is_sequence': True,
                'primary_label': None,
            },
            'text_embedding': {
                'type': 'numerical',
                'shape': [512],
                'is_sequence': True,
                'primary_label': None,
            },
        }
        
        # 
        if self.font_vocab_size > 0:
            input_columns['font_family'] = {
                'type': 'categorical',
                'input_dim': self.font_vocab_size,
                'shape': [1],
                'is_sequence': True,
                'primary_label': None,
            }
        
        # UUID 
        input_columns['uuid'] = {
            'demo_only': True,
            'type': 'categorical',
            'input_dim': 1215,
            'shape': [1],
            'is_sequence': True,
            'primary_label': None,
        }
        
        return input_columns


def collate_fn(batch: List[Dict]) -> Dict[str, torch.Tensor]:
    """"""
    keys = batch[0].keys()
    collated = {}
    
    for key in keys:
        if key in ['id', 'uuid']:  # iduuid
            collated[key] = [item[key] for item in batch]
        else:
            collated[key] = torch.stack([item[key] for item in batch])
    
    return collated


def create_dataloader(
    data_path: str,
    split: str = 'train',
    batch_size: int = 32,
    shuffle: bool = True,
    num_workers: int = 4,
    **dataset_kwargs
) -> DataLoader:
    """"""
    dataset = DesignLayoutDataset(data_path, split=split, **dataset_kwargs)
    
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=shuffle,
        num_workers=num_workers,
        collate_fn=collate_fn,
        pin_memory=True,
    )
    
    return dataloader

