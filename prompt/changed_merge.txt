dataset.py

"""
PyTorchæ•°æ®åŠ è½½å™¨
ç”¨äºåŠ è½½è½¬æ¢åçš„JSONæ ¼å¼è®¾è®¡æ•°æ®
"""

import json
import torch
from torch.utils.data import Dataset, DataLoader
from pathlib import Path
import numpy as np
from typing import Dict, List, Optional


class DesignLayoutDataset(Dataset):
    """è®¾è®¡å¸ƒå±€æ•°æ®é›†"""
    
    def __init__(
        self,
        data_path: str,
        split: str = 'train',
        max_length: int = 20,
        bins: int = 64,
    ):
        """
        Args:
            data_path: JSONæ•°æ®æ–‡ä»¶è·¯å¾„
            split: æ•°æ®é›†åˆ’åˆ† ('train', 'val', 'test')
            max_length: æœ€å¤§å…ƒç´ æ•°é‡
            bins: ä½ç½®ç¦»æ•£åŒ–çš„åŒºé—´æ•°
        """
        self.data_path = Path(data_path)
        self.split = split
        self.max_length = max_length
        self.bins = bins
        
        # åŠ è½½æ•°æ®
        json_file = self.data_path / f"{split}.json"
        print(f"åŠ è½½æ•°æ®: {json_file}")
        with open(json_file, 'r', encoding='utf-8') as f:
            self.data = json.load(f)
        
        print(f"âœ“ åŠ è½½äº† {len(self.data)} ä¸ªæ ·æœ¬")
        
        # åŠ è½½è¯æ±‡è¡¨
        vocab_file = self.data_path.parent / "vocabulary.json"
        with open(vocab_file, 'r', encoding='utf-8') as f:
            self.vocabulary = json.load(f)
        
        # æ„å»ºæŸ¥æ‰¾è¡¨
        self._build_lookups()
    
    def _build_lookups(self):
        """æ„å»ºå­—ç¬¦ä¸²åˆ°ç´¢å¼•çš„æ˜ å°„"""
        self.type_to_idx = {v: i for i, v in enumerate(self.vocabulary['type'])}
        self.font_to_idx = {v: i for i, v in enumerate(self.vocabulary.get('font_family', []))}
        
        # æ·»åŠ ç‰¹æ®Štoken
        self.type_to_idx['<MASK>'] = len(self.type_to_idx)
        self.type_to_idx['<NULL>'] = len(self.type_to_idx)
        self.font_to_idx['<MASK>'] = len(self.font_to_idx)
        self.font_to_idx['<NULL>'] = len(self.font_to_idx)
    
    def discretize(self, value: float, min_val: float = 0.0, max_val: float = 1.0) -> int:
        """å°†è¿ç»­å€¼ç¦»æ•£åŒ–åˆ°bins"""
        value = np.clip(value, min_val, max_val)
        return int((value - min_val) / (max_val - min_val) * (self.bins - 1))
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx) -> Dict[str, torch.Tensor]:
        """è·å–å•ä¸ªæ ·æœ¬"""
        item = self.data[idx]
        length = min(item['length'], self.max_length)
        
        # å‡†å¤‡è¿”å›å­—å…¸
        sample = {
            'id': item['id'],
            'length': torch.tensor([length], dtype=torch.long),
            'canvas_width': torch.tensor([item['canvas_width']], dtype=torch.long),
            'canvas_height': torch.tensor([item['canvas_height']], dtype=torch.long),
        }
        
        # å¤„ç†åºåˆ—ç‰¹å¾
        for key in ['left', 'top', 'width', 'height']:
            # ç¦»æ•£åŒ–ä½ç½®å’Œå°ºå¯¸
            values = [self.discretize(item[key][i]) for i in range(length)]
            # Paddingåˆ°max_length
            values += [0] * (self.max_length - length)
            sample[key] = torch.tensor(values, dtype=torch.long).unsqueeze(-1)
        
        # ç±»å‹ç¼–ç 
        type_ids = [self.type_to_idx.get(item['type'][i], 0) for i in range(length)]
        type_ids += [self.type_to_idx['<NULL>']] * (self.max_length - length)
        sample['type'] = torch.tensor(type_ids, dtype=torch.long).unsqueeze(-1)
        
        # ä¸é€æ˜åº¦ï¼ˆå½’ä¸€åŒ–å€¼ï¼‰
        opacity = item['opacity'][:length] + [0.0] * (self.max_length - length)
        sample['opacity'] = torch.tensor(opacity, dtype=torch.float32).unsqueeze(-1)
        
        # é¢œè‰² (RGB)
        colors = []
        for i in range(length):
            colors.append(item['color'][i])
        # Padding
        for _ in range(self.max_length - length):
            colors.append([0, 0, 0])
        sample['color'] = torch.tensor(colors, dtype=torch.long)
        
        # å­—ä½“
        if 'font_family' in item:
            font_ids = [self.font_to_idx.get(item['font_family'][i], 0) for i in range(length)]
            font_ids += [self.font_to_idx['<NULL>']] * (self.max_length - length)
            sample['font_family'] = torch.tensor(font_ids, dtype=torch.long).unsqueeze(-1)
        
        # åµŒå…¥å‘é‡
        if 'image_embedding' in item:
            image_embs = item['image_embedding'][:length]
            # Padding
            for _ in range(self.max_length - length):
                image_embs.append([0.0] * 512)
            sample['image_embedding'] = torch.tensor(image_embs, dtype=torch.float32)
        
        if 'text_embedding' in item:
            text_embs = item['text_embedding'][:length]
            # Padding
            for _ in range(self.max_length - length):
                text_embs.append([0.0] * 512)
            sample['text_embedding'] = torch.tensor(text_embs, dtype=torch.float32)
        
        return sample


def collate_fn(batch: List[Dict]) -> Dict[str, torch.Tensor]:
    """æ‰¹å¤„ç†å‡½æ•°"""
    # è·å–æ‰€æœ‰é”®
    keys = batch[0].keys()
    collated = {}
    
    for key in keys:
        if key == 'id':
            collated[key] = [item[key] for item in batch]
        else:
            collated[key] = torch.stack([item[key] for item in batch])
    
    return collated


def create_dataloader(
    data_path: str,
    split: str = 'train',
    batch_size: int = 32,
    shuffle: bool = True,
    num_workers: int = 4,
    **dataset_kwargs
) -> DataLoader:
    """
    åˆ›å»ºæ•°æ®åŠ è½½å™¨
    
    Args:
        data_path: æ•°æ®è·¯å¾„
        split: æ•°æ®é›†åˆ’åˆ†
        batch_size: æ‰¹å¤§å°
        shuffle: æ˜¯å¦æ‰“ä¹±
        num_workers: å·¥ä½œè¿›ç¨‹æ•°
        **dataset_kwargs: ä¼ é€’ç»™Datasetçš„é¢å¤–å‚æ•°
    
    Returns:
        DataLoader
    """
    dataset = DesignLayoutDataset(data_path, split=split, **dataset_kwargs)
    
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=shuffle,
        num_workers=num_workers,
        collate_fn=collate_fn,
        pin_memory=True,
    )
    
    return dataloader


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    # æµ‹è¯•æ•°æ®åŠ è½½
    data_path = "/home/dell/Project-HCL/BaseLine/flex-dm/data/crello_json"
    
    # åˆ›å»ºè®­ç»ƒé›†åŠ è½½å™¨
    train_loader = create_dataloader(
        data_path=data_path,
        split='train',
        batch_size=16,
        shuffle=True,
    )
    
    print(f"è®­ç»ƒé›†æ‰¹æ¬¡æ•°: {len(train_loader)}")
    
    # æµ‹è¯•ä¸€ä¸ªæ‰¹æ¬¡
    batch = next(iter(train_loader))
    print("\næ ·æœ¬æ‰¹æ¬¡:")
    for key, value in batch.items():
        if isinstance(value, torch.Tensor):
            print(f"  {key:20s}: shape={list(value.shape)}, dtype={value.dtype}")
        else:
            print(f"  {key:20s}: {type(value)}")
    
    # æ˜¾ç¤ºç¬¬ä¸€ä¸ªæ ·æœ¬
    print(f"\nç¬¬ä¸€ä¸ªæ ·æœ¬ID: {batch['id'][0]}")
    print(f"é•¿åº¦: {batch['length'][0].item()}")
    print(f"ç”»å¸ƒå¤§å°: {batch['canvas_width'][0].item()} x {batch['canvas_height'][0].item()}")

demo_pytorch.py

"""
PyTorchç‰ˆæœ¬çš„MFPæ¨¡å‹æ¼”ç¤ºå’Œå¯è§†åŒ–
ç”¨äºæµ‹è¯•å’Œå¯è§†åŒ–å¸ƒå±€ç”Ÿæˆç»“æœ
"""
import json
import itertools
import logging
from pathlib import Path
from typing import Dict, List
import json

import torch
import numpy as np
from IPython.display import display, HTML

# å¯¼å…¥PyTorchæ¨¡å‹å’Œå·¥å…·
from models_pytorch import MFP
from dataset import DesignLayoutDataset
from svg_builder_pytorch import SVGBuilder
from retriever_pytorch import ImageRetriever, TextRetriever

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# è®¾ç½®éšæœºç§å­
torch.manual_seed(0)
np.random.seed(0)


class DemoConfig:
    """æ¼”ç¤ºé…ç½®"""
    def __init__(self):
        self.ckpt_dir = "/home/dell/Project-HCL/BaseLine/flexdm_pt/chechpoints"
        self.dataset_name = "crello_json"
        self.db_root = "/home/dell/Project-HCL/BaseLine/flexdm_pt/data/crello_json"
        self.batch_size = 20
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        
        # ä»»åŠ¡ç±»å‹: elem, pos, attr, txt, img
        self.target_task = "pos"
        
        # åˆ—åé…ç½®
        self.column_names = {
            "txt": ["gt-layout", "gt-visual", "input", "pred"],
            "img": ["gt-layout", "gt-visual", "input", "pred"],
            "attr": ["gt-layout", "gt-visual", "input", "pred"],
            "pos": ["gt-layout", "gt-visual", "pred-layout", "pred-visual"],
            "elem": ["gt-layout", "gt-visual", "input-layout", "input-visual", "pred-layout", "pred-visual"],
        }
        
        # å±æ€§åˆ†ç»„
        self.attribute_groups = {
            "type": ["type"],
            "pos": ["left", "top", "width", "height"],
            "attr": ["opacity", "color", "font_family"],
            "img": ["image_embedding"],
            "txt": ["text_embedding"],
        }


def load_model(checkpoint_path: str, input_columns: Dict, device: str = 'cuda'):
    """
    åŠ è½½PyTorchæ¨¡å‹
    
    Args:
        checkpoint_path: checkpointè·¯å¾„
        input_columns: è¾“å…¥åˆ—é…ç½®
        device: è®¾å¤‡
    
    Returns:
        åŠ è½½å¥½çš„æ¨¡å‹
    """
    # åˆ›å»ºæ¨¡å‹
    model = MFP(
        input_columns=input_columns,
        embed_dim=256,
        num_blocks=4,
        num_heads=8,
        dropout=0.1,
    )
    
    # åŠ è½½æƒé‡
    checkpoint = torch.load(checkpoint_path, map_location=device)
    if 'state_dict' in checkpoint:
        state_dict = checkpoint['state_dict']
    else:
        state_dict = checkpoint
    
    # åŠ è½½æƒé‡ï¼ˆå…è®¸éƒ¨åˆ†åŒ¹é…ï¼‰
    missing, unexpected = model.load_state_dict(state_dict, strict=False)
    
    if missing:
        logger.warning(f"Missing keys: {len(missing)}")
    if unexpected:
        logger.warning(f"Unexpected keys: {len(unexpected)}")
    
    model.to(device)
    model.eval()
    
    logger.info(f"âœ“ Model loaded from {checkpoint_path}")
    return model


def get_seq_mask(lengths: torch.Tensor, max_len: int = None) -> torch.Tensor:
    """
    ç”Ÿæˆåºåˆ—æ©ç 
    
    Args:
        lengths: (B,) é•¿åº¦å¼ é‡
        max_len: æœ€å¤§é•¿åº¦
    
    Returns:
        mask: (B, S) å¸ƒå°”æ©ç 
    """
    if lengths.dim() == 2:
        lengths = lengths.squeeze(-1)
    
    batch_size = lengths.size(0)
    if max_len is None:
        max_len = lengths.max().item()
    
    # åˆ›å»ºæ©ç 
    mask = torch.arange(max_len, device=lengths.device).unsqueeze(0) < lengths.unsqueeze(1)
    return mask


def get_initial_masks(input_columns: Dict, seq_mask: torch.Tensor) -> Dict[str, torch.Tensor]:
    """
    åˆå§‹åŒ–æ©ç å­—å…¸ï¼ˆæ‰€æœ‰ä¸ºFalseï¼‰
    
    Args:
        input_columns: è¾“å…¥åˆ—é…ç½®
        seq_mask: åºåˆ—æ©ç 
    
    Returns:
        masks: æ©ç å­—å…¸
    """
    masks = {}
    batch_size, seq_len = seq_mask.shape
    
    for key, column in input_columns.items():
        if column.get('is_sequence', False):
            masks[key] = torch.zeros_like(seq_mask, dtype=torch.bool)
        else:
            masks[key] = torch.ones(batch_size, dtype=torch.bool)
    
    return masks


def set_visual_default(item: Dict) -> Dict:
    """è®¾ç½®å¯è§†åŒ–é»˜è®¤å€¼"""
    item = item.copy()
    for elem in item.get('elements', []):
        if 'color' not in elem or elem['color'] is None:
            elem['color'] = [0, 0, 0]
        if 'opacity' not in elem or elem['opacity'] is None:
            elem['opacity'] = 1.0
        if 'font_family' not in elem or elem['font_family'] is None:
            elem['font_family'] = 'DummyFont'
    return item


def tensor_to_list(data: Dict) -> List[Dict]:
    """
    å°†æ‰¹æ¬¡å¼ é‡è½¬æ¢ä¸ºæ ·æœ¬åˆ—è¡¨
    
    Args:
        data: æ‰¹æ¬¡æ•°æ®å­—å…¸
    
    Returns:
        æ ·æœ¬åˆ—è¡¨
    """
    batch_size = data['length'].size(0)
    items = []
    
    for i in range(batch_size):
        item = {
            'id': data['id'][i] if 'id' in data else f'sample_{i}',
            'canvas_width': data['canvas_width'][i].item() if 'canvas_width' in data else 800,
            'canvas_height': data['canvas_height'][i].item() if 'canvas_height' in data else 600,
            'length': data['length'][i].item(),
            'elements': []
        }
        
        # è·å–æœ‰æ•ˆé•¿åº¦
        length = item['length'] + 1  # åŸºäº0çš„ç´¢å¼•
        
        # æ„å»ºå…ƒç´ åˆ—è¡¨
        for j in range(length):
            element = {}
            
            for key, value in data.items():
                if key in ['id', 'length', 'canvas_width', 'canvas_height']:
                    continue
                
                if not torch.is_tensor(value):
                    continue
                    
                if value.dim() >= 2 and value.size(1) > j:
                    elem_value = value[i, j]
                    
                    # è½¬æ¢ä¸ºPythonåŸç”Ÿç±»å‹
                    if elem_value.dim() == 0:
                        # æ ‡é‡
                        element[key] = elem_value.item()
                    elif elem_value.dim() == 1:
                        # ä¸€ç»´å‘é‡
                        if elem_value.size(0) == 1:
                            # å•ä¸ªå€¼ï¼Œå±•å¼€
                            element[key] = elem_value[0].item()
                        else:
                            # å¤šä¸ªå€¼ï¼ˆå¦‚RGBæˆ–embeddingï¼‰
                            element[key] = elem_value.cpu().numpy().tolist()
                    else:
                        # å¤šç»´ï¼ˆå¯¹äºåˆ†ç±»å˜é‡ï¼Œå–argmaxï¼‰
                        if elem_value.dim() == 2:
                            # (num_features, vocab_size) -> å–argmax
                            indices = elem_value.argmax(dim=-1)
                            if indices.size(0) == 1:
                                element[key] = indices[0].item()
                            else:
                                element[key] = indices.cpu().numpy().tolist()
                        else:
                            element[key] = elem_value.cpu().numpy().tolist()
            
            item['elements'].append(element)
        
        items.append(item)
    
    return items


def apply_task_masks(
    example: Dict,
    input_columns: Dict,
    target_task: str,
    attribute_groups: Dict,
    device: str
) -> Dict[str, torch.Tensor]:
    """
    åº”ç”¨ä»»åŠ¡ç‰¹å®šçš„æ©ç 
    
    Args:
        example: è¾“å…¥æ ·æœ¬
        input_columns: è¾“å…¥åˆ—é…ç½®
        target_task: ç›®æ ‡ä»»åŠ¡
        attribute_groups: å±æ€§åˆ†ç»„
        device: è®¾å¤‡
    
    Returns:
        masks: æ©ç å­—å…¸
    """
    seq_mask = get_seq_mask(example['length'], example['left'].size(1))
    mfp_masks = get_initial_masks(input_columns, seq_mask)
    
    for key in mfp_masks.keys():
        if not input_columns[key].get('is_sequence', False):
            continue
        
        mask = mfp_masks[key].clone()
        
        if target_task == "elem":
            # å…ƒç´ çº§æ©ç ï¼šéšè—ç¬¬ä¸€ä¸ªå…ƒç´ 
            mask[:, 0] = True
        else:
            # ç‰¹å¾çº§æ©ç 
            if key == "type":
                continue
            
            if target_task in attribute_groups:
                attr_keys = attribute_groups[target_task]
                if key in attr_keys:
                    mask = seq_mask.clone()
        
        mfp_masks[key] = mask.to(device)
    
    return mfp_masks


def visualize_reconstruction(
    model: torch.nn.Module,
    example: Dict,
    builders: Dict,
    config: DemoConfig,
    input_columns: Dict,
):
    """
    å¯è§†åŒ–é‡å»ºç»“æœ
    
    Args:
        model: PyTorchæ¨¡å‹
        example: è¾“å…¥æ ·æœ¬
        builders: SVGæ„å»ºå™¨å­—å…¸
        config: é…ç½®
        input_columns: è¾“å…¥åˆ—é…ç½®
    
    Returns:
        SVGåˆ—è¡¨
    """
    svgs = []
    target_task = config.target_task
    
    # è½¬æ¢ä¸ºæ ·æœ¬åˆ—è¡¨
    items = tensor_to_list(example)
    
    # GTå¸ƒå±€å’Œè§†è§‰
    svgs.append(list(map(builders["layout"], items)))
    svgs.append(list(map(builders["visual"], items)))
    
    # è¾“å…¥å¯è§†åŒ–ï¼ˆæ ¹æ®ä»»åŠ¡ç±»å‹ï¼‰
    if target_task == "txt":
        svgs.append(list(map(builders["visual_wo_text"], items)))
    elif target_task == "img":
        svgs.append(list(map(builders["visual_wo_image"], items)))
    elif target_task == "attr":
        svgs.append(list(map(builders["visual"], [set_visual_default(x) for x in items])))
    
    # åº”ç”¨æ©ç 
    mfp_masks = apply_task_masks(
        example, input_columns, target_task, 
        config.attribute_groups, config.device
    )
    
    # å…ƒç´ çº§ä»»åŠ¡çš„ç‰¹æ®Šå¤„ç†
    if target_task == "elem":
        # åˆ›å»ºç§»é™¤ç¬¬ä¸€ä¸ªå…ƒç´ åçš„æ ·æœ¬
        example_copy = {}
        for key, value in example.items():
            if isinstance(value, torch.Tensor) and value.dim() >= 2 and value.size(1) > 1:
                # ç§»é™¤ç¬¬ä¸€ä¸ªå…ƒç´ 
                indices = torch.where(~mfp_masks[key][0, :])[0]
                example_copy[key] = torch.index_select(value, 1, indices)
            else:
                example_copy[key] = value
        
        example_copy['length'] = example['length'] - 1
        
        items_copy = tensor_to_list(example_copy)
        svgs.append(list(map(builders["layout"], items_copy)))
        svgs.append(list(map(builders["visual"], items_copy)))
    
    # æ¨¡å‹é¢„æµ‹
    with torch.no_grad():
        # å°†æ©ç ä¿¡æ¯æ·»åŠ åˆ°è¾“å…¥
        pred = model_inference_with_masks(model, example, mfp_masks)
    
    # åˆå¹¶é¢„æµ‹å’ŒåŸå§‹è¾“å…¥
    for key in example:
        if key not in pred:
            pred[key] = example[key]
    
    # é¢„æµ‹å¯è§†åŒ–
    pred_items = tensor_to_list(pred)
    
    if target_task in ["pos", "elem"]:
        svgs.append(list(map(builders["layout"], pred_items)))
    svgs.append(list(map(builders["visual"], pred_items)))
    
    return [list(grouper(row, len(config.column_names[target_task]))) for row in zip(*svgs)]


def model_inference_with_masks(model, inputs, masks):
    """
    ä½¿ç”¨æ©ç è¿›è¡Œæ¨¡å‹æ¨ç†
    
    Args:
        model: æ¨¡å‹
        inputs: è¾“å…¥æ•°æ®
        masks: æ©ç å­—å…¸
    
    Returns:
        é¢„æµ‹ç»“æœ
    """
    # åº”ç”¨æ©ç åˆ°è¾“å…¥
    masked_inputs = {}
    for key, value in inputs.items():
        if key in masks and torch.is_tensor(value):
            mask = masks[key]
            if mask.any():
                # åº”ç”¨æ©ç ï¼ˆä½¿ç”¨ç‰¹æ®Štokenï¼‰
                masked_value = value.clone()
                if value.dim() == 3:  # (B, S, F)
                    masked_value[mask] = 0  # æˆ–ä½¿ç”¨ç‰¹æ®Šå€¼
                masked_inputs[key] = masked_value
            else:
                masked_inputs[key] = value
        else:
            masked_inputs[key] = value
    
    # æ¨¡å‹æ¨ç†
    outputs = model(masked_inputs)
    
    return outputs


def grouper(iterable, n):
    """å°†å¯è¿­ä»£å¯¹è±¡åˆ†ç»„"""
    args = [iter(iterable)] * n
    return itertools.zip_longest(*args, fillvalue=None)


def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®
    config = DemoConfig()
    
    logger.info("="*80)
    logger.info("MFP PyTorch Demo")
    logger.info("="*80)
    
    # åŠ è½½æ•°æ®
    logger.info(f"Loading dataset from {config.db_root}")
    dataset = DesignLayoutDataset(
        config.db_root, 
        split='test',
        max_length=20
    )
    
    # åˆ›å»ºDataLoader
    from torch.utils.data import DataLoader
    from dataset import collate_fn
    
    dataloader = DataLoader(
        dataset,
        batch_size=config.batch_size,
        shuffle=False,
        collate_fn=collate_fn
    )
    
    # è·å–ä¸€ä¸ªæ‰¹æ¬¡
    example = next(iter(dataloader))
    
    # ç§»åŠ¨åˆ°è®¾å¤‡
    for key in example:
        if torch.is_tensor(example[key]):
            example[key] = example[key].to(config.device)
    
    # # è·å–è¾“å…¥åˆ—é…ç½®
    # input_columns = {
    #     'type': {'is_sequence': True, 'type': 'categorical', 'input_dim': 7, 'shape': [1]},
    #     'left': {'is_sequence': True, 'type': 'categorical', 'input_dim': 64, 'shape': [1]},
    #     'top': {'is_sequence': True, 'type': 'categorical', 'input_dim': 64, 'shape': [1]},
    #     'width': {'is_sequence': True, 'type': 'categorical', 'input_dim': 64, 'shape': [1]},
    #     'height': {'is_sequence': True, 'type': 'categorical', 'input_dim': 64, 'shape': [1]},
    #     'image_embedding': {'is_sequence': True, 'type': 'numerical', 'shape': [512]},
    # }

    with open('/home/dell/Project-HCL/BaseLine/flexdm_pt/scripts/input_columns_generated.json', 'r') as f:
        input_columns = json.load(f)    
    
    # åŠ è½½æ¨¡å‹
    logger.info(f"Loading model from {config.ckpt_dir}")
    checkpoint_path = Path(config.ckpt_dir) / "best_pytorch.pth"
    model = load_model(str(checkpoint_path), input_columns, config.device)
    
    # æ„å»ºæ£€ç´¢æ•°æ®åº“
    logger.info("Building retrieval databases...")
    db_root = Path(config.db_root).parent / config.dataset_name
    
    image_db = ImageRetriever(db_root, image_path=db_root / "images")
    image_db.build("test")
    
    text_db = TextRetriever(db_root, text_path=db_root / "texts")
    text_db.build("test")
    
    # åˆ›å»ºSVGæ„å»ºå™¨
    logger.info("Creating SVG builders...")
    builders = {}
    
    # å¸ƒå±€æ„å»ºå™¨
    builders["layout"] = SVGBuilder(
        max_width=128,
        max_height=192,
        key="type",
    )
    
    # è§†è§‰æ„å»ºå™¨
    patterns = [
        ("visual", image_db, text_db),
        ("visual_wo_text", image_db, None),
        ("visual_wo_image", None, text_db),
    ]
    
    for (name, idb, tdb) in patterns:
        builders[name] = SVGBuilder(
            max_width=128,
            max_height=192,
            key="color",
            image_db=idb,
            text_db=tdb,
            render_text=True,
        )
    
    # å¯è§†åŒ–é‡å»º
    logger.info(f"Visualizing reconstruction for task: {config.target_task}")
    logger.info(f"Columns: {', '.join(config.column_names[config.target_task])}")
    
    svgs = visualize_reconstruction(
        model, example, builders, config, input_columns
    )
    
    # æ˜¾ç¤ºç»“æœ
    for i, row in enumerate(svgs):
        print(f"Sample {i}:")
        display(HTML("<div>%s</div>" % " ".join(itertools.chain.from_iterable(row))))
    
    logger.info("âœ“ Demo completed!")


if __name__ == "__main__":
    main()

models_pytorch.py
"""
PyTorchæ¨¡å‹æ¶æ„ - å®Œå…¨ä¿®å¤ç‰ˆæœ¬
ä½¿ç”¨ModuleListæ›¿ä»£ModuleDictï¼Œé¿å…é”®å†²çª
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, Optional
import math


# ==================== Transformer Components ====================

class MultiHeadSelfAttention(nn.Module):
    """å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶"""
    
    def __init__(
        self,
        embed_dim: int,
        num_heads: int = 8,
        dropout: float = 0.1,
        lookahead: bool = True,
    ):
        super().__init__()
        assert embed_dim % num_heads == 0
        
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        self.lookahead = lookahead
        
        self.q_proj = nn.Linear(embed_dim, embed_dim)
        self.k_proj = nn.Linear(embed_dim, embed_dim)
        self.v_proj = nn.Linear(embed_dim, embed_dim)
        self.out_proj = nn.Linear(embed_dim, embed_dim)
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None):
        B, S, D = x.shape
        
        q = self.q_proj(x).view(B, S, self.num_heads, self.head_dim).transpose(1, 2)
        k = self.k_proj(x).view(B, S, self.num_heads, self.head_dim).transpose(1, 2)
        v = self.v_proj(x).view(B, S, self.num_heads, self.head_dim).transpose(1, 2)
        
        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)
        
        if mask is not None:
            mask = mask.unsqueeze(1).unsqueeze(2)
            scores = scores.masked_fill(~mask, float('-inf'))
            
            if not self.lookahead:
                causal_mask = torch.triu(
                    torch.ones(S, S, device=x.device, dtype=torch.bool),
                    diagonal=1
                )
                scores = scores.masked_fill(causal_mask, float('-inf'))
        
        attn_weights = F.softmax(scores, dim=-1)
        attn_weights = self.dropout(attn_weights)
        
        out = torch.matmul(attn_weights, v)
        out = out.transpose(1, 2).contiguous().view(B, S, D)
        out = self.out_proj(out)
        return out


class TransformerBlock(nn.Module):
    """Transformerå—ï¼ˆDeepSVGé£æ ¼ï¼‰"""
    
    def __init__(
        self,
        embed_dim: int = 128,
        num_heads: int = 8,
        ff_dim: Optional[int] = None,
        dropout: float = 0.1,
        lookahead: bool = True,
    ):
        super().__init__()
        ff_dim = ff_dim or (2 * embed_dim)
        
        self.norm1 = nn.LayerNorm(embed_dim)
        self.attn = MultiHeadSelfAttention(
            embed_dim, num_heads, dropout, lookahead
        )
        self.dropout1 = nn.Dropout(dropout)
        
        self.norm2 = nn.LayerNorm(embed_dim)
        self.ffn = nn.Sequential(
            nn.Linear(embed_dim, ff_dim),
            nn.ReLU(),
            nn.Linear(ff_dim, embed_dim),
        )
        self.dropout2 = nn.Dropout(dropout)
    
    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None):
        residual = x
        x = self.norm1(x)
        x = self.attn(x, mask)
        x = self.dropout1(x)
        x = residual + x
        
        residual = x
        x = self.norm2(x)
        x = self.ffn(x)
        x = self.dropout2(x)
        x = residual + x
        
        return x


class TransformerBlocks(nn.Module):
    """å †å çš„Transformerå—"""
    
    def __init__(
        self,
        num_blocks: int = 4,
        embed_dim: int = 128,
        num_heads: int = 8,
        dropout: float = 0.1,
        lookahead: bool = True,
    ):
        super().__init__()
        self.blocks = nn.ModuleList([
            TransformerBlock(embed_dim, num_heads, dropout=dropout, lookahead=lookahead)
            for _ in range(num_blocks)
        ])
    
    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None):
        for block in self.blocks:
            x = block(x, mask)
        return x


# ==================== Encoderï¼ˆä¿®å¤ç‰ˆæœ¬ï¼‰====================

class Encoder(nn.Module):
    """ç¼–ç å™¨ - ä½¿ç”¨ModuleListé¿å…é”®å†²çª"""
    
    def __init__(
        self,
        input_columns: Dict,
        embed_dim: int = 128,
        dropout: float = 0.1,
        max_length: int = 50,
    ):
        super().__init__()
        self.input_columns = input_columns
        self.embed_dim = embed_dim
        
        # ä½¿ç”¨åˆ—è¡¨è€Œä¸æ˜¯å­—å…¸å­˜å‚¨å±‚
        self.emb_layers = nn.ModuleList()
        self.emb_keys = []
        self.emb_types = []
        self.emb_configs = []
        
        print("åˆå§‹åŒ–Encoder:")
        for key, column in input_columns.items():
            if not column.get('is_sequence', False):
                continue
            
            self.emb_keys.append(key)
            self.emb_types.append(column['type'])
            self.emb_configs.append(column)
            
            if column['type'] == 'categorical':
                vocab_size = column['input_dim'] + 2
                self.emb_layers.append(nn.Embedding(vocab_size, embed_dim))
                print(f"  {key}: Embedding({vocab_size}, {embed_dim})")
            elif column['type'] == 'numerical':
                input_size = column['shape'][-1] if 'shape' in column else 1
                self.emb_layers.append(nn.Linear(input_size, embed_dim))
                print(f"  {key}: Linear({input_size}, {embed_dim})")
        
        print(f"æ€»è®¡: {len(self.emb_keys)} ä¸ªç‰¹å¾")
        
        self.pos_embedding = nn.Embedding(max_length + 1, embed_dim)
        self.dropout = nn.Dropout(dropout)
    
    # def forward(self, inputs: Dict[str, torch.Tensor]) -> tuple:
    #     batch_size = inputs['length'].size(0)
        
    #     # æ‰¾åˆ°åºåˆ—é•¿åº¦
    #     seq_len = None
    #     for key in self.emb_keys:
    #         if key in inputs:
    #             seq_len = inputs[key].size(1)
    #             break
        
    #     if seq_len is None:
    #         raise ValueError("æœªæ‰¾åˆ°åºåˆ—ç‰¹å¾")
        
    #     # ç¼–ç æ¯ä¸ªç‰¹å¾
    #     seq_embs = []
    #     for idx, key in enumerate(self.emb_keys):
    #         if key not in inputs:
    #             continue
            
    #         x = inputs[key]
    #         if key in ['color']:
    #             x = x.float()
    #         emb = self.emb_layers[idx](x)
            
    #         # å¤„ç†å¤šç»´ç‰¹å¾ï¼ˆå¦‚RGBï¼‰
    #         if len(emb.shape) == 4:
    #             emb = emb.sum(dim=2)
            
    #         seq_embs.append(emb)
        
    #     # èåˆç‰¹å¾
    #     seq = torch.stack(seq_embs).sum(dim=0)
        
    #     # ä½ç½®ç¼–ç 
    #     positions = torch.arange(seq_len, device=seq.device).unsqueeze(0).expand(batch_size, -1)
    #     seq = seq + self.pos_embedding(positions)
    #     seq = self.dropout(seq)
        
    #     # ç”Ÿæˆæ©ç 
    #     lengths = inputs['length'].squeeze(-1)
    #     mask = torch.arange(seq_len, device=seq.device).unsqueeze(0) < lengths.unsqueeze(1)
        
    #     return seq, mask
    def forward(self, inputs: Dict[str, torch.Tensor]) -> tuple:
        """ä¿®å¤äº†ç±»å‹è½¬æ¢çš„forwardæ–¹æ³•"""
        batch_size = inputs['length'].size(0)
        
        # æ‰¾åˆ°åºåˆ—é•¿åº¦
        seq_len = None
        for key in self.emb_keys:
            if key in inputs:
                seq_len = inputs[key].size(1)
                break
        
        if seq_len is None:
            raise ValueError("æœªæ‰¾åˆ°åºåˆ—ç‰¹å¾")
        
        # ç¼–ç æ¯ä¸ªç‰¹å¾
        seq_embs = []
        for idx, key in enumerate(self.emb_keys):
            if key not in inputs:
                continue
            
            x = inputs[key]
            layer = self.emb_layers[idx]
            
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šæ ¹æ®å±‚ç±»å‹è‡ªåŠ¨è½¬æ¢è¾“å…¥æ•°æ®ç±»å‹
            if isinstance(layer, nn.Embedding):
                # Embeddingå±‚éœ€è¦Longç±»å‹
                if x.dtype != torch.long:
                    x = x.long()
            elif isinstance(layer, nn.Linear):
                # Linearå±‚éœ€è¦Floatç±»å‹
                if x.dtype != torch.float:
                    x = x.float()
            
            emb = layer(x)
            
            # å¤„ç†å¤šç»´ç‰¹å¾ï¼ˆå¦‚RGBï¼‰
            if len(emb.shape) == 4:
                emb = emb.sum(dim=2)
            
            seq_embs.append(emb)
        
        # èåˆç‰¹å¾
        seq = torch.stack(seq_embs).sum(dim=0)
        
        # ä½ç½®ç¼–ç 
        positions = torch.arange(seq_len, device=seq.device).unsqueeze(0).expand(batch_size, -1)
        seq = seq + self.pos_embedding(positions)
        seq = self.dropout(seq)
        
        # ç”Ÿæˆæ©ç 
        lengths = inputs['length'].squeeze(-1)
        mask = torch.arange(seq_len, device=seq.device).unsqueeze(0) < lengths.unsqueeze(1)
        
        return seq, mask


# ==================== Decoderï¼ˆä¿®å¤ç‰ˆæœ¬ï¼‰====================

class Decoder(nn.Module):
    """è§£ç å™¨ - ä½¿ç”¨ModuleListé¿å…é”®å†²çª"""
    
    def __init__(self, input_columns: Dict, embed_dim: int = 128):
        super().__init__()
        self.input_columns = input_columns
        
        # ä½¿ç”¨åˆ—è¡¨è€Œä¸æ˜¯å­—å…¸å­˜å‚¨å±‚
        self.head_layers = nn.ModuleList()
        self.head_keys = []
        self.head_configs = []
        
        print("åˆå§‹åŒ–Decoder:")
        for key, column in input_columns.items():
            if not column.get('is_sequence', False):
                continue
            
            self.head_keys.append(key)
            self.head_configs.append(column)
            
            if column['type'] == 'categorical':
                shape = column.get('shape', [1])
                output_dim = shape[-1] * column['input_dim']
                self.head_layers.append(nn.Linear(embed_dim, output_dim))
                print(f"  {key}: Linear({embed_dim}, {output_dim}) -> ({shape[-1]}, {column['input_dim']})")
            else:
                shape = column.get('shape', [1])
                output_dim = shape[-1]
                self.head_layers.append(nn.Linear(embed_dim, output_dim))
                print(f"  {key}: Linear({embed_dim}, {output_dim})")
        
        print(f"æ€»è®¡: {len(self.head_keys)} ä¸ªè¾“å‡ºå¤´")
    
    def forward(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:
        outputs = {}
        batch_size, seq_len, _ = x.shape
        
        for idx, key in enumerate(self.head_keys):
            column = self.head_configs[idx]
            pred = self.head_layers[idx](x)
            
            if column['type'] == 'categorical':
                shape = column.get('shape', [1])
                num_features = shape[-1]
                vocab_size = column['input_dim']
                pred = pred.view(batch_size, seq_len, num_features, vocab_size)
            
            outputs[key] = pred
        
        return outputs


# ==================== MFP Model ====================

class MFP(nn.Module):
    """Masked Field Predictionæ¨¡å‹"""
    
    def __init__(
        self,
        input_columns: Dict,
        embed_dim: int = 128,
        num_blocks: int = 4,
        num_heads: int = 8,
        dropout: float = 0.1,
        max_length: int = 50,
    ):
        super().__init__()
        self.input_columns = input_columns
        
        print("\n" + "="*60)
        print("åˆå§‹åŒ–MFPæ¨¡å‹")
        print("="*60)
        
        self.encoder = Encoder(
            input_columns, embed_dim, dropout, max_length
        )
        
        print("\nåˆå§‹åŒ–Transformer:")
        print(f"  blocks={num_blocks}, embed_dim={embed_dim}, num_heads={num_heads}")
        self.transformer = TransformerBlocks(
            num_blocks, embed_dim, num_heads, dropout, lookahead=True
        )
        
        print("")
        self.decoder = Decoder(input_columns, embed_dim)
        
        total_params = sum(p.numel() for p in self.parameters())
        print(f"\næ€»å‚æ•°æ•°: {total_params:,}")
        print("="*60 + "\n")
    
    def forward(self, inputs: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        x, mask = self.encoder(inputs)
        x = self.transformer(x, mask)
        outputs = self.decoder(x)
        return outputs
    
    def load_converted_weights(self, checkpoint_path: str):
        """åŠ è½½è½¬æ¢åçš„æƒé‡"""
        checkpoint = torch.load(checkpoint_path, map_location='cpu')
        state_dict = checkpoint['state_dict']
        
        missing_keys, unexpected_keys = self.load_state_dict(state_dict, strict=False)
        
        if missing_keys:
            print(f"è­¦å‘Š: ç¼ºå¤± {len(missing_keys)} ä¸ªé”®")
        if unexpected_keys:
            print(f"è­¦å‘Š: å¤šä½™ {len(unexpected_keys)} ä¸ªé”®")
        
        print("âœ“ æƒé‡åŠ è½½å®Œæˆ")


# ==================== æµ‹è¯•ä»£ç  ====================

if __name__ == "__main__":
    print("æµ‹è¯•MFPæ¨¡å‹ï¼ˆä¿®å¤ç‰ˆæœ¬ï¼‰\n")
    
    input_columns = {
        'type': {'is_sequence': True, 'type': 'categorical', 'input_dim': 7, 'shape': [1]},
        'left': {'is_sequence': True, 'type': 'categorical', 'input_dim': 64, 'shape': [1]},
        'top': {'is_sequence': True, 'type': 'categorical', 'input_dim': 64, 'shape': [1]},
        'width': {'is_sequence': True, 'type': 'categorical', 'input_dim': 64, 'shape': [1]},
        'height': {'is_sequence': True, 'type': 'categorical', 'input_dim': 64, 'shape': [1]},
        'image_embedding': {'is_sequence': True, 'type': 'numerical', 'shape': [512]},
    }
    
    model = MFP(input_columns, embed_dim=256, num_blocks=4)
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    batch_size = 2
    seq_len = 10
    
    test_input = {
        'length': torch.tensor([[5], [7]], dtype=torch.long),
        'type': torch.randint(0, 7, (batch_size, seq_len, 1)),
        'left': torch.randint(0, 64, (batch_size, seq_len, 1)),
        'top': torch.randint(0, 64, (batch_size, seq_len, 1)),
        'width': torch.randint(0, 64, (batch_size, seq_len, 1)),
        'height': torch.randint(0, 64, (batch_size, seq_len, 1)),
        'image_embedding': torch.randn(batch_size, seq_len, 512),
    }
    
    print("æµ‹è¯•å‰å‘ä¼ æ’­...")
    with torch.no_grad():
        outputs = model(test_input)
    
    print("\nâœ“ å‰å‘ä¼ æ’­æˆåŠŸ!")
    print("\nè¾“å‡ºå½¢çŠ¶:")
    for key, value in outputs.items():
        print(f"  {key:20s}: {list(value.shape)}")